{
  "gpt-5-nano": {
    "model": "gpt-5-nano",
    "num_samples": 100,
    "avg_relevance": 9.21,
    "avg_clarity": 9.1,
    "avg_discriminative": 9.12,
    "avg_composite": 9.143333333333334,
    "results": [
      {
        "role_id": "be-401",
        "role_title": "Backend Engineer - Training Infrastructure",
        "question": "Question:\nYou need to submit a simple Ray-based training task on a single node with multiple GPUs. Write a minimal Python snippet that:\n\n- Uses Ray to launch 2 parallel train tasks.\n- Each task should simulate a training step by burning 0.5 seconds and then return a unique scalar loss value (e.g., 0.9 and 0.8) based on the task id.\n- After launching, collect the results and print the average loss.\n\nConstraints:\n- Use ray.init() local mode or standard mode as you prefer.\n- Do not rely on external files or GPUs in the snippet (you can simulate the GPU with sleeps).\n- Provide clear comments explaining how you would extend this to multiple nodes and GPUs in a real cluster.",
        "scores": {
          "relevance": 9,
          "clarity": 10,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests Ray (core for distributed ML training orchestration, job scheduling, GPU allocation per role); minor deduction as role emphasizes multi-node but question scopes to single node with extension (-1). Clarity: Crystal clear, well-scoped with precise constraints, comments requirement, no ambiguity. Discriminative: Tests applied Ray usage (ray.init, remote tasks, result collection), simulation forces understanding of parallelism, extension comments probe deeper cluster knowledge and trade-offs.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-402",
        "role_title": "Backend Engineer - Model Deployment",
        "question": "Question: You have a trained ML model you want to deploy on KServe with a rolling deployment strategy that can automatically rollback on performance cliff.\n\n- Provide a concise Python snippet (pseudo-code acceptable) that:\n  - Creates a KServe InferenceService for a model served from a container image.\n  - Configures a rolling rollout split of 20% canary and 80% stable.\n  - Adds a basic automated rollback condition: if external monitoring indicates AUC drops by more than 5% compared to the stable baseline for the last N minutes, trigger rollback to 100% stable.\n- Briefly explain how you would integrate a Redis-based metric flag as the trigger (what data you\u2019d store and how the service would read it).\n\nConstraints:\n- Focus on the core deployment definitions and rollback trigger logic (no need to implement full monitoring system).\n- Keep the code concise enough to fit in 2-5 minutes of reading.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (model deployment, KServe, rolling strategies, rollback automation); minor ding as KServe isn't explicitly named but fits MLOps focus (-1). Clarity: Crystal clear with precise scope, constraints, and time guide; no ambiguity. Discriminative: Requires applied KServe YAML/Python knowledge, rollout config, and integration design with trade-offs, strongly separating experienced MLOps engineers.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-410",
        "role_title": "Backend Engineer - Bulk Operations",
        "question": "Question:\nDesign a small Go function signature and outline an in-memory approach to implement idempotent bulk upsert operations (insert or update) for a batch of records, using a unique client-provided operation_id to ensure exactly-once semantics. Assume records have fields: id (UUID), data (string), and operation_id (string). Your answer should include:\n\n- The function signature (inputs/outputs) for processing a batch of records.\n- How you would handle partial failures within the batch (e.g., some records succeed, some fail) and report per-record status.\n- A high-level outline of how you\u2019d achieve idempotency using operation_id, including any data structures or storage considerations (no need for full code, but show critical steps and error handling).\n\nConstraints to consider:\n- Must work with a PostgreSQL-backed store and a Redis cache per the stack.\n- Emphasize practicality for bulk operations in a mid-senior Go role.",
        "scores": {
          "relevance": 9,
          "clarity": 10,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core role skills (bulk ops, idempotency, partial success) with exact Go/PostgreSQL/Redis stack (-1 minor for not deeply testing batch processing internals). Clarity: Crystal clear, well-scoped with specific requirements, constraints, and output expectations. Discriminative: Tests applied design knowledge, trade-offs (partial failure, storage), and mid-senior depth without being trivial or overly broad.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-407",
        "role_title": "Staff Backend Engineer - ML Infrastructure",
        "question": "Question: You are asked to design a simple, cost-efficient training job launcher for a Python-based ML workload using Ray on Kubernetes. The workload runs a PyTorch training script with a single node and 4 GPUs. Describe (in ~3\u20135 sentences or bullet points) how you would:\n\n- package and submit the job so it can be executed reproducibly on a Kubernetes cluster using Ray.\n- ensure the driver and workers are scaled appropriately (e.g., when using Ray with a small budget vs. when more resources are available).\n- minimize idle GPU and pod costs between runs (e.g., using preemption, autoscaling, or job cancellation).\n\nInclude any key Ray or Kubernetes components you would leverage and any minimal YAML or command-line snippet you would use to launch a single-trainer, multi-GPU job.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core ML infra skills (Ray on K8s, cost optimization, training platforms) matching role; minor deduction as PyTorch is specified but not core to infra leadership (-1). Clarity: Crystal clear, well-scoped with constraints (single node/4 GPUs, 3-5 sentences, specific subpoints, YAML snippets), no ambiguity. Discriminative: Requires applied knowledge of Ray/K8s integration, scaling trade-offs, and cost strategies, strongly separates experienced ML infra engineers from juniors.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-408",
        "role_title": "Junior Backend Developer - Health Endpoints",
        "question": "Question:\nIn a Go-based backend service, implement a simple HTTP health check endpoint and a readiness check endpoint. The readiness check should return 200 OK only when an in-memory slice of \"jobs\" is non-empty and a boolean variable isReady is true. The health check should always return 200 OK with a JSON body {\"status\":\"ok\"}. Provide a minimal Go code snippet (no external packages beyond the standard library) that wires these two endpoints using the http package, and show how you would simulate toggling readiness and jobs in code comments.\n\nWhat to include:\n- A main function setting up two HTTP handlers: /health and /ready\n- /health returns status 200 with body {\"status\":\"ok\"}\n- /ready returns 200 only when len(jobs) > 0 && isReady is true; otherwise returns 503 with body {\"status\":\"not-ready\"}\n- A simple in-memory declaration of the variables: var jobs []string and var isReady bool, with comments on how they would be updated in real scenarios.",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core focus on health/liveness/readiness endpoints in Go backend, directly testing exact skills needed (no red flags). Clarity: Crystal clear with precise requirements, paths, responses, and constraints; minor deduction for lacking explicit time/format limits. Discriminative: Requires applied Go knowledge (HTTP handlers, JSON, logic, state), distinguishes candidates able to write functional code vs. superficial knowledge; tests trade-offs in endpoint wiring.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-406",
        "role_title": "Senior Backend Engineer - LLM Serving",
        "question": "Question:\nYou are deploying a vLLM-based inference service behind a Kubernetes cluster. You want to implement a simple batching strategy that groups incoming requests for the same model to maximize throughput, while ensuring streaming responses are delivered to clients in the correct order and with minimal latency.\n\n- Describe how you would design a lightweight Python component (no external frameworks required) that:\n  - Buffers incoming requests for a given model/version.\n  - Creates batches up to a maximum batch size or a max wait time.\n  - Dispatches batches to vLLM and streams results back to the respective client connections in the order of their original requests.\n  - Maintains per-request KV cache warm-start where possible to reduce latency (conceptually, you can describe how KV cache indices would be reused across requests in the same batch).\n\n- Provide a concise pseudocode or code sketch (\u224820-40 lines) illustrating the batching loop, request tracking, and streaming of results.\n\n- Highlight potential race conditions or edge cases and how you would mitigate them.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Relevance: Directly targets core role responsibilities (batching optimization, KV cache management, streaming) using vLLM in K8s context; no red flags. Clarity: Well-scoped with specific requirements, constraints (lightweight Python, no frameworks, concise code), unambiguous despite multiple subparts. Discriminative: Requires applied systems knowledge, trade-offs in batching/KV reuse, race condition analysis, and practical code sketch\u2014excellent separation of strong vs weak candidates.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-409",
        "role_title": "Junior Backend Engineer - File Upload",
        "question": "Question:\nIn a FastAPI app, implement a small endpoint that accepts a file upload via multipart/form-data and saves it to an S3 bucket with a generated UUID filename. The answer should include:\n\n- A FastAPI route definition that accepts a file upload (UploadFile) and uses appropriate validation (e.g., max file size of 5 MB).\n- Reading the file content in chunks and streaming it to S3 without loading the entire file into memory.\n- Generation of a unique S3 object key using UUID.\n- Minimal error handling for common failures (e.g., missing file, upload error).\n- Pseudocode or actual code snippet showing the AWS S3 client usage (you can assume credentials are configured).\n\nProvide just the code snippet (no explanations).",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core file upload API skills (multipart, validation, S3 streaming) matching role; minor deduction as FastAPI/S3 not explicitly listed but implied by context (-1). Clarity: Precise scope, constraints, and format ('just code') make it unambiguous and well-scoped. Discriminative: Requires applied integration of FastAPI UploadFile, streaming, UUID, S3 client, and error handling\u2014tests practical skills with trade-offs (memory efficiency), distinguishing strong entry-level candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-403",
        "role_title": "Backend Engineer - Inference Optimization",
        "question": "Question:\nYou have a PyTorch model that you want to deploy with TensorRT for low-latency inference. You can only use a single GPU and must support dynamic input batch sizes at inference time.\n\n- Outline a concise, step-by-step approach to convert the model to TensorRT with minimal latency impact, including handling dynamic batch sizes.\n- Provide a minimal Python snippet that:\n  - Loads a TorchScript or ONNX model.\n  - Creates a TensorRT engine with a dynamic batch dimension.\n  - Performs an inference with a batch size that can vary between 1 and 32.\n- Highlight at least two pitfalls you would watch for (e.g., memory usage, dynamic shapes, or calibration needs) and how you would mitigate them.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core inference optimization skills (TensorRT conversion, dynamic batching, low-latency) matching role's quantization/batching/GPU focus; minor ding as PyTorch/TensorRT not explicitly listed but perfectly aligned (-1). Clarity: Well-scoped steps with precise constraints, unambiguous phrasing. Discriminative: Requires applied knowledge of workflow, code, and trade-offs/pitfalls for strong separation; not mere recall.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-404",
        "role_title": "Senior Backend Engineer - Feature Platform",
        "question": "Question:\n\nYou are designing a simple online feature store surface for a ML team. You have a streaming source of user events (user_id, event_time, event_type) and a batch table of user features (user_id, feature_a, feature_b, as_of_time). The online serving layer must return the latest values of feature_a and feature_b for a given user_id at a requested query_time.\n\n- Outline (in bullets or a short snippet) how you would implement this using Python, considering:\n  - How you would structure the data (e.g., which data stores, keys, and timestamps).\n  - How to ensure consistency between batch features and streaming updates.\n  - How to handle late-arriving events and out-of-order feature updates.\n\n- Provide a minimal code sketch (pseudo-Python) showing:\n  - A function get_features(user_id, query_time) that returns the latest feature_a and feature_b as of query_time.\n  - How you would update the online store with a new streaming event to keep features current.\n\nAssume you are using a simple in-memory structure for the sketch, but describe the data flow and any assumptions you make about Feast-like schemas or typical feature registry considerations.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core feature store skills (online serving, batch/streaming, timestamps) matching role's feature computation/serving/discovery; minor ding for assuming Feast-like without role spec (-1). Clarity: Well-scoped with specific data, clear asks (outline + code), constraints provided. Discriminative: Probes applied knowledge of consistency, late events, point-in-time queries\u2014reveals trade-offs and depth beyond basics.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-405",
        "role_title": "Senior Backend Engineer - Model Monitoring",
        "question": "Question:\nYou have a Python service that serves a simple binary classifier and logs model output to Prometheus metrics: prediction probability (float 0-1) and a binary decision (0/1). Write a short Python snippet (\u226420 lines) that calculates a simple drift indicator by comparing the current batch\u2019s mean prediction probability to a baseline mean, and emits a Prometheus gauge metric named model_drift_score with the absolute difference. Assume:\n- baseline_mean is already computed and available as a float.\n- current_batch_probs is a list of prediction probabilities for the current batch.\n- You already have a Prometheus Gauge named model_drift_score created (from prometheus_client import Gauge) as model_drift_score.\n- You should not rely on external services beyond Prometheus client.\n\nEdge cases: handle empty current_batch_probs gracefully (no emission). Provide a brief note on how you would trigger an alert when drift_score exceeds a threshold in a real system.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 8,
          "reasoning": "Relevance: Directly tests core ML monitoring skills (drift detection via prediction probs, Prometheus metrics) matching role's focus on model monitoring/alerting; minor ding for not specifying exact tech stack but assumes Python/Prometheus reasonably (-1). Clarity: Crystal clear with precise constraints (\u226420 lines, assumptions listed, edge cases specified), unambiguous scope. Discriminative: Tests applied ML ops knowledge (drift calc, metrics emission, edge handling) with good separation; brief alert note adds insight without being recall (-2 for short answer component).",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-413",
        "role_title": "Senior Backend Engineer - API Gateway Logic",
        "question": "Question:\nYou are implementing a simple request transformation in an Envoy-based API gateway using Lua filter. Given an incoming HTTP request with a JSON body and headers, implement a Lua snippet that:\n- Reads the JSON body, extracts the field \"user_id\".\n- Adds a new header \"X-User-ID\" with that value.\n- If \"user_id\" is missing, returns a 400 Bad Request with a JSON body: {\"error\":\"user_id required\"} and stops further processing.\n\nAssume the body is a JSON object and the content-type is application/json. Provide:\n- The Lua code snippet (to be placed in the Envoy Lua filter).\n- A brief explanation of how it ensures 400 response early if missing.\n- Any caveats you\u2019d consider in a real deployment (e.g., streaming bodies, large payloads, idempotency).",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core API gateway skills (request transformation, Lua filters) matching role's Envoy-based extensions (-0 red flags). Clarity: Precise scope, clear requirements, well-constrained for screening. Discriminative: Requires applied Lua/Envoy knowledge, error handling, and real-world trade-offs, strongly separating skilled candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-414",
        "role_title": "Senior Backend Engineer - Long-Polling",
        "question": "Question: \n\nYou are designing a long-polling endpoint in Go that clients poll with a session ID. If there is no new data, the server should hold the request up to a maximum timeout (e.g., 30 seconds) and then respond with either new data or a timeout indication. Implement a concise, self-contained pseudo-implementation or outline (you can describe code blocks or provide small Go snippets) that demonstrates:\n\n- How you would manage per-request lifecycle (context cancellation, timeouts) and avoid goroutine leaks.\n- How you would integrate a lightweight in-memory notification mechanism (e.g., a per-session channel or pub/sub) to wake up the poll when new data becomes available.\n- How you would handle a client disconnect (HTTP connection closed) and ensure resources are released promptly.\n- How you would fall back to a max-timeout response if no data arrives.\nAssume Go, no external dependencies beyond standard library (you may mention Redis as a backend concept but focus on in-process approach for the long-poll). Provide a brief explanation of the trade-offs.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (long-polling APIs, connection mgmt, timeouts, fallbacks) in Go with exact tech match; minor Redis mention is conceptual only (-0). Clarity: Crystal clear, well-scoped with specific bullets, constraints (std lib, concise), no ambiguity. Discriminative: Requires applied Go knowledge (context, channels, goroutines, leaks), trade-offs distinguish deep understanding; not trivial recall, appropriately scoped for screening.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-411",
        "role_title": "Backend Engineer - Admin APIs",
        "question": "Question:\nDesign a small FastAPI endpoint that allows an admin to deactivate a user account. The system uses PostgreSQL for user data and Redis for short-lived admin session tokens. Provide a concise code snippet (you can assume SQLAlchemy-like ORM and Redis client) that:\n\n- Validates that the request includes a valid admin session token in a header (e.g., X-Admin-Token).\n- Checks that the token is present in Redis with a non-expired TTL.\n- Deactivates the user by setting an is_active flag to False in PostgreSQL for a given user_id path parameter.\n- Writes a simple audit log entry to stdout (or a provided audit_log function) with: timestamp, admin_id, action=\"deactivate_user\", target_user_id.\n\nAssumptions you can make:\n- User model has id (int) and is_active (bool).\n- Redis client is available as redis_client and stores tokens as key: \"admin:token:<token>\" with value containing admin_id.\n- The function should be self-contained within a FastAPI route handler, focusing on the flow rather than full app setup.\n\nProvide the code snippet.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests FastAPI, PostgreSQL, Redis, auth validation, audit logging - core to admin APIs (minor -1 for assuming ORM/Redis client detail). Clarity: Precise scope, clear requirements, well-constrained for screening. Discriminative: Requires integrating auth, DB ops, Redis TTL check, auditing with error handling - tests applied role skills and trade-offs effectively.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-417",
        "role_title": "Junior Backend Developer - Event Sourcing",
        "question": "Question:\nYou are building a simple event-sourced order service. An Order can be created and then each item addition is represented as an event. There is a separate Read Model (projection) that maintains the total order value.\n\nGiven the following events:\n- OrderCreated { orderId: \"A1\", customerId: \"C1\", createdAt: 12345 }\n- ItemAdded { orderId: \"A1\", productId: \"P1\", price: 10, quantity: 2, occurredAt: 12350 }\n- ItemAdded { orderId: \"A1\", productId: \"P2\", price: 5, quantity: 1, occurredAt: 12360 }\n\nDescribe:\n1) A simple in-memory projection update method that keeps track of the total order value (sum of price * quantity) when ItemAdded events are received for an order.\n2) How you would handle the case where ItemAdded for a non-existent order is received (e.g., ensuring idempotency and consistency).\n3) The minimal state you would store per order in the projection to answer: \"What is the current total value for order A1?\", and why.\n\nProvide code-like pseudocode or concise Java-ish snippets to illustrate your solution. Limit to 2-5 minutes of reading time.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core event sourcing skills (event handlers, projections, aggregate state) for the role; minor ding for not explicitly covering full aggregate state (-1). Clarity: Crystal clear with specific events, scoped tasks, time limit, and pseudocode request; no bundling or ambiguity. Discriminative: Requires applied knowledge of projections, idempotency, trade-offs in state minimization; distinguishes understanding from rote recall.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-416",
        "role_title": "Staff Backend Engineer - External APIs",
        "question": "Question:\nYou are designing a new external REST API for partner integrations. You have a Go backend exposing endpoints behind Kong API Gateway, with OpenAPI documentation. Partners must be able to access a limited set of resources, with per-partner rate limits and a simple monetization model (first 1000 requests per month free, then throttle).\n\nDescribe, at a high level, the minimal implementation plan to support:\n- per-partner authentication and identification (without modifying backend business logic)\n- per-partner rate limiting with a 1-second burst cap and a monthly quota\n- clean exposure of an OpenAPI spec through Kong, including versioning and deprecation strategy\n- a lightweight monetization hook that counts usage and enforces the quota (without heavy infra)\n\nIn your answer, outline:\n- which components you would implement or configure (Go middleware, Kong plugins, OpenAPI changes, Kubernetes considerations)\n- the data model (how you identify a partner, store usage, and plan)\n- high-level flows for a single API call from partner to backend\n- how you would test this end-to-end in 2\u20135 minutes of hands-on work (minimal script or command steps)\n\nKeep it concise and concrete.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core role skills (API strategy, monetization, partner enablement, integrations) with exact tech (Go, Kong, OpenAPI); minor ding for no Kubernetes mention in role. Clarity: Crystal clear, well-scoped with explicit requirements and outline; no ambiguity or bundling issues. Discriminative: Tests deep applied knowledge of auth, rate limiting, versioning, monetization trade-offs, plus practical testing; strong separation via implementation choices and flows.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-418",
        "role_title": "Junior Backend Engineer - Service Clients",
        "question": "Question:\n\nYou are implementing a simple gRPC client in Go that calls a remote UserService.GetUser(ctx, req). The service is flaky: sometimes it returns gRPC errors with codes Unavailable or DeadlineExceeded. You want to implement a minimal retry with backoff and a circuit breaker using a counter in memory.\n\n- Constraints:\n  - Implement a function getUserWithRetry(code: string) -> (User, error) that:\n    - Retries up to 3 times on Unavailable or DeadlineExceeded.\n    - Uses exponential backoff starting at 100ms, max 1s.\n    - If there have been 3 consecutive failures (on any of those errors), trip a simple in-memory circuit breaker for 5 seconds, during which all calls fail fast with a specific error ErrCircuitOpen.\n    - After the circuit breaker is open for 5 seconds, the next attempt should be allowed to try once (half-open), and if it succeeds, the circuit closes; if it fails, it reopens.\n  - You can assume the gRPC client call is: resp, err := userClient.GetUser(ctx, &GetUserRequest{Id: reqId})\n  - You must not use external libraries; implement the retry and circuit-breaker logic with standard Go primitives.\n  - The function should be safe for concurrent use (multiple goroutines can call it). Use a simple mutex to guard shared state.\n\n- Provide:\n  - A concise Go code snippet implementing getUserWithRetry as described, including necessary types and state, and comments explaining the behavior.\n\nNote: You do not need to implement the entire gRPC client or server. Focus on the retry/backoff and circuit breaker logic around the given call.",
        "scores": {
          "relevance": 10,
          "clarity": 10,
          "discriminative": 10,
          "reasoning": "Perfect match to role: directly tests entry-level implementation of retries, exponential backoff, and in-memory circuit breaker in Go for gRPC clients. Crystal clear with precise constraints, scope, and concurrency requirements. High discriminative power tests applied knowledge, trade-offs (e.g., half-open state, mutex safety), and clean stdlib implementation\u2014no red flags.",
          "composite": 10.0
        }
      },
      {
        "role_id": "be-420",
        "role_title": "Backend Engineer - Graceful Shutdown",
        "question": "Question: You are running a Go gRPC service behind Kubernetes. When Kubernetes sends a SIGTERM for a pod during a rolling update, implement a minimal graceful shutdown flow that:\n\n- stops accepting new gRPC requests\n- drains in-flight requests within 2 seconds, then shuts down\n- ensures the Pod exits with a non-zero code only if there are undrained requests after the timeout\n\nProvide a concise code snippet (Go) showing:\n- how you listen for OS signals and trigger shutdown\n- how you stop accepting new connections without closing existing ones\n- how you track in-flight RPCs (hint: use a sync.WaitGroup or similar)\n- how you use context with a timeout to enforce the 2-second drain window\n\nAssume a standard gRPC server setup; focus on the shutdown logic and integration points.",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Relevance: Perfectly matches role's core focus on graceful shutdown, connection draining, signals, and zero-downtime deploys in exact context (Go gRPC + K8s). No red flags.\nClarity: Crystal clear, well-scoped to minimal code snippet with explicit requirements and integration points; assumes standard setup without ambiguity.\nDiscriminative: Requires deep applied knowledge of Go concurrency (WaitGroup), gRPC server lifecycle (GracefulStop), signals, contexts/timeouts, and trade-offs in pod exit codes\u2014distinguishes strong engineers exceptionally well.",
          "composite": 9.666666666666666
        }
      },
      {
        "role_id": "be-415",
        "role_title": "Senior Backend Engineer - Multipart APIs",
        "question": "Question:\n\nDesign a minimal Python FastAPI endpoint (just the handler logic, no full app is required) that supports resuming a large file upload using chunked multipart uploads. Assume:\n\n- Each chunk is uploaded via POST /upload/{upload_id}/chunk with JSON body: { \"chunk_index\": int, \"chunk_data\": \"<base64-encoded bytes>\" }.\n- Uploads are tracked in memory with a simple dict: uploads[upload_id] = { \"total_chunks\": int, \"received\": {chunk_index: data_bytes}, \"complete\": bool }.\n- When a chunk is received, store it in the in-memory map, and return a JSON response indicating the current progress: { \"uploaded_chunks\": n, \"total_chunks\": m, \"complete\": bool }.\n- If all chunks have been received, set complete = True and return { \"uploaded_chunks\": m, \"total_chunks\": m, \"complete\": true }.\n- You may assume the client knows total_chunks from a separate initialization step (not part of this question). For simplicity, you can ignore authentication, disk writes, and persistence.\n\nTask:\n- Provide the FastAPI endpoint function (including router decorator) that handles POST /upload/{upload_id}/chunk.\n- Include minimal in-function in-memory state management to track chunk receipt and completion.\n- Include basic input validation (e.g., chunk_index non-negative, chunk_data present).\n- Return the progress as described.\n\nNote: Focus on correctness and clarity of the chunk-resume logic suitable for a senior backend engineer.",
        "scores": {
          "relevance": 9,
          "clarity": 10,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (resumable chunked uploads, multipart handling) with exact FastAPI/Python stack (-0 red flags). Clarity: Crystal clear specs, precise constraints, unambiguous endpoint/task (-0 red flags). Discriminative: Requires applied FastAPI knowledge, state management, validation, resumption logic with trade-offs (e.g., in-memory dict handling), distinguishes seniors effectively (not recall, ~5-8min answer).",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-412",
        "role_title": "Backend Engineer - Analytics APIs",
        "question": "Question:\nYou are designing a simple analytics API endpoint in Go that returns daily user activity counts for the last 7 days. Data is stored in ClickHouse with a table schema:\n- events(event_id UUID, user_id UUID, event_time DateTime, event_type String)\n\nRequirements:\n- The API should return a JSON object: { \"dates\": [\"YYYY-MM-DD\", ...], \"counts\": [int, ...] } for each of the last 7 days in UTC.\n- Use a single ClickHouse query to fetch the counts per day, even if some days have zero events.\n- Handle the case where there are days with no events by returning 0 for those days.\n- The API should be implemented as a function signature in Go (no need for full server setup): fetchLast7DaysActivity(db *sql.DB) ([]string, []int, error)\n\nProvide:\n1) The Go function body implementing the query, data mapping, and handling of zero-event days.\n2) The ClickHouse SQL you would use inside the query (as a string), with appropriate functions to generate the last 7 dates and left-join to ensure zero counts for missing days.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (analytics APIs, ClickHouse aggregation, time-series) with exact tech stack; minor deduction as Go web framework not specified (-1). Clarity: Precise scope, clear requirements, function signature, and deliverables; no bundling or ambiguity. Discriminative: Requires applied knowledge of ClickHouse date generation/left-join for zeros, Go DB integration, and edge cases\u2014strong separation of strong vs weak candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-419",
        "role_title": "Backend Engineer - Retry Logic",
        "question": "Question:\n\nYou are designing a retry helper in Go for distributed operations with the following constraints:\n- Exponential backoff with a maximum cap.\n- Add jitter to avoid thundering herd.\n- Implement a simple retry budget: each operation has a budget that allows up to N retries per window (e.g., per minute). If the budget is exhausted, no further retries should be attempted until the window resets.\n- The question focuses on implementing a single function that determines whether a retry should proceed and computes the next backoff duration.\n\nTask:\nImplement a Go function with the signature:\n\ntype BackoffConfig struct {\n    MinDelay   time.Duration // base delay\n    MaxDelay   time.Duration // saturation cap\n    BackoffFactor float64     // e.g., 2.0\n    Jitter     time.Duration  // max jitter to apply\n    Budget     int              // retries allowed per window\n    Window     time.Duration    // window length for the budget\n}\n\ntype BackoffState struct {\n    Attempts   int\n    StartWindow time.Time\n}\n\nfunc ShouldRetry(cfg BackoffConfig, st *BackoffState, err error) (bool, time.Duration)\n\nBehavior requirements:\n- If err is nil, return (false, 0).\n- Maintain per-operation state (Attempts, window start). If the current time is past StartWindow+Window, reset Attempts to 0 and StartWindow to now.\n- Compute exponential backoff: delay = minDelay * (BackoffFactor)^(Attempts-1). If this exceeds MaxDelay, cap at MaxDelay.\n- Apply jitter: finalDelay = delay + random between [-Jitter, +Jitter].\n- Before returning true, ensure that the budget permits another retry: If Attempts >= Budget, return (false, 0) without updating state. If allowed, increment Attempts and return (true, finalDelay).\n- Ensure the random jitter uses a deterministic source if a seed is provided (you can sketch or describe how to inject a rand.Rand or use crypto/rand for safety).\n\nProvide a concise Go implementation snippet that compiles and demonstrates the core logic. No need for full package or tests, just the function and necessary types.",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Relevance: Perfectly matches role's core requirements (retry infra with backoff, jitter, budgets) using Go; no red flags. Clarity: Crystal clear signature, precise behavior specs, well-scoped for screening; minor jitter randomness detail is unambiguous. Discriminative: Requires deep applied knowledge of retry patterns, state management, concurrency safety, and Go idioms, forcing trade-offs in jitter/crypto impl; excellent separation.",
          "composite": 9.666666666666666
        }
      },
      {
        "role_id": "be-426",
        "role_title": "Junior Backend Engineer - DevOps Scripting",
        "question": "Question:\n\nYou need to create a small script in Python (3.x) that will be used in a CI environment to prepare a deployment workspace. The script should:\n\n- Create a directory named deploy_workspace in the current working directory.\n- Within that directory, create two files:\n  - config.json containing {\"env\":\"production\",\"version\":\"0.1.0\"}\n  - run.sh which prints \"Starting deployment...\" when executed.\n- Ensure the script is safe to run multiple times (idempotent) and will not overwrite existing files without a prompt (you can simulate the prompt by checking if files exist and exiting with a non-zero status if they already exist).\n- Make run.sh executable from the Python script after creation.\n\nProvide the Python script that accomplishes this. It should be concise (2-4 lines of Python code or a small, readable block) and work in a typical Linux environment.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 8,
          "reasoning": "Relevance: Directly tests Python scripting for CI/DevOps automation, idempotency, and Linux env setup\u2014core to role (-1 minor for generic JSON/shell vs specific pipelines). Clarity: Precise scope, constraints, and expectations; well-formed for screening. Discriminative: Tests applied Python (file ops, JSON, chmod, conditionals); distinguishes practical scripting skills from theory.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-424",
        "role_title": "Backend Engineer - Message Serialization",
        "question": "Question:\nYou have an Avro-encoded message schema updated from a current version to a new version. The old schema (v1) is used by producers, and the new schema (v2) is used by consumers. The field \"user_id\" (string, required) exists in v1 but has been renamed to \"user_identifier\" (string, optional) in v2, and a new optional field \"timestamp\" (long) has been added in v2.\n\nAssume the Avro schema is registered in Schema Registry with compatibility set to Backward and you cannot change existing producers/consumers logic.\n\na) Describe how you would configure the Avro schemas and the serialization/deserialization to ensure backward compatibility. Specifically address: schema naming/versioning, field defaults, and how a consumer using v2 reads messages produced with v1.\n\nb) Given an actual v1 message: {\"user_id\":\"alice\"} with only the required field, explain what happens when a v2-aware consumer deserializes it. What will be the value of user_identifier and timestamp in the deserialized object?\n\nc) If you instead configure compatibility to Forward, what changes (if any) would you need to make to the v1 producer or v2 consumer to successfully deserialize messages?",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Relevance: Directly tests core schema evolution, backward/forward compatibility, and Avro specifics from role (-1 minor for assuming Schema Registry not explicitly listed). Clarity: Well-structured with clear parts a/b/c, precise scenario, no ambiguity. Discriminative: Excellent at separating via applied Avro knowledge, trade-offs, and precise prediction of deserialization behavior with defaults/missing fields.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-423",
        "role_title": "Staff Engineer - Incident Response Platform",
        "question": "Question:\nDescribe a minimal Go function design for collecting and tracking an on-call incident in a PostgreSQL-backed SLO-aware system. Specifically:\n\n- Outline a Go function that receives an incident event (e.g., alert triggered, acknowledged, resolved) and updates a PostgreSQL table with at least: incident_id, status, severity, oncall_message, start_time, end_time, and an updated_slo_status.\n- Include what SQL tables and indexes you would create to support fast lookups for active incidents and SLO status, plus a brief note on how you would compute a simple SLO breach (e.g., % incidents resolved within target time).\n- Provide a concise code sketch (only the function body and essential types) showing:\n  - how you would upsert the incident record,\n  - how you would update an on-call message (Slack/PagerDuty) if needed,\n  - how you would update a derived SLO status field.\n\nAssume you have:\n- Go with a PostgreSQL driver (pgx or database/sql),\n- Basic incident events: { incident_id string, event string: \"triggered\" | \"acknowledged\" | \"resolved\", severity string, started_at, resolved_at },\n- A single table incidents(id, status, severity, oncall_message, started_at, resolved_at, slo_status),\n- An external helper to send Slack/PagerDuty updates via a message string.\n\nProvide linkage comments on how this would fit into an event-driven flow and emphasize correctness for staff-level reliability focus.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests Go, PostgreSQL, and SLO/incident tracking core to role; no red flags (-0). Clarity: Well-scoped with specifics, assumptions, and constraints; single focused question (-0). Discriminative: Requires applied design, trade-offs (indexes, upsert, SLO calc), code sketch, and system integration; strong separation for staff-level skills (-0).",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-421",
        "role_title": "Senior Backend Engineer - Security Automation",
        "question": "Question:\nYou have a Python script that reads a list of AWS IAM roles and their attached policies, identifies roles with attached policies granting broad access (e.g., actions: \"*\", or Resource: \"*\"), and prints the role ARN for remediation. The script should also avoid flagging roles that are explicitly exempted via a provided EXEMPT_ROLES list.\n\nGiven:\n- A JSON file roles.json with structure:\n  [\n    {\"role_name\": \"app-readonly\", \"arn\": \"arn:aws:iam::123456789012:role/app-readonly\", \"policies\": [{\"name\": \"ReadOnlyPolicy\", \"statements\": [{\"Effect\": \"Allow\", \"Action\": [\"ecs:*\"], \"Resource\": \"*\"}]}]},\n    ...\n  ]\n- A Python function is_broad_access(stmt) that returns True if a statement grants broad access (Action contains \"*\" or Resource contains \"*\"), and False otherwise.\n- A set EXEMPT_ROLES = {\"arn:aws:iam::123456789012:role/infra-maintenance\"}\n\nWrite the core logic (2-5 minutes) to:\n- Load roles.json\n- For each role, skip if ARN is in EXEMPT_ROLES\n- For each statement in each policy, use is_broad_access to detect broad access\n- If any statement is broad, print the role ARN (one per line)\n\nProvide only the essential Python snippet (no file-walking or error handling boilerplate beyond the minimal necessary), assuming roles.json is in the current directory.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 8,
          "reasoning": "Relevance: Directly tests Python scripting for AWS IAM policy analysis, core to security automation pipelines (-1 for no red flags). Clarity: Precise structure, constraints, and scope (2-5 min, essential snippet only). Discriminative: Tests practical data processing, iteration, conditionals, and JSON handling with exemptions; distinguishes applied Python skills (-2 for not probing edge cases or is_broad_access nuances).",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-425",
        "role_title": "Senior Backend Engineer - Graph Analytics",
        "question": "Question:\nYou have a social network graph stored in Neo4j where nodes are users and edges are \u201cFOLLOWS\u201d relationships with a property weight representing interaction strength (0.0 to 1.0). You want to build a lightweight influence score for a given user U by considering the top-3 most influential followers (neighbors who follow U directly) where influence is defined as the neighbor\u2019s own weighted out-going engagement score. The engagement score for a user N is the average of weights on all edges from N to their followers (i.e., N\u2019s followers are the incoming neighbors to N if you think in graph terms, but you\u2019ll compute from N to their followees). In Cypher, outline (pseudo-code is fine) how you would:\n\n- Retrieve the top-3 followers of U by the follower\u2019s own engagement score (computed as average weight of edges from that follower to their own followees).\n- Return U\u2019s influence score as the sum (or average) of those top-3 followers\u2019 engagement scores, limited to 3.\n\nAssume:\n- Graph uses relationships :FOLLOWS with property weight on both follower->followee edges.\n- U is a parameter userId.\n- You can use APOC or user-defined procedures if needed, but keep it concise. Provide the Cypher query and brief explanation of each major step.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests Neo4j/Cypher for graph traversal and analytics at scale, core to role's graph pipelines and billion-edge optimization (-1 for not explicitly mentioning scale handling). Clarity: Well-scoped with precise definitions, assumptions, and output requirements. Discriminative: Requires multi-hop traversal, aggregation, ranking, and Cypher fluency to distinguish strong graph DB experts from generalists.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-422",
        "role_title": "Junior Backend Engineer - API Testing",
        "question": "Question:\nYou have a FastAPI endpoint that creates a new user and returns the created user object with status 201. Write a pytest test function that:\n- sends a POST request to /users with JSON body {\"name\": \"Alice\", \"email\": \"alice@example.com\"}\n- asserts the response status is 201 and the response contains an id, name, and email matching the request\n- uses a TestClient from fastapi.testclient\n- uses a simple in-memory fixture for the app (no real DB)\n\nAssume the FastAPI app instance is named app and defined as:\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\n\napp = FastAPI()\n\nclass User(BaseModel):\n    id: int\n    name: str\n    email: str\n\n@app.post(\"/users\", response_model=User, status_code=201)\ndef create_user(user: User):\n    # pretend to save and assign an id\n    return User(id=1, name=user.name, email=user.email)\n\nProvide the complete test function code snippet including any necessary imports and fixture.",
        "scores": {
          "relevance": 9,
          "clarity": 10,
          "discriminative": 8,
          "reasoning": "Relevance: Directly tests FastAPI, pytest, and TestClient for API testing role (-1 for not covering fixtures/advanced patterns explicitly). Clarity: Crystal clear with exact requirements, code context, and constraints. Discriminative: Tests practical applied knowledge of testing FastAPI endpoints, distinguishing those who can write functional tests from those who can't; not trivial but not deeply trade-off heavy.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-428",
        "role_title": "Senior Backend Engineer - Payment Orchestration",
        "question": "Question:\nDescribe and sketch a simple Go function (or small module) for routing a payment request to multiple providers with failover. The function should:\n\n- Accept a payment request (amount, currency, customerId) and a list of provider endpoints with reliability weights.\n- Attempt to process the payment through the highest-weight provider first, and on failure, automatically retry with the next provider in the prioritized list, up to all providers.\n- Include how you would record a lightweight success/failure metric (e.g., using Redis or in-memory map) to influence future routing decisions.\n- Be mindful of idempotency: how would you avoid duplicate charges if a retry occurs?\n- Provide pseudo-Go code (or real Go code snippets) and brief explanation of the core logic and edge cases.\n\nWhat would your routing function look like, and what are the key failure/edge cases you would handle?",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core payment routing, failover, metrics, and idempotency from role (-1 minor for not explicitly mentioning cost optimization). Clarity: Well-scoped with specific inputs/requirements, clear pseudo-code expectation, no bundling. Discriminative: Requires applied Go knowledge, trade-offs (weights, metrics, idempotency), edge cases; strong separation of strong vs weak candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-427",
        "role_title": "Backend Engineer - Feature Store",
        "question": "Question:\n\nYou are building a simple feature store for real-time ML inference. You have a feature \u201cuser_age_bucket\u201d that should be computed from a user table with fields: user_id (string), birth_year (int), and a streaming events table with events (user_id, event_time). The feature should be a bucketed age group (e.g., 0-17, 18-24, 25-34, \u2026) based on the current date, and it must be available for online serving within a small freshness window (e.g., last 24 hours). The offline training will use a batch dataset with the latest computed bucket per user as of train_time.\n\nTask:\n- Describe a compact approach (data model + computation flow) to compute and serve this feature using Python and typical components (e.g., Feast-like feature registry, Redis for online store, PostgreSQL for offline store, and Kafka for streaming).\n- Include:\n  - How you would define the feature in the feature registry (name, type, dependencies).\n  - How you would compute and update the online feature value when new user_birth_year or user events arrive.\n  - How you would ensure the online feature is consistent with offline training (training with a fixed timestamp notion).\n- Provide a minimal code snippet (pseudo-Python is fine) showing:\n  - a small feature definition, and\n  - a simple online-to-offline sync step that would update the online store.\n\nAssumptions:\n- You may use Python-like pseudocode and simple function signatures.\n- Do not rely on external libraries beyond standard data structures; focus on the logic.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core feature store skills (computation, versioning, real-time serving) matching role; minor ding for assuming specific tools (Feast/Redis) not in desc (-1). Clarity: Crystal clear with precise scope, constraints, and deliverables; assumptions resolve ambiguities. Discriminative: Requires applied ML infra knowledge, trade-offs in sync/consistency, and code logic; strong separation of capable candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-429",
        "role_title": "Backend Engineer - Log Aggregation",
        "question": "Question:\nYou are building a log ingestion path in Go that reads raw log lines from Kafka, parses them into a structured event, and writes to ClickHouse for long-term storage. The pipeline must handle at least 1e6 events/second with minimal tail latency and support simple enrichment (e.g., adding hostname and a processed_at timestamp). Describe a concrete, minimal design (high-level architecture and key data structures) and provide a short Go snippet that:\n\n- Consumes messages from a Kafka topic (use a mock interface if you prefer)\n- Parses a log line in the following format: [LEVEL] 2024-07-01T12:34:56Z hostname: message content\n- Enriches with hostname and processed_at\n- Writes a structured row to ClickHouse using a batch write approach\n\nWhat considerations would you make to ensure low latency and high throughput (e.g., batching, concurrency, backpressure), and how would you handle potential parsing errors without dropping valid events? Provide the core code snippet (parsing + enrichment + batch assembly) and a brief note on how you\u2019d integrate with ClickHouse in production.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core log pipeline skills (Go, high-throughput processing, parsing/enrichment) matching role's millions/sec focus; minor deduction as ClickHouse/Kafka specifics unconfirmed but highly relevant (-1). Clarity: Crystal clear scope, format, constraints, and expectations; well-scoped for screening. Discriminative: Requires applied design (architecture, trade-offs like batching/backpressure), concrete code, error handling; strong separation of deep vs shallow candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-430",
        "role_title": "Junior Backend Engineer - Search Indexing",
        "question": "Question:\n\nYou have a PostgreSQL table \"articles\" with columns id (int), title (text), body (text), and published_at (timestamp). You want to build a simple Python job that indexes each article into Elasticsearch under an index named \"articles\". The index should map the fields as:\n- title: text with.Keyword subfield for exact matches\n- body: text\n- published_at: date\n\nTask:\n1) Write a concise Python snippet (using any Elasticsearch Python client) that:\n   - Connects to Elasticsearch\n   - Creates the index \"articles\" with the above mapping (ignore if it already exists)\n   - Iterates through a small batch (e.g., 100 at a time) of rows from PostgreSQL (you can mock the data with a list of dicts if you prefer) and bulk indexes them into Elasticsearch\n2) Show how you would structure the bulk request payload for Elasticsearch\n3) Briefly explain how you would adjust the mapping if you wanted to support a simple \"search in title\" use case with a fast exact match option.\n\nTimeboxed: answer in ~2\u20135 minutes. Focus on practical correctness and clear code structure.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core search indexing skills (Elasticsearch mapping, bulk indexing) matching role focus (-1 for minor PostgreSQL assumption not in role). Clarity: Precise scope, clear tasks, timebox, mock data option. Discriminative: Requires applied knowledge of mappings, bulk API structure, trade-offs (text+keyword), distinguishes practical skills from trivia.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-434",
        "role_title": "Principal Engineer - Database Strategy",
        "question": "Question: In a multi-region deployment using PostgreSQL and Redis, you must ensure low-latency reads, strong consistency for critical writes, and cost-effective failover. Describe a concrete, step-by-step plan for: \n\n- data placement and replication strategy across regions (including any role of PostgreSQL vs Redis, and how you handle consistency guarantees). \n- a failover scenario during regional outage (what components failover to which regions, how do you minimize data loss, and how do you verify correctness after failover). \n- metrics and monitoring you would track to maintain operational excellence, and a brief outline of a dark/chaos test you would run to validate the plan. \n\nKeep your answer high-level but with specific configuration choices or trade-offs you would consider (e.g., synchronous vs asynchronous replication, read replicas, Redis caching patterns, Kafka as a change data capture pipe).",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Relevance: Directly tests core multi-region database strategy, replication, failover, and ops excellence; matches PostgreSQL/Redis mention with perfect role alignment (-0 red flags). Clarity: Precise scope, well-structured subpoints, clear high-level depth with examples; no ambiguity or bundling issues. Discriminative: Requires deep applied knowledge of trade-offs (sync/async, caching, CDC), planning, and chaos testing; excellent separation of strong vs weak candidates.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-435",
        "role_title": "Junior Backend Engineer - Queue Consumers",
        "question": "Question:\nYou are implementing a simple RabbitMQ consumer in Python that processes messages from a queue named \"orders\". Each message contains an order_id. The processing is idempotent and can be retried safely, but you must ensure a message is not reprocessed if it has already been completed successfully.\n\nIn pseudo-code or Python-like code, sketch a minimal consumer loop that:\n- Acks a message only after successful processing\n- Uses a PostgreSQL table \"processed_orders\" to track completed order_ids\n- Handles at-least-once delivery by detecting duplicates and rejecting/acking appropriately\n- Includes a basic error path that requeues on transient failure and routes permanently failed messages to a dead-letter queue (dlx) named \"orders.dlq\"\n\nProvide:\n- The core logic (snippets) for consuming, checking the database for duplicates, committing the transaction, and acknowledging/nacking messages\n- Brief explanation of how idempotency is achieved in this setup\n\nNote: You don\u2019t need to provide full runnable code or boilerplate, just the essential control flow and database interactions.",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role (RabbitMQ consumers, ack patterns, dead letter handling, idempotency, PostgreSQL); no red flags. Clarity: Crystal clear scope, explicit requirements, well-constrained for screening; minor -1 for lacking exact ack/nack API details but note mitigates. Discriminative: Tests applied knowledge of core patterns (idempotency via DB, at-least-once, DLQ), trade-offs in ack/requeue/nack; strong separation without being overly broad.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-432",
        "role_title": "Backend Engineer - Data Validation",
        "question": "Question:\n\nYou have a Python function that processes a data batch and writes results to a PostgreSQL table. After processing, you want to quickly validate two things using Great Expectations (GE) before the batch is considered successful:\n\n1) The output table should have no NULLs in the columns: user_id (int, non-null), event_type (str, non-null), and event_timestamp (datetime, non-null).\n2) The mean value of a numeric column revenue should be within [0, 1000] for the batch.\n\nWrite a small GE validation script snippet (using a Batch and a simple Expectation Suite or inline expectations) that:\n- Creates a batch of the target PostgreSQL table\n- Applies the two checks above\n- Reads the results and returns a boolean indicating whether the batch passed validation\n\nAssume:\n- SQLAlchemy/PostgreSQL connection is available as an engine object named engine\n- The target table is named public.events_YYYYMMDD\n- You can instantiate a GE context in memory (no need to persist a suite)\n\nProvide only the code snippet and brief explanation (1\u20132 sentences).",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Directly tests core data validation skills (schema/NULL checks, statistical ranges) using Great Expectations on PostgreSQL, matching role's pipeline quality focus; crystal clear with precise scope, constraints, and assumptions; requires applied GE knowledge and trade-offs (inline vs suite), distinguishing strong candidates without being trivial or overly broad.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-431",
        "role_title": "Staff Engineer - Real-time Collaboration",
        "question": "Question:\nYou\u2019re designing a real-time text editor using CRDTs and operational transforms. Describe a concise approach to handling concurrent edits from two users that arrive almost simultaneously over WebSocket connections. Specifically:\n- How would you model the incoming operations (CRDT vs OT) and their ordering?\n- What is your strategy to resolve conflicts and maintain a consistent document state across clients and the server?\n- Outline minimal data structures (types or interfaces) you would define to represent an operation and an edit batch, and how you would apply them on both server and client.\n- Briefly explain how presence information and vector clocks (or causal metadata) integrate with your approach to ensure correct synchronization.",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Relevance: Perfectly matches role's core focus on real-time collaboration, conflict resolution, presence, and operational transforms; directly tests CRDT/OT which are central. Clarity: Crystal clear, well-scoped subpoints with specific asks, no ambiguity or bundling issues. Discriminative: Excellent at separating experts (requiring deep applied knowledge of trade-offs, data modeling, integration) from superficial knowers; tests synthesis over recall.",
          "composite": 9.666666666666666
        }
      },
      {
        "role_id": "be-433",
        "role_title": "Senior Backend Engineer - Platform APIs",
        "question": "Question:\nYou are designing a simple internal platform API to provision \"compute\" resources for developers. The API should be a gRPC service with two methods:\n\n- CreateResource(CreateResourceRequest) -> CreateResourceResponse\n- GetResource(GetResourceRequest) -> GetResourceResponse\n\nRequirements:\n- Each resource has: id (string), owner (string), type (\"compute\"), and status (\"provisioning\" | \"ready\" | \"error\"). The resource state should be stored in PostgreSQL.\n- Provisioning is simulated: when CreateResource is called, the resource is inserted with status \"provisioning\", and after a short async delay (you can describe the approach, not implement), it becomes \"ready\" or \"error\" based on an arbitrary condition you define (e.g., if owner contains \"fail\", it becomes \"error\"; otherwise \"ready\").\n- Use Redis to debounce duplicate CreateResource requests by id within a 1-minute window (idempotency key).\n- Provide a concise Go interface (protobuf-like) for the service and the messages, and outline the core logic for handling idempotency, persistence, and the provisioning state transition without full boilerplate.\n\nTask:\n- Sketch the proto definitions (messages and service) and provide a brief high-level Go pseudo-implementation outline for CreateResource, focusing on:\n  - idempotent handling with Redis\n  - persisting initial state to PostgreSQL\n  - queuing/triggering the asynchronous state transition\n  - id, owner, and status fields management\n\nYou can use pseudo-code or simplified snippets. Explain your approach in 2\u20134 short bullets.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core API design, gRPC, PostgreSQL, Redis, async workflows, and Go for internal platform provisioning (-1 for not explicitly listing gRPC/Redis in role but perfect conceptual match). Clarity: Crystal clear scope, precise requirements, focused task with pseudo-code allowance. Discriminative: Requires applied knowledge of idempotency, async patterns, DB integration, and trade-offs in real-time systems, strongly separates strong candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-436",
        "role_title": "Backend Engineer - API Documentation",
        "question": "Question: You\u2019re building an API documentation tool that auto-generates OpenAPI specs for a Node.js/TypeScript backend. You have an endpoint:\n\nPOST /users\nRequest body:\n{\n  \"name\": string (required),\n  \"email\": string (required, unique),\n  \"age\": number (optional, minimum 0)\n}\nResponse:\n201 Created with body:\n{ \"id\": string, \"name\": string, \"email\": string, \"age\": number | null }\n\na) Sketch how you would implement a small OpenAPI 3.0 spec snippet for this endpoint, including request body schema, response schema, and a 400 error if email is invalid. Show at least one example value.\n\nb) Describe how you would ensure this spec stays in sync with a TypeScript type used in your Express route handler, and provide a minimal code snippet illustrating the mapping between the TS type and the OpenAPI schema (no external tools).",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests OpenAPI spec generation, TypeScript/Node.js integration, and sync mechanisms\u2014core to API doc tooling role (-1 for not explicitly mentioning versioning/SDK but highly on-target). Clarity: Precise endpoint details, scoped parts a/b, clear expectations without ambiguity. Discriminative: Requires schema authoring, error handling, TS-OpenAPI mapping with code\u2014tests practical applied knowledge and trade-offs in type safety/sync, strongly separates skilled candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-439",
        "role_title": "Backend Engineer - Billing Integration",
        "question": "Question:\nYou are implementing a monthly subscription with Stripe. A customer downgrades from Pro to Basic mid-cycle, and prorations should apply to the current period. Describe (and sketch code) how you would:\n\n- compute the prorated credit for the remaining days of the current period\n- apply that credit to the next invoice rather than issuing a separate credit note\n- ensure idempotency and avoid duplicate proration when a retry occurs\n\nAssume:\n- Python backend, Stripe API, PostgreSQL for state\n- You have the Stripe customer_id, current_subscription_id, current_period_end, and plan prices\n- The system uses Celery for async tasks and Redis for idempotency keys\n\nWhat would your approach be, and provide a compact code snippet illustrating the core Stripe calls and idempotency check.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core role skills (proration, subscription changes) with exact tech stack (Stripe, Python); minor ding for not mentioning PostgreSQL usage explicitly (-1). Clarity: Precise scope, clear asks, well-defined assumptions, compact code request. Discriminative: Tests deep applied knowledge of Stripe proration/idempotency/tradeoffs, separates strong engineers; not trivial recall.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-437",
        "role_title": "Senior Backend Engineer - Secrets Management",
        "question": "Question:\nYou are designing a Go service that rotates database credentials stored in HashiCorp Vault every 24 hours. The credentials are used by a PostgreSQL client in your app, and you must ensure the app always uses the latest credentials without restarting the process.\n\n- Outline a minimal design with:\n  - How you would fetch and refresh credentials from Vault in a running Go application.\n  - How you would propagate rotated credentials to the PostgreSQL client without downtime.\n  - A brief note on handling encryption-at-rest and audit considerations in Vault for these credentials.\n\n- Provide a concise pseudocode snippet (or real Go-ish code) showing:\n  - Initialization (initial fetch and cache setup).\n  - A background refresh loop that updates the in-memory credentials.\n  - A safe way to replace the credentials for new DB connections while existing connections are closed gracefully or reused.\n\nAssume: Vault is using AppRole authentication, credentials have a short TTL, and you use a standard Go PostgreSQL driver. The answer should reflect practical implementation details and edge cases.",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core focus on secrets rotation, Vault, encryption-at-rest, and audit logging in Go; no red flags. Clarity: Crystal clear with precise scope, constraints, and assumptions; minor deduction only for not specifying exact response length. Discriminative: Tests deep applied knowledge of Vault integration, Go concurrency, zero-downtime credential refresh, and edge cases like TTL/TTL expiry, strongly separating strong candidates.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-440",
        "role_title": "Junior Backend Engineer - Metrics Collection",
        "question": "Question:\nYou have a small Go HTTP service that processes user requests. Add minimal code snippets to demonstrate proper metrics instrumentation using Prometheus client_golang. Specifically:\n\n1) Expose a /metrics endpoint with a Counter named requests_total that increments on every request.\n2) Expose a Gauge named in_progress_requests that tracks the number of in-flight requests (increment on request start, decrement on done), and ensure it does not go negative.\n3) Expose a Histogram named request_duration_seconds to observe the time taken to handle each request (seconds).\n\nProvide:\n- The Go code snippets showing how you would declare and update these metrics and hook them into a simple http.Handler that sleeps for 10-50ms to simulate work.\n- How you would start the HTTP server and expose /metrics.\n\nKeep the code concise and runnable, focusing on the metric usage.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core metrics instrumentation (counter/gauge/histogram) with Prometheus client_golang in Go for entry-level role; matches tech stack perfectly (-0 red flags). Clarity: Precise scope, numbered tasks, clear deliverables, well-constrained for screening. Discriminative: Requires applied knowledge of metric patterns, concurrency safety (gauge), timing (histogram), and integration; distinguishes candidates understanding trade-offs from rote memorizers.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-438",
        "role_title": "Staff Engineer - ML Serving Infrastructure",
        "question": "Question:\nYou are deploying a real-time inference service using NVIDIA Triton Inference Server behind a Kubernetes ingress. The model requires sub-50ms latency for 95th percentile at a 100 qps peak load. You have two optimization knobs: (A) dynamic batching at the Triton level, and (B) TensorRT optimization for the model. Describe a concise, practical plan to achieve the target latency, including:\n- How you would configure Triton dynamic batching (batching window, max batch size, and any constraints) and how to determine their values.\n- How you would enable and verify TensorRT optimization for the model within Triton.\n- What metrics and instrumentation you would monitor (including Prometheus metrics) to validate latency targets during rollout.\n- A minimal rollback or safe-guard strategy in case latency regresses.",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Relevance: Directly tests core ML serving skills (Triton, batching, TensorRT, latency optimization, Kubernetes) from role description, no red flags. Clarity: Crystal clear, well-scoped with specific constraints, minor deduction for lacking exact time limit. Discriminative: Requires deep applied knowledge, trade-offs, and practical ML infra experience to distinguish experts.",
          "composite": 9.666666666666666
        }
      },
      {
        "role_id": "be-442",
        "role_title": "Backend Engineer - Event Streaming",
        "question": "Question:\nYou are building a real-time event streaming service in Rust using NATS for the transport and Redis for ephemeral state. You need a simple fan-out mechanism with per-subscription filtering and a short replay capability.\n\nTask:\n- Outline a minimal Rust function (or code snippet) that subscribes a consumer to a NATS subject, maintains a per-consumer filter predicate (e.g., only events where event.type == \"order\" and event.country == \"US\"), and delivers only matching events to the consumer.\n- Include how you would implement:\n  - A lightweight in-memory filter state per subscriber (without using Redis for filtering logic).\n  - A basic replay capability that, upon subscription, replays the last N events stored in Redis (assume events are serialized as JSON and stored with keys like \"event:{id}\").\n- Briefly explain how you would wire this with NATS subscriptions and what data structures you would use to ensure O(1) filtering after receiving an event.\n\nNotes:\n- You do not need a full runnable program; provide concise code sketches and rationale.\n- Focus on practical design decisions and potential performance considerations for mid-level backend engineers.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (event streaming, fan-out, filtering, replay) with exact tech stack (Rust, NATS, Redis); no red flags (-0). Clarity: Crystal clear task, well-scoped with constraints, no ambiguity or bundling (-0). Discriminative: Requires applied Rust/NATS/Redis knowledge, data structure design for O(1) filtering, trade-offs, and performance reasoning; strong separation for mid-level engineers (-0).",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-444",
        "role_title": "Backend Engineer - Service Templates",
        "question": "Question:\n\nYou are given a minimal Go service template that uses a standard HTTP server and a Dockerfile. Describe and provide a short code snippet to add the following in 2 minutes or less:\n\n- A health check endpoint at /health that returns 200 OK with a JSON body { \"status\": \"ok\" }.\n- A basic request metric: increment a Prometheus counter named \"requests_total\" for every incoming HTTP request.\n- A simple cookiecutter-style template variable we could replace to set the service name (e.g., {{cookiecutter.service_name}}) and show how you would expose it in the Go code (without modifying build config). \n\nBonus (optional): outline what minimal Kubernetes manifest would be updated to deploy this new service using ArgoCD, focusing only on the deployment name and container image tag referencing a placeholder that would be substituted by the CI/CD process.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (service templating, observability via Prometheus, best practices for health checks) using Go/Docker; minor ding for cookiecutter (not mentioned) but fits templating (-1). Clarity: Well-scoped with time limit, specific requirements, unambiguous. Discriminative: Requires applied knowledge of Go HTTP/Prometheus integration, templating, and K8s/ArgoCD basics; tests practical skills with trade-offs in minimalism, strong separation.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-443",
        "role_title": "Senior Backend Engineer - Data Lake Architecture",
        "question": "Question:\nYou have an Iceberg table in a Spark job stored on S3. The table was created with a nested schema that includes a struct field user.info with a timestamp field last_login. A new requirement is to evolve the schema to add a new field user.info.nickname (string) and to enable time travel to query the state of the table as of yesterday.\n\n- Describe the minimal code changes you would make in Spark to:\n  a) perform a safe schema evolution to add user.info.nickname without breaking existing reads, and\n  b) run a time-travel query to retrieve data as of yesterday.\n\n- Include:\n  - the key Iceberg/Spark options you would rely on,\n  - how you would reference the historical snapshot in Spark SQL or DataFrame API,\n  - any considerations for nested fields (structs) during evolution,\n  - and how this interacts with partition pruning or performance.\n\nNote: No more than a few short code snippets, and focus on the practical steps and options rather than theory.",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core skills (Iceberg schema evolution, time travel, partitioning) with exact tech stack (Spark, S3, Iceberg nested schemas). No red flags. Clarity: Crystal clear, well-scoped with specific subparts, constraints (short snippets, practical focus), and context; minor deduction for bundled a/b but tightly related. Discriminative: Tests deep applied knowledge of Iceberg APIs, options, nested evolution trade-offs, and performance interactions, strongly separating experienced candidates.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-446",
        "role_title": "Staff Engineer - Distributed Tracing",
        "question": "Question:\nYou are designing a sampling strategy for a high-traffic distributed service stack using OpenTelemetry, Jaeger, and Kafka. The system emits traces with a 128-bit Trace ID. You want to implement a probabilistic sampling policy that:\n\n- Keeps head-of-line latency low for user requests.\n- Ensures stable traces for a given service path (i.e., the same trace should be either sampled consistently or not sampled across its span).\n- Allows easy reconfiguration without redeploying services.\n\nDescribe (briefly) how you would implement this in Go, focusing on:\n- How you would compute a per-request sample decision without expensive hashing of the entire trace.\n- How you would ensure consistency of sampling for a trace across different services/processes (trace-id based sampling).\n- How you would propagate the sampling decision through the pipeline (from client to collector via Kafka) and how you would expose the configurability (sampling rate) for runtime changes.\n\nProvide a small code snippet (pseudocode or Go-like) illustrating the per-request sampling decision and the propagation of the sampling flag.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Relevance: Directly tests core distributed tracing/sampling skills with exact tech stack (OpenTelemetry, Jaeger, Kafka); minor deduction as role emphasizes SLO correlation less (-1). Clarity: Crystal clear, well-scoped with specific focuses and code request; no bundling or ambiguity. Discriminative: Excellent at separating strong candidates via trade-offs (low latency, consistency, propagation), applied Go knowledge, and runtime config\u2014deep, role-specific, not recall/trivia.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-448",
        "role_title": "Senior Backend Engineer - Webhook Infrastructure",
        "question": "Question:\n\nYou are designing a webhook delivery worker in Go with guaranteed delivery. Describe a simple, end-to-end approach (no code required) to ensure that a webhook payload is delivered at least once, including how you would:\n\n- store the retry state and avoid duplicate deliveries on retry,\n- handle exponential backoff with jitter,\n- verify the request signature on the receiver side and enforce delivery guarantees when a signature check fails,\n- surface basic delivery analytics (success rate, retry count, oldest pending webhook) using PostgreSQL and Redis.\n\nBe concise but concrete enough to implement in a real system.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (webhook delivery, retries, signatures, analytics) with exact tech (Go, PostgreSQL, Redis); minor deduction for not being Go-specific in depth (-1). Clarity: Well-scoped, unambiguous, clear sub-bullets with 'simple end-to-end' and 'concise but concrete' constraints. Discriminative: Requires applied knowledge of trade-offs (at-least-once, idempotency, backoff, DB/Redis partitioning), distinguishes strong architects from weak ones.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-450",
        "role_title": "Principal Engineer - ML Platform Strategy",
        "question": "Question:\nYou are tasked with deploying a reusable, multi-tenant ML model serving platform on Kubernetes that integrates Kubeflow for experimentation, MLflow for model governance, and Ray for distributed training/inference. In 2\u20135 minutes, outline a high-level design (no code) that addresses:\n- How you would isolate workloads and data for multiple teams/models securely on a shared cluster.\n- How you would standardize model lifecycle from experimentation to production with governance and lineage.\n- How you would enable cross-team collaboration while preventing resource contention and ensuring scalability.\n\nInclude: key components/services you'd deploy, main data/model lineage artifacts, and a simple 2\u20133 sentence rationale for your choices.",
        "scores": {
          "relevance": 7,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Tests core ML platform strategy, governance, and tooling (Kubernetes, Kubeflow/MLflow/Ray align with ML platform needs), but deduct -2 for focusing only on these specific tools (potentially not main role focus) and -1 for generic design without role's org-wide adoption emphasis. Clarity: Crystal clear, well-scoped with time limit, specific asks, and format; no bundling or ambiguity. Discriminative: Excellent at testing applied trade-offs in multi-tenancy, lifecycle, and scalability, distinguishing strategic thinkers from rote learners.",
          "composite": 8.333333333333334
        }
      },
      {
        "role_id": "be-445",
        "role_title": "Junior Backend Engineer - Database Migrations",
        "question": "Question: You have a PostgreSQL table users(id UUID PK, email TEXT UNIQUE, created_at TIMESTAMP). You need to add a new non-nullable column last_login_at TIMESTAMP with default to current_timestamp, but existing rows must be valid. Outline a safe Alembic migration that:\n\n- Adds the column without locking the table for long (consider PostgreSQL behavior).\n- Fills the column for existing rows with a sensible value (e.g., created_at) without downtime.\n- Sets a server-side default for future inserts.\n- Includes a rollback plan.\n\nProvide the SQL statements or Alembic operations you would include in the upgrade() and downgrade() functions, plus any considerations you would document in comments. Assume SQLAlchemy ORM models may not be fully in sync during migration.",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Relevance: Perfectly matches role (database migrations, zero-downtime, rollback) with PostgreSQL/Alembic exactly as described. Clarity: Crystal clear with precise scope, constraints, and expectations; minor deduction for no explicit time limit. Discriminative: Tests deep applied knowledge of zero-downtime patterns, PG behaviors, backfill strategies, and trade-offs, strongly separating capable candidates.",
          "composite": 9.666666666666666
        }
      },
      {
        "role_id": "be-447",
        "role_title": "Backend Engineer - Load Shedding",
        "question": "Question:\nDesign a simple backpressure-driven request limiter in Go for a Redis-backed queue used by a Go service. Requirements:\n- Each incoming HTTP request should either be accepted to enqueue a task or rejected with a 503 if the system is overloaded.\n- The limiter should use a fixed-size in-memory semaphore (counting semaphore) to cap concurrent processing, and use Redis to coordinate a priority queue where higher-priority tasks are enqueued before lower-priority ones.\n- Explain how you would:\n  -- check current load and decide accept/reject without blocking callers\n  -- implement graceful degradation so that when the queue fills, lower-priority tasks are dropped before higher-priority ones\n  -- expose a simple API to signal backpressure (e.g., a Redis pub/sub or metric-based cue) to upstream services\n\nProvide concise code sketches (Go snippets) for:\n- a request handler that decides to enqueue or reject\n- a basic in-memory semaphore for concurrency\n- a function stub showing Redis LPUSH/LZ ADD (with priorities) and how you\u2019d pull the next task for workers\n\nAssume Go, Redis client, and a HTTP server context. No need for production-ready error handling, just demonstrate the core logic.",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core focus on backpressure, priority queues, and graceful degradation using exact tech (Go, Redis). No red flags. Clarity: Crystal clear with specific requirements, scoped code sketches, and assumptions; minor deduction for no explicit time constraint. Discriminative: Tests deep applied knowledge of semaphores, Redis priority queues (LPUSH/LZADD implies sorted sets), backpressure signals, and trade-offs in load shedding\u2014distinguishes strong systems thinkers.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-449",
        "role_title": "Junior Backend Engineer - Auth Integration",
        "question": "Question:\nYou have a Node.js TypeScript backend that accepts an OAuth2 authorization code from a frontend login flow. Outline and briefly implement (pseudo-code or real code) the minimal steps to exchange the authorization code for tokens with an OAuth provider, validate the ID token if present, and securely store the access token in PostgreSQL. Include how you would handle token refresh logic and error cases in a way suitable for a junior engineer.\n\nDeliverables:\n- A concise code snippet (TypeScript) showing:\n  - Exchange of authorization code for tokens via a POST request to the provider\u2019s token endpoint.\n  - Optional validation of an ID token (e.g., JWT verification) if provided.\n  - Storing the access token (and refresh token if returned) in PostgreSQL with a simple schema.\n  - A basic token refresh flow (using the refresh token to obtain new tokens) and how you would update the DB.\n  - Minimal error handling and logging.\n\nNotes:\n- Do not rely on provider-specific libraries; show generic HTTP calls and JSON handling.\n- Emphasize security aspects: use HTTPS, proper error handling, not logging sensitive tokens, and safe storage.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 8,
          "reasoning": "Relevance: Directly tests OAuth code exchange, token handling, refresh flows, and DB storage\u2014core to role; minor deduction for assuming Node.js/TS/PostgreSQL not explicitly listed (-1). Clarity: Crystal clear with precise deliverables, constraints, and security emphasis; well-scoped for screening. Discriminative: Tests applied knowledge of OAuth flow, security, and implementation with trade-offs (error handling, refresh logic), distinguishing juniors who understand vs. those who don't; slightly broad but constrained by 'minimal/concise'.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-452",
        "role_title": "Senior Backend Engineer - Faceted Search",
        "question": "Question:\n\nYou are implementing a faceted search feature on a product catalog using Elasticsearch. You have two requirements:\n\n1) A user can filter by category, brand, price range, and a boolean \u201cin_stock\u201d flag.\n2) The UI shows facet counts that reflect the number of results after applying all active filters (including nested category paths) and must be accurate at scale.\n\nProvide a succinct, concrete approach (no code necessary) that answers:\n- How you would structure the Elasticsearch query (including aggregations and filters) to ensure correct facet counts under multiple concurrent filters.\n- How you would handle performance considerations for large catalogs (e.g., optimizing aggregations, using composable aggregations, or shard-level strategies).\n- Any trade-offs or caveats you would consider (e.g., count accuracy vs. performance, nested vs. parent aggregations, or reverse-lookup of category paths).\n\nBriefly describe what the resulting query would do and why this approach is sound for a senior backend engineer.",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Relevance: Perfectly matches role's faceted search, dynamic filtering, aggregation optimization, and scale requirements; no red flags. Clarity: Crystal clear with specific requirements, well-scoped prompts, minor deduction for lacking explicit time constraint. Discriminative: Tests deep applied knowledge of ES query structure, performance trade-offs, and scale strategies, strongly separating senior engineers.",
          "composite": 9.666666666666666
        }
      },
      {
        "role_id": "be-454",
        "role_title": "Junior Backend Engineer - ETL Scripts",
        "question": "Question:\nYou have a CSV file \"sales.csv\" with columns: order_id (int), customer_id (int), order_date (YYYY-MM-DD), amount (float), category (text). Write a short Python function using pandas that:\n\n- Reads the CSV from a given file path.\n- Filters rows where amount > 100.\n- Groups by category and computes total revenue and the number of orders.\n- Returns a DataFrame with columns: category, total_revenue, order_count, sorted by total_revenue descending.\n\nAlso include a minimal example call to demonstrate it works (you can use a small in-memory CSV via StringIO). No need to handle missing values or errors beyond basic.",
        "scores": {
          "relevance": 9,
          "clarity": 10,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests pandas ETL skills core to role (-1 for minor generic programming elements). Clarity: Crystal clear with precise requirements, constraints, and example. Discriminative: Tests applied pandas knowledge (filter, groupby, agg, sort) distinguishing capable entry-level candidates from weak ones via practical implementation.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-451",
        "role_title": "Backend Engineer - Subscription Billing",
        "question": "Question:\nYou implement a simple monthly subscription with usage-based pricing. Each user has a plan with a monthly base price and a per-unit rate for overages. At the end of the billing period, you must compute the invoice amount as:\n- base_price + max(0, usage_units - included_units) * per_unit_rate\nAdditionally, support plan changes mid-cycle with proration:\n- If a user upgrades on day 15 in a 30-day month, the new plan\u2019s base price starts immediately and is prorated for the remaining days of the cycle; while the old plan is prorated accordingly for the time used. The total charge for the cycle should reflect: (old plan days remaining) and (new plan days used).\n\nGiven:\n- Month length = 30 days\n- User A on Plan Basic: base 10, included 100 units, overage 0.5 per unit, usage 120 units for the cycle\n- On day 15, User A upgrades to Plan Pro: base 25, included 200 units, overage 0.4 per unit\n- No dunning, no taxes\n\nQuestion: Write the high-level calculation steps (or pseudocode) you would implement to produce the final invoice amount for User A for that cycle, including proration. Explain the formulas you use and the final amount. You have 2\u20135 minutes.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core subscription billing with proration and usage metering from role (-1 for generic pseudocode vs specific stack). Clarity: Precise scenario, constraints, and time limit; unambiguous formulas expected. Discriminative: Requires applied math/trade-off understanding of proration logic, distinguishes those grasping proportional billing from rote learners.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-458",
        "role_title": "Backend Engineer - API Gateway Plugins",
        "question": "Question:\nYou are writing a Kong Lua plugin that must add a custom header X-User-Role to the upstream request based on a token in the Authorization header. The token is a simple string in the format \"Role:<roleName>\". If the header is missing or the token is invalid, respond with 401 and a JSON body {\"error\":\"invalid_token\"}.\n\n- Describe the minimal Lua code snippet (within the access phase) to:\n  - Read the Authorization header from the incoming request.\n  - Validate the token format and extract roleName.\n  - Set kong.response.set_header(\"X-User-Role\", roleName) and allow the request to proceed if valid.\n  - Return a 401 with the specified JSON body if invalid.\n- Explain how you would handle the case where multiple plugins might set the same header, ensuring the token takes precedence.\n\nProvide only the essential code snippet and concise explanation.",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's API gateway plugin development for Kong with auth/transformation (no red flags). Clarity: Crystal clear, well-scoped to access phase with precise requirements, minor deduction for lacking explicit code length constraint. Discriminative: Tests applied Lua/Kong knowledge, parsing, error handling, and thoughtful trade-offs on header precedence, strongly separates skilled candidates.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-453",
        "role_title": "Backend Engineer - Notification Delivery",
        "question": "Question:\nYou are implementing a Go function to batch and enqueue device tokens for push notification delivery. The system must:\n\n- Accept a list of device tokens with their platform (iOS or Android) and an associated message payload.\n- Batch tokens into groups of up to 500 for FCM and up to 100 for APNS per batch (these are typical provider limits).\n- Deduplicate tokens within the batch to avoid duplicate sends.\n- Return a list of batches, where each batch includes the platform, a token list, and the payload.\n\nGiven the following input type:\ntype DeviceToken struct {\n  Token   string\n  Platform string // \"ios\" or \"android\"\n}\n\ntype PushPayload struct {\n  Title string\n  Body  string\n}\n\nImplement a Go function:\n\nfunc CreateDeliveryBatches(tokens []DeviceToken, payload PushPayload) ([]Batch, error)\n\nWhere Batch is:\ntype Batch struct {\n  Platform string   // \"ios\" or \"android\"\n  Tokens   []string\n  Payload  PushPayload\n}\n\nConstraints:\n- Remove duplicate tokens across the entire input (not just within a platform).\n- Partition into batches such that:\n  - iOS batches have at most 100 tokens.\n  - Android batches have at most 500 tokens.\n- If a token appears with both platforms (which should not happen in valid data), keep the token only in the first platform encountered.\n\nProvide just the function implementation. You may assume the input is small enough to process in memory. Focus on correctness and clarity over performance micro-optimizations.",
        "scores": {
          "relevance": 9,
          "clarity": 10,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core batching/deduplication for push notifications in Go, matching role's device registration/batching focus (-1 for no explicit role tech stack match). Clarity: Crystal clear with precise types, constraints, scoping, and focus on correctness. Discriminative: Requires applied data structures (maps for dedup), grouping logic, and edge case handling (cross-platform dupes), strongly separates skilled Go devs.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-455",
        "role_title": "Staff Engineer - Developer Portal",
        "question": "Question:\nYou are asked to implement a lightweight, extensible \u201cService Catalog\u201d widget within a Backstage-based developer portal. The widget should display a list of services from a PostgreSQL-backed catalog and support a simple plugin hook that allows teams to add a custom field to each service (e.g., owner or service owner email) without altering the core schema.\n\nRequirements:\n- Describe the minimal database schema changes (if any) and the backend API endpoints (CRUD not required, but read + extensible field is fine). Include sample SQL for the schema and a brief explanation of how the dynamic field would be stored.\n- Outline how you would architect the React component in Backstage to render the catalog, fetch data via the backend, and allow a plugin to inject an additional field into each service item without modifying the main component code.\n- Explain how Backstage\u2019s plugin architecture would facilitate onboarding a new team to add their own fields and views to the catalog.\n\nProvide:\n- A concise schematic (textual is fine) showing data flow from PostgreSQL to API to React, and where the plugin hook hooks in.\n- A short code snippet (pseudo or TS) demonstrating:\n  - A backend route that returns the catalog with a dynamic extra field if provided by a plugin.\n  - A React hook or component method that consumes the API and renders the dynamic field, delegating rendering of that field to the plugin.\n\nTime: 2\u20135 minutes to answer.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests Backstage plugin architecture, PostgreSQL integration, and developer portal extensibility\u2014core to the role (-0 red flags). Clarity: Precise scope, explicit requirements, time-bound, unambiguous despite minor bundling. Discriminative: Probes schema design trade-offs, React/Backstage patterns, and plugin extensibility, separating strong architects from basics.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-456",
        "role_title": "Backend Engineer - Query Optimization",
        "question": "Question:\nYou have a PostgreSQL table orders(id serial primary key, user_id int, product_id int, amount numeric, created_at timestamptz). A frequent query is:\n\nSELECT count(*) FROM orders WHERE user_id = $1 AND created_at >= now() - interval '30 days';\n\nAssume the table has 1e8 rows and a btree index on (user_id, created_at). The query planner often chooses a sequential scan with a high cost, and the actual query runs slowly enough to notice.\n\n- What is the most likely reason the planner prefers a sequential scan over the index for this query?\n- Propose a concrete, minimal change (one or two steps) to PostgreSQL configuration or schema/indexing that would make this query use an index scan again and improve performance, with justification.\n- How would you validate your change after applying it?\n\nAnswer should be concise and focused on practical steps.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests PostgreSQL query optimization, indexing, and planner behavior - core to role's slow query detection and index recommendations (-0 red flags). Clarity: Precise scenario with schema/index/query, clear sub-questions, well-scoped for screening. Discriminative: Requires deep understanding of Postgres planner heuristics (e.g., selectivity, costs), practical fixes (e.g., index tweaks), and validation - distinguishes experts via trade-offs and real-world application.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-459",
        "role_title": "Junior Backend Engineer - Container Security",
        "question": "Question:\nYou have a Docker image built for a Go web service. You want to add a quick, practical check to ensure that your image includes no known critical vulnerabilities before deployment.\n\n- Describe the exact steps and commands you would run using Trivy to scan the image locally.\n- Include how you would interpret the results and decide whether to fail or proceed.\n- Show how you would integrate this into a simple CI snippet (bash) that runs the scan and exits with a non-zero status if any vulnerability has a severity of Critical or High.\n\nProvide the commands and brief rationale.",
        "scores": {
          "relevance": 7,
          "clarity": 9,
          "discriminative": 8,
          "reasoning": "Relevance: Tests container scanning with Trivy (core to role), but penalize -2 for focusing only on one minor tool ignoring broader role areas like policy enforcement/base image mgmt, and -1 for generic scanning approach applicable to any container role. Clarity: Crystal clear, well-scoped steps with specific asks. Discriminative: Tests applied knowledge of scanning, result interpretation, and CI integration with trade-offs in severity thresholds, separating practical candidates.",
          "composite": 8.0
        }
      },
      {
        "role_id": "be-457",
        "role_title": "Senior Backend Engineer - SLO Automation",
        "question": "Question:\nSuppose you have a service with an SLO of 99.9% availability per 5-minute burn window. The service receives requests already instrumented with success/failure outcomes. You have a burn-rate alerting system implemented in Go that stores a sliding window of the last 7 days and computes burn rate as the ratio of used error budget to total available error budget in the current window.\n\nGiven:\n- Error budget per 5-minute window = 0.000833... (i.e., 0.1% of 1, 0.1% over 5 minutes)\n- You observe for the current 5-minute window: 2 failed requests out of 1000 total requests.\n\na) Compute the burn rate for this 5-minute window.\nb) Explain how you would design a simple alert condition (logic) to trigger a burn-rate alert when the current burn rate exceeds a threshold for at least two consecutive windows. What threshold would you choose for a high-severity alert, and why?\nc) Outline a minimal Go snippet (pseudo-code or working logic) that updates the burn rate with a new 5-minute window result and determines whether an alert should fire, using a sliding window approach. Focus on core logic (no I/O or external systems).",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core SLO/burn rate skills for the role (-1 for no explicit Go requirement in desc but matches infra focus). Clarity: Precise scenario, well-scoped parts a-c, clear constraints. Discriminative: a) math application, b) design/trade-offs, c) Go logic distinguish deep vs superficial knowledge.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-460",
        "role_title": "Backend Engineer - Experiment Tracking",
        "question": "Question:\nYou are integrating an experiment tracking feature using MLflow in a Python project. A researcher logs two metrics for each run: \"accuracy\" and \"loss\". They want to compare two runs and automatically determine which one is better based on a combined score where higher accuracy is better and lower loss is better. The score should be defined as: score = accuracy - 0.5 * loss. \n\nProvide a minimal Python snippet (not a full app) that:\n- Loads two MLflow runs by their run_ids (assume they exist in the same tracking URI).\n- Retrieves the latest logged values for \"accuracy\" and \"loss\" for each run.\n- Computes the score for each run using the formula above.\n- Prints the run_id with the higher score and its score.\n\nAssume:\n- MLflow is already installed and configured.\n- You can use MLflow's tracking API to fetch metrics.\n- If a metric is missing for a run, you should skip that run and print a message indicating the missing metric.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests MLflow (core to ML experiment tracking role), metric handling, and comparison logic - perfect match (-0 red flags). Clarity: Precise scope, clear requirements, explicit assumptions, handles edge cases well. Discriminative: Requires applied MLflow API knowledge, error handling, and computation - distinguishes strong candidates via trade-offs like latest values and skipping invalids.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-461",
        "role_title": "Senior Backend Engineer - Fraud Detection",
        "question": "Question: You have a stream of transaction events coming in real-time via Kafka. Each event includes: user_id, transaction_id, amount, timestamp, location, and device_id. You need to score fraud risk using a simple feature-based approach in Python and publish scores to a Redis sorted set for downstream routing.\n\nTask:\n- Outline a minimal Python function that:\n  - consumes a single transaction dict,\n  - computes a simple risk score using: (a) amount > 1000, (b) same device_id as last transaction for the same user (persisted in-memory), and (c) if location differs from last known location for the user.\n  - stores/updates the user's last transaction info in an in-memory dict.\n  - returns the risk score (0-100) and a reason string describing active rules.\n- Explain how you would publish this score to Redis as a sorted set \"fraud:scores\" with score as the score and member as transaction_id, ensuring idempotence on replayed events.\n- Briefly discuss how you would adapt this to real-time streaming with Kafka and what considerations you would add for production.\n\nNote: You do not need to implement full Kafka consumer/producer code, just the core scoring function and Redis interaction outline. Provide concise code snippets.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core fraud detection pipeline skills (real-time scoring, rule engines) with exact tech stack (Kafka, Redis, Python); minor ding for in-memory state not production-scale. Clarity: Crystal clear task breakdown, precise scope, constraints explicit. Discriminative: Requires applied coding (features, state, idempotence), trade-offs (streaming/production), distinguishes practical engineers from theorists.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-441",
        "role_title": "Senior Backend Engineer - Autocomplete Systems",
        "question": "Question:\n\nYou are building a high-throughput autocomplete service. A request contains a userId and a prefix string. You have an index of previous search queries stored in Elasticsearch with fields: userId, query, timestamp. To support personalization, you want to bias results toward queries the user has searched recently, while still returning relevant global prefix matches.\n\nDesign a minimal but concrete approach (conceptual steps and data flows) to implement this within a sub-50ms latency target at ~100k QPS. Specifically address:\n\n- How you would structure the Elasticsearch query to combine prefix matching with a lightweight personalization boost (e.g., using a per-user recent-query weight).\n- How you would store or maintain the per-user recency signal (data model and eviction strategy) without causing hotspots.\n- How you would ensure low latency (caching, routing, and index shard considerations) and what metrics you would monitor to keep latency under 50ms.\n\nProvide a rough query snippet (pseudo-SQL or DSL) and a brief justification of the design choices. You do not need to implement code, just outline the approach.",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Relevance: Perfectly matches role's core requirements (autocomplete, prefix matching, personalization, 50ms@100K QPS) with exact tech (Elasticsearch); no red flags. Clarity: Crystal clear, well-scoped with specific sub-bullets, pseudo-code request, and constraints; minor deduction for no explicit time limit. Discriminative: Tests deep applied knowledge, trade-offs (latency, hotspots, caching), and role-specific design; excellent separation of strong vs weak candidates.",
          "composite": 9.666666666666666
        }
      },
      {
        "role_id": "be-462",
        "role_title": "Backend Engineer - Alert Routing",
        "question": "Question:\nYou are building an alert routing microservice in Go that ingests alerts from a source (e.g., Prometheus) and routes them to responders via PagerDuty. Given a simplified alert object:\n\ntype Alert struct {\n  ID        string\n  Status    string // \"firing\" or \"resolved\"\n  Severity  string // \"critical\" or \"warning\"\n  Labels    map[string]string\n  Annotations map[string]string\n  StartsAt  time.Time\n  EndsAt    time.Time\n}\n\nRequirements:\n- Implement a function routeAlert(a Alert) string that returns the ID of the responder target (e.g., \"pagerduty-oncall-foo\" or \"pagerduty-esc-1\") or \"\" if no routing applies.\n- Routing rules (hard-coded in the function for this exercise):\n  - If Status == \"firing\" and Severity == \"critical\", route to \"pagerduty-oncall-urgent\".\n  - If Status == \"firing\" and Severity == \"warning\", route to \"pagerduty-oncall-standard\".\n  - If Status == \"resolved\", always route to \"\" (no routing).\n  - If Labels has key \"team\" with value \"infra\", route to \"pagerduty-oncall-infra\" regardless of severity.\n- Deduplicate rapid repeated alerts: if the same alert (same ID) has Status unchanged for more than 5 minutes, return \"\" to simulate deduplication.\n- Provide a minimal, self-contained Go implementation of routeAlert, including handling for a simple in-memory state to track the last status time per alert ID.\n\nEdge cases to consider:\n- When an alert fires, but a matching exception label is present (e.g., Labels[\"ignore\"] == \"true\"), route to \"\".\n\nKeep the implementation succinct (2-5 minutes to write) and focused on core logic. You can assume necessary imports. Provide only the code snippet.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests Go coding for alert routing, deduplication, and escalation logic matching role (Prometheus/PagerDuty style); minor deduction as rules are simplified/hard-coded (-1). Clarity: Crystal clear spec, well-scoped with time estimate, precise requirements, no ambiguity. Discriminative: Requires applied Go knowledge (structs, maps, time logic, state management), edge cases, and deduplication trade-offs; distinguishes strong coders effectively.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-464",
        "role_title": "Staff Engineer - Presence Systems",
        "question": "Question:\nDesign a lightweight heartbeat protocol for a real-time presence system used by a chat app. You have a large fleet of WebSocket connections (Go backend) and need to detect dead/idle clients quickly while minimizing network and CPU overhead.\n\nRequirements:\n- Each client maintains a heartbeat with the server and vice versa.\n- The server should detect a dead connection within a configurable window (e.g., 5\u201315 seconds) and mark the user as offline if no heartbeat is received.\n- Consider how to handle network latency, NAT, and bursty traffic without flooding the system with heartbeats.\n- Outline the data format (JSON or binary), the message types, and the server-side logic to:\n  - register a new connection,\n  - refresh or update the last-seen timestamp,\n  - clean up and notify others when a user goes offline,\n  - scale across multiple instances (e.g., via Redis or Kafka).\n\nProvide a concrete example of the heartbeat flow (sequence of messages and timeouts) and a brief discussion of potential edge cases and how you would mitigate them.",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Relevance: Directly targets core role skills (heartbeat protocols, WebSocket connection management, state sync at scale) with exact tech mentions (Go, Redis/Kafka); no red flags. Clarity: Crystal clear, well-scoped with specific requirements, constraints, and outputs; minor deduction for no explicit time limit. Discriminative: Tests deep applied knowledge, trade-offs (latency/NAT/bursty traffic), scaling, edge cases\u2014strong separation of strong vs weak candidates.",
          "composite": 9.666666666666666
        }
      },
      {
        "role_id": "be-465",
        "role_title": "Backend Engineer - CDC Pipelines",
        "question": "Question:\nYou are configuring a PostgreSQL source using Debezium to stream changes into Kafka for a data lake. A table has an integer primary key id and a TEXT column name. You notice that during a full initial snapshot, Debezium is taking a long time, and after the snapshot, downstream consumers occasionally receive duplicate events if a transient failure occurs, even though you have configured Kafka exactly-once semantics.\n\n- Explain a practical approach to ensure idempotent processing and at-least-once vs exactly-once guarantees in this setup.\n- Include (a) what Debezium configuration or mode you would use for the initial snapshot, (b) how you would structure Kafka topics or keys to achieve idempotence, and (c) what downstream consumer logic (in Python) would look like to deduplicate events and maintain exactly-once semantics across restarts.\n\nProvide concise snippets or pseudo-code for the consumer deduplication portion and describe how you would handle schema changes/DDL in this pipeline. The answer should demonstrate how you would diagnose and fix issues in a real-time CDC pipeline end-to-end.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Relevance: Directly tests core CDC pipeline skills (Debezium, PostgreSQL, Kafka, snapshots, exactly-once, schema changes) matching role; minor deduction as Debezium isn't explicitly listed but is standard for described tasks (-1). Clarity: Well-scoped with specific parts (a,b,c), clear expectations for code/snippets, no bundling issues. Discriminative: Excellent at separating experts via trade-offs (at-least vs exactly-once, idempotence, consumer logic, schema handling, diagnosis), requires deep applied knowledge not rote recall.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-466",
        "role_title": "Senior Backend Engineer - GitOps Tooling",
        "question": "Question:\nYou are building a simple GitOps reconciliation loop in Go. Given a desired state stored in a Git repository (a YAML manifest) and a live Kubernetes cluster state, outline and implement a minimal reconcile function that:\n\n- Reads a YAML manifest with a single Deployment (name, namespace, image, replicas).\n- Compares it to the corresponding Deployment in the cluster.\n- If the live state differs (image or replicas), prints the needed Kubernetes patch operations (instead of applying them) to bring the live state in line with the desired state.\n\nConstraints:\n- Use a Go function signature: func Reconcile(desiredYAML []byte) ([]string, error)\n- Do not make actual API calls to Kubernetes; simulate the cluster state with an in-memory map of deployments (keyed by namespace/name) that you initialize inside the function.\n- Return a list of human-readable patch messages, e.g., \"Update deployment ns/app: replicas from 2 to 3\", \"Update deployment ns/app: image from foo:1.0 to foo:1.1\".\n- If the desired state perfectly matches the live state, return an empty slice with nil error.\n\nProvide:\n- A compact Go code snippet implementing Reconcile, including:\n  - YAML parsing into a DeploymentSpec struct\n  - In-memory live state initialization\n  - Diff logic for replicas and image\n  - Proper error handling for malformed YAML\n- Brief explanation (1-2 sentences) of how this would fit into a real controller, and one potential extension for drift detection.",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly targets core GitOps reconciliation loops, YAML manifests, Kubernetes Deployments, and Go\u2014directly matching role's declarative infra tooling focus (no red flags). Clarity: Crystal clear with precise signature, constraints, simulation, and scoped outputs; minor deduction for not specifying exact YAML structure. Discriminative: Requires applied Go/YAML/K8s knowledge, diff logic, error handling, and controller insight\u2014strong separation of strong vs weak candidates.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-468",
        "role_title": "Junior Backend Engineer - CI Pipelines",
        "question": "Question:\nYou are setting up a GitHub Actions workflow to build and test a Python project in a CI pipeline. The repository uses a Docker-based test environment and has several Python test modules. You want to optimize the workflow so that:\n\n- The Python dependencies are installed only once per workflow run and cached for subsequent jobs.\n- Tests can run in parallel across modules to speed up the pipeline.\n- The final step publishes a test coverage report if tests pass.\n\nProvide a concise YAML snippet (GitHub Actions workflow) showing:\n- a job that caches Python dependencies across the workflow using actions/cache\n- parallelizable test jobs (one job per test module directory, e.g., tests/moduleA, tests/moduleB)\n- a final job that runs after the test jobs to generate and upload a coverage report (assume coverage tool is pytest-cov)\n\nKeep the snippet focused on the core logic and include minimal commands to illustrate the steps. No external explanations.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core GitHub Actions skills (caching, parallelization, workflows) from role description (-0 red flags). Clarity: Crystal clear scope, constraints, and deliverables; no ambiguity or bundling. Discriminative: Requires applied YAML knowledge, trade-offs in caching/parallelization, and integration; distinguishes strong entry-level candidates able to produce functional snippet.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-467",
        "role_title": "Backend Engineer - Connection Pooling",
        "question": "Question:\nYou are tasked with designing a simple, in-process connection pool for PostgreSQL in Go to be used alongside PgBouncer in a Kubernetes environment. The pool should:\n\n- Limit the maximum number of active connections to PostgreSQL to N (configurable).\n- Reuse connections efficiently for sequential queries within a request.\n- Support a single transaction mode per connection (i.e., when a transaction starts, it must finish before releasing the connection).\n- Detect and gracefully handle a failed connection by removing it from the pool and creating a new one.\n\nProvide a concise, high-level outline (pseudo-code or bullet steps) for:\n1) Pool data structures and synchronization primitives you would use in Go.\n2) The logic for AcquireConnection and ReleaseConnection, including how you\u2019d handle a transaction boundary (BEGIN/COMMIT/ROLLBACK) and error cases.\n3) How you would ensure safe interaction with PgBouncer in a Kubernetes environment (e.g., connection multiplexing considerations, idle timeouts, and pool sizing hints).\n\nAssume you already have a working Go PostgreSQL driver (database/sql with pq or pgx) available. No need to write full code, but include enough detail to assess correctness and practicality.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core role skills (pool sizing N, transaction modes, failover) in exact tech stack (Go, PostgreSQL) with role context (PgBouncer, K8s); no red flags. Clarity: Crystal clear, well-scoped to high-level outline with numbered parts and assumptions; minor jargon but contextualized. Discriminative: Tests deep applied knowledge of Go concurrency, transaction handling, error recovery, and PgBouncer/K8s trade-offs; strong separation without being too broad.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-469",
        "role_title": "Senior Backend Engineer - Chaos Engineering",
        "question": "Question:\nYou are deploying a Go-based microservice on a Kubernetes cluster managed with Chaos Mesh. You want to ensure that when a network partition occurs between two service pods, the system maintains at least 99.9% availability for a 5-minute window, and you should observe recovery within 2 minutes after the partition ends.\n\n- Outline a concise plan (including the specific Chaos Mesh experiment you would use, how you would configure it, and what metrics you would monitor).\n- Provide a minimal Go snippet (or pseudo-code) showing how your service would gracefully degrade or retry to maintain availability during the partition.\n- Describe how you would validate the blast radius and recovery time using Prometheus/Grafana dashboards.\n\nAssume you have:\n- A Kubernetes cluster with Chaos Mesh installed.\n- A service mesh or standard Kubernetes networking setup.\n- Basic Go HTTP service with a retryable upstream call.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests chaos engineering (Chaos Mesh, failure injection), Go microservices, K8s, resilience, blast radius, and recovery\u2014core to role (-0 red flags). Clarity: Well-scoped with explicit parts, clear assumptions, no ambiguity. Discriminative: Requires applied knowledge of Chaos Mesh config, Go resilience coding, and observability setup with trade-offs, strongly separates candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-470",
        "role_title": "Backend Engineer - GraphQL Federation",
        "question": "Question:\nYou are building a GraphQL gateway using Apollo Federation to compose two subgraphs: Users and Orders. The Users subgraph exposes a User type with key field id, and the Orders subgraph exposes an Order type with a field userId that references the User. You want to enable cross-service entity resolution so a query can fetch a User along with their Orders in a single federated query.\n\nTask:\n1) Describe (pseudo-code or concise steps) how you would configure the User and Order types to be representable as federated entities, including:\n   - How to mark the entity types, keys, and reference resolvers in each subgraph.\n   - How to implement a federated resolver in the gateway to fetch an aggregated User with their related Orders.\n\n2) Provide an example of a GraphQL query that a client could run against the gateway to fetch a user by id along with their orders, and outline how the gateway would resolve this query across the two subgraphs.\n\nConstraints:\n- Use TypeScript-style pseudocode for the resolver signatures.\n- Focus on the core federation concepts: type definitions, _entities, and reference resolution, not full server setup.",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core requirements (federated GraphQL gateway, schema stitching, entity resolution) with exact tech (Apollo Federation). No red flags. Clarity: Crystal clear with numbered tasks, specific constraints, focused scope; minor deduction for no explicit time limit. Discriminative: Tests deep applied knowledge of federation setup, resolvers, query planning with trade-offs in entity resolution; strong separation between experts and juniors. Minor deduction as slightly broad but well-constrained (<10min feasible).",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-471",
        "role_title": "Staff Engineer - Zero Trust Architecture",
        "question": "Question:\nYou are designing a microservices platform using SPIFFE/SPIRE and mTLS. A service A in Namespace ns-a needs to call service B in Namespace ns-b. Outline, in concrete steps, how you would:\n\n- issue and validate mTLS certificates end-to-end between A and B using SPIFFE IDs\n- enforce a policy that only service A can call service B from within the cluster\n- handle certificate rotation without downtime\n\nAssume you are using Kubernetes, Envoy as the sidecar proxy, and SPIRE as the CA/identity agent. Be specific about SPIRE workload registration, Envoy config hints (e.g., SDS, TLS validation), and how you would configure the SPIRE policy/attestation to achieve the zero-trust requirement.",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Relevance: Perfectly matches role's core focus on zero-trust microservices, mTLS automation, and policy enforcement with exact tech stack (SPIFFE/SPIRE, mTLS, Kubernetes). No red flags.\nClarity: Crystal clear, specific components and scope defined; minor deduction only for not specifying response time/format.\nDiscriminative: Tests deep applied knowledge of integration, trade-offs (e.g., rotation without downtime), and configuration specifics, strongly separating architects from juniors.",
          "composite": 9.666666666666666
        }
      },
      {
        "role_id": "be-472",
        "role_title": "Backend Engineer - Vector Search",
        "question": "Question:\nYou have a FastAPI service that receives a text query and should return the top-3 most similar documents from a Pinecone index. Each document has a precomputed embedding stored in Pinecone with the external ID equal to the document_id. The service should also fallback to Redis-stored keyword matches if Pinecone latency exceeds 120ms on average.\n\nSketch a minimal, runnable Python snippet (no full app required) that:\n- Accepts a string query.\n- Generates its embedding using a provided function generate_embedding(text) -> List[float].\n- Queries a Pinecone index named \"docs-index\" to retrieve top-3 matches by cosine similarity.\n- If the Pinecone query latency exceeds 120ms on average (simulated), fall back to performing a simple Redis-based keyword match using a Redis key pattern \"doc:keywords:<document_id>\" where the value is a space-separated string of keywords; return the top-3 documents with the highest keyword overlap with the query.\n- Returns a list of document_ids for the final results.\n\nNotes:\n- You can assume the Pinecone client is already initialized as pinecone_client and the index as pinecone_index.\n- You can simulate latency threshold by checking a provided variable pinecone_latency_ms.\n- Focus on the control flow and integration logic, not on full error handling or deployment concerns.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core vector search (Pinecone embeddings, cosine similarity), hybrid search fallback (Redis keywords), and FastAPI integration - matches role focus perfectly (-0 red flags). Clarity: Crystal clear scope, precise constraints, simulation for latency, minimal viable snippet (-0 red flags). Discriminative: Requires applied knowledge of Pinecone/Redis APIs, embedding integration, latency-based fallback logic, and ranking trade-offs - strong separation of strong vs weak candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-473",
        "role_title": "Senior Backend Engineer - Refund Processing",
        "question": "Question:\nDesign a high-level approach and provide a concise code sketch (Java/Spring) for handling a partial refund in a refund processing service that integrates with a ledger system.\n\nRequirements:\n- Accept a refund request with orderId, totalAmount, and refundAmount (where refundAmount <= totalAmount).\n- Update an in-memory ledger (simulated with a Map) to reflect the partial refund against the original charge line item, ensuring idempotency for repeated refund requests with the same refundId.\n- Publish a refund event to Kafka (topic: refunds.events) after successful ledger update.\n- Record an audit trail entry (in-memory list) with timestamp, refundId, orderId, refundedAmount, and status.\n- Include basic error handling for over-refund attempts and idempotent behavior for duplicate refundId.\n\nWhat is the core data model, method signature, and a compact implementation sketch (pseudocode or real Java) that demonstrates these aspects? Focus on correctness and clarity suitable for a senior engineer.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core refund processing, ledger integration, idempotency, events, and audits from role; Java/Spring matches implied stack (-0 red flags). Clarity: Precise requirements, scoped constraints, clear deliverables; no bundling or vagueness. Discriminative: Requires applied design, trade-offs (idempotency, error handling), and code synthesis distinguishing seniors; not recall/trivia, well-scoped <10min.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-476",
        "role_title": "Backend Engineer - Live Updates",
        "question": "Question:\nYou are implementing a live dashboard using Socket.io with multiple rooms representing different data feeds. A user can subscribe to a subset of feeds and may disconnect/reconnect (reconnect should automatically rejoin the previously subscribed rooms). Describe or provide a minimal TypeScript/Node.js snippet that demonstrates:\n\n- How you would manage user subscriptions to rooms (subscribe/unsubscribe) using Socket.io namespaces or rooms.\n- How you would ensure that on reconnection, the client automatically re-enters the same set of rooms they were in before disconnect.\n- A brief outline or code snippet showing how you would broadcast an update only to users subscribed to a specific feed (room) without leaking to others.\n\nConstraints for answer:\n- Use TypeScript with Socket.io (server-side).\n- Include how you would store per-user subscription state (in-memory or Redis) and how you would rehydrate it on reconnect.\n- Keep the code snippet concise (approx 15-25 lines of core logic).",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests Socket.io rooms, reconnection, selective broadcasting - core to role's live update system; minor ding for not specifying storage tech but covers role focus perfectly (-1 generic feel avoided).\nClarity: Crystal clear with specific requirements, constraints, scope (15-25 lines), no ambiguity or bundling.\nDiscriminative: Tests applied Socket.io knowledge, state management trade-offs (in-memory/Redis), reconnection logic - distinguishes strong candidates via practical implementation depth.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-463",
        "role_title": "Junior Backend Engineer - Full-text Search",
        "question": "Question:\n\nYou have a PostgreSQL table articles(id serial primary key, title text, body text, tsvector_search tsvector). You want to implement a simple full-text search that:\n\n- updates tsvector_search when articles are inserted or updated\n- searches using a query string, returning the top 5 most relevant articles by simple ranking (ts_rank_cd with normalization 32D)\n\nTasks (do not need to write entire app, just the SQL and a minimal Python snippet):\n\n1) Write the SQL statements to:\n   - create a trigger function to automatically set the tsvector_search column from (title || ' ' || body) for inserts and updates.\n   - create a trigger on the articles table to call that function.\n   - create a GIN index on tsvector_search.\n\n2) Given a Python snippet using psycopg2 (or any psycopg3), show how you would execute a search for a user-provided query string q, returning id, title, and a ranking score, limited to 5 results, ordered by the rank. Use a simple ts_rank_cd with a tsquery built from plainto_tsquery on q.\n\nProvide only the necessary SQL and the minimal Python code to perform the search.",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's PostgreSQL full-text search focus (tsvector, triggers, ranking, GIN index). No red flags.\nClarity: Crystal clear tasks with precise specs (ts_rank_cd 32D, plainto_tsquery, top 5), minimal Python scope. Minor deduction for not explicitly stating Python library preference.\nDiscriminative: Tests hands-on applied skills (triggers, indexing, query building, integration) distinguishing implementers from theorists; not recall, strong separation for entry-level.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-475",
        "role_title": "Junior Backend Engineer - Search Analytics",
        "question": "Question:\nYou have a simple search log table in PostgreSQL with columns: id, user_id, query_text, timestamp, clicks, and result_rankings (a JSON array of document_id in the order shown to the user). Write a Python function that, given a user's search query and a list of observed document_ids returned in that order (as a JSON array), updates a SQLite/PostgreSQL table of per-query statistics to increment a \"total_queries\" counter for that exact query and append a new entry to a separate table \"query_results\" capturing query_text, document_id, rank, and timestamp. The function should:\n- Upsert the per-query statistics (create if not exists, then increment total_queries).\n- Insert all (query_text, document_id, rank, timestamp) rows for the observed results.\n- Do this efficiently for typical short result lists (e.g., 10 items).\n\nProvide a concise Python snippet (using psycopg2 or sqlalchemy) that performs this operation within a single transaction. Do not worry about schema setup, just show the function body and necessary imports.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests PostgreSQL, Python (psycopg2/SQLAlchemy), JSON handling, and query logging/updates core to search analytics role (-1 minor for mentioning SQLite option). Clarity: Crystal clear with precise schema, requirements, constraints, and scope. Discriminative: Requires applied SQL (upsert/insert), transaction handling, efficiency awareness, and Python DB integration, strongly separating capable entry-level engineers.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-477",
        "role_title": "Senior Backend Engineer - Data Catalog",
        "question": "Question:\nYou have a data pipeline that ingests raw JSON logs from Kafka, processes them with a Python Airflow DAG, and stores transformed records in PostgreSQL. You want to automatically capture metadata and lineage in Apache Atlas for both the input (Kafka topic) and output (PostgreSQL table) entities, including a simple data quality check (e.g., a field \"user_id\" must be non-null and a field \"event_ts\" must be within the last 30 days).\n\nDescribe, at a high level, the minimal steps and key code or configuration snippets you would implement to:\n- register Atlas entities for the Kafka topic and PostgreSQL table\n- create and attach lineage between the Kafka ingest step and the PostgreSQL write step\n- implement a basic data quality check that updates Atlas with a quality score or status\n\nExplain how you would trigger and ensure this metadata is captured automatically as part of the Airflow DAG execution.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (metadata extraction, lineage tracking) with exact tech stack (Airflow, Kafka, PostgreSQL) matching data catalog needs; minor ding for not mentioning profiling/governance explicitly (-1). Clarity: Well-scoped high-level question with precise requirements, no ambiguity or bundling. Discriminative: Requires applied knowledge of Atlas integration, lineage APIs, Airflow hooks, and DQ implementation with trade-offs, strongly separates experienced candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-474",
        "role_title": "Backend Engineer - Dashboards API",
        "question": "Question:\nYou are implementing a minimal dashboard rendering endpoint in Go for an observability platform. The frontend passes a dashboard template ID and a map of variable values (e.g., { \"host\": \"host-1\", \"region\": \"us-east\" }). The template contains placeholders in the form ${var} that should be substituted with the provided values. Additionally, some variables have default values if not supplied (e.g., \"region\" defaults to \"us-west\").\n\nDesign a concise approach and provide a small code snippet (Go) that:\n- Retrieves a template string from a mocked store (you can hardcode a template in the snippet).\n- Substitutes variables using the provided map, applying defaults where missing.\n- Returns the final rendered dashboard JSON string with all ${var} placeholders replaced.\n\nConstraints:\n- Do not rely on external libraries for template rendering; implement a simple substitution.\n- If a required variable is missing and has no default, return an error.\n- Include error handling for missing template or invalid input.\n\nExample:\nTemplate: { \"title\": \"Dashboard for ${host}\", \"region\": \"${region}\" }\nInput vars: { \"host\": \"db-01\" }\nDefaults: region -> \"us-west\"\nOutput: { \"title\": \"Dashboard for db-01\", \"region\": \"us-west\" }\n\nProvide just the code snippet and brief explanation (1-2 sentences).",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Directly tests Go API development, variable substitution, and error handling\u2014core to dashboard rendering in the observability role (-0 red flags). Crystal clear with precise scope, constraints, example, and output format. Strong separation via custom simple substitution implementation requiring applied Go knowledge, defaults, and error cases\u2014not trivial recall.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-479",
        "role_title": "Principal Engineer - Data Architecture",
        "question": "Question: You are designing a data platform that handles both transactional (OLTP) and analytical (OLAP) workloads in a single environment. You have PostgreSQL for OLTP, ClickHouse for analytics, Kafka for streaming, Redis for caching, and Kubernetes for orchestration.\n\nScenario: A new feature requires real-time user activity tracking with low-latency dashboards and periodic batch aggregations for BI reports. Outline a concrete high-level dataflow (components and data movement) that satisfies:\n\n- Real-time ingestion from a Kafka topic into the OLTP and/or analytical store\n- Efficient upserts and referential integrity in PostgreSQL for transactional data\n- Efficient, scalable analytics in ClickHouse without impacting OLTP performance\n- Caching and fast reads for dashboards\n\nProvide:\n1) The data routing/processing approach you would implement (include any adapters, stream processing concepts, and where you would materialize data).\n2) A succinct justification for chosen storage choices (why PostgreSQL vs ClickHouse for which data).\n3) A brief outline of deployment considerations on Kubernetes (scaling, schema migrations, and fault tolerance).\n\nLimit to 200-300 words.",
        "scores": {
          "relevance": 9,
          "clarity": 10,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core data architecture skills for mixed OLTP/OLAP workloads using exact tech stack from role (-0 red flags). Clarity: Crystal clear scenario, precise requirements, scoped with word limit and numbered parts. Discriminative: Requires applied trade-offs in dataflow design, storage choices, and K8s deployment, distinguishing strong architects from rote learners.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-480",
        "role_title": "Junior Backend Engineer - Deployment Automation",
        "question": "Question:\nYou have a Python script that installs a small web app on an Ubuntu server. The script should:\n- Create a virtual environment in /opt/myapp/venv\n- Install requirements.txt from the repository\n- Start the app with a simple Python HTTP server on port 8000\n- Ensure that if the server process dies, it is automatically restarted (basic restart policy)\n\nProvide a short Python snippet that:\n1) Creates the virtual environment and installs dependencies from requirements.txt\n2) Starts the HTTP server in the background in a way that can be restarted if it exits (you can assume the app is a simple script app.py in /opt/myapp)\n3) Demonstrates a minimal, portable way to implement a basic restart mechanism (without relying on systemd)\n\nNote: You don\u2019t need to implement full error handling, just show the core commands/logic to accomplish these steps.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 8,
          "reasoning": "Relevance: Directly tests Python scripting for Ubuntu deployment, virtualenvs, and restart logic\u2014core to entry-level deployment scripts role (-1 minor for not mentioning config mgmt/rollbacks explicitly). Clarity: Well-scoped with clear requirements, assumptions, and constraints; no ambiguity. Discriminative: Requires applied knowledge of venv, background processes, and restart loops to distinguish capable scripters from novices; not trivial recall.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-478",
        "role_title": "Backend Engineer - Cost Attribution",
        "question": "Question:\nYou are designing a cost attribution feature for a cloud platform. You receive resource usage data in a daily CSV feed with fields: resource_id, team_id, cost, and tags (a semicolon-delimited key=value string). Each resource is owned by exactly one team, but tags can include multiple attributes (e.g., env=prod;app=backend;region=us-east-1). The system must support generating a daily cost report that attributes each resource's cost to its team and also allows filtering by a tag key=value (e.g., app=backend).\n\n- Describe a minimal Go data model (structs) and a small in-memory processing snippet that:\n  - Parses a single CSV line into useful objects\n  - Aggregates total cost per team for those resources matching a given tag filter\n  - Produces a map[string]float64 where key is team_id and value is the total cost\n\n- Include edge-case handling (missing fields, malformed tags) and how you would extend this to handle large-scale data (without implementing full batch processing).\n\nYou may provide concise code snippets and brief explanations.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (cost attribution, resource tagging, team allocation, reports) with Go (common for cloud infra) and exact data model from role (-0 generic red flags). Clarity: Crystal clear scope, single focused task with explicit requirements/edges, well-constrained for screening. Discriminative: Requires applied Go (structs/parsing), aggregation logic, error handling, and scalable thinking with trade-offs, strongly separates strong candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-481",
        "role_title": "Backend Engineer - Canary Deployments",
        "question": "Question:\nYou are implementing a canary deployment for a Go microservice on Kubernetes with Istio. You deploy a new version (v2) alongside the current version (v1) and want to automatically promote to v2 only if the following conditions are met within 5 minutes:\n- The error rate of v2 remains <= 0.5% of requests.\n- The latency 95th percentile for v2 is within 10% of v1's 95th percentile.\n- The success rate (2xx/3xx/4xx/5xx) of v2 is not worse than v1 by more than 5%.\n\nDescribe in concrete terms:\n- How you would configure the Istio VirtualService/DestinationRule to route 5-10% of traffic to v2 initially and then shift traffic gradually.\n- What metrics you would collect in Prometheus, and how you would compute the three conditions above using Prometheus queries.\n- A high-level outline of an automated rollback/promotion decision loop (pseudocode or steps) that runs every 30 seconds and reverts if any condition fails, or promotes to 100% if all pass.\n\nAssume you have:\n- Service: \"orders\" with versions \"v1\" and \"v2\"\n- Istio metrics enabled and Prometheus scraping enabled\n- Go service emitting standard HTTP metrics (latency histograms, request counts, error counts) labeled by version\n\nKeep the answer concise but implementable.",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Relevance: Perfectly matches role's core focus on canary deployments, traffic shifting, metrics, and gates using exact tech (Kubernetes/Istio/Prometheus/Go); no red flags. Clarity: Crystal clear with precise constraints, assumptions, and scoping; minor -1 for no explicit time limit but well-contained. Discriminative: Tests deep applied knowledge of Istio config, PromQL queries, and automation logic with trade-offs, strongly separates experts.",
          "composite": 9.666666666666666
        }
      },
      {
        "role_id": "be-482",
        "role_title": "Senior Backend Engineer - API Monetization",
        "question": "Question:\nYou are implementing a simple quota system for an API service. Each API key has a monthly quota (e.g., 10,000 requests). When a request comes in, you need to check and increment usage, and return an error if the quota is exceeded. The system should be safe for concurrent requests and reset quotas at the start of each month.\n\nPropose a concise, implementable approach (in Go) using PostgreSQL and Redis that ensures:\n- Atomic check-and-increment of usage per API key within the month.\n- Fast reads for availability and minimal locking.\n- Reset of quotas at month boundary without data drift.\n\nProvide:\n- A brief data model (tables/columns) you would use.\n- The exact Redis or SQL operation(s) you would perform for a single request to enforce the quota.\n- How you would handle month rollover (e.g., a scheduled task or on-demand cleanup) and avoid double-counting or gaps.\n\nKeep code snippets minimal and focused on the critical path. Timebox your answer to 5 minutes.",
        "scores": {
          "relevance": 9,
          "clarity": 10,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core quota management and concurrency for API monetization role using relevant tech (Postgres/Redis), minor deduction for not mentioning billing integration (-1). Clarity: Crystal clear, well-scoped with explicit requirements, constraints, and timebox. Discriminative: Tests applied knowledge of atomic ops, caching, race conditions, and rollover strategies, strong separation of strong vs average candidates.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-483",
        "role_title": "Backend Engineer - WAF Integration",
        "question": "Question:\n\nYou are wiring up a Go service to manage ModSecurity rules for a commercial WAF integration. Given a request coming through Nginx, describe (in code or pseudocode) how you would:\n\n- Detect a simple rate-limiting pattern (e.g., more than 100 requests from the same IP within 60 seconds) using Redis as the counter store.\n- Trigger a ModSecurity rule from Go when the rate limit is exceeded, so the request is blocked at the WAF layer.\n- Ensure the detection is efficient and race-free in a concurrent Go environment.\n\nBriefly outline:\n- The Redis data structure and command sequence you would use.\n- How you would integrate with ModSecurity (e.g., via a rule push or API) and what data you would pass to trigger the block.\n- Any potential pitfalls and how you would mitigate them (e.g., clock drift, rate-limit reset, backoffs).",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Relevance: Directly tests Go, Redis, rate limiting, WAF/ModSecurity integration, and concurrency - core to role (-1 for no explicit Nginx mention in role). Clarity: Precise scope, clear components, well-constrained for screening. Discriminative: Requires applied knowledge of Redis Lua/scripts, Go concurrency (sync/atomic/context), ModSecurity triggering, and trade-offs/pitfalls like races/clock skew - excellent separation of strong candidates.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-484",
        "role_title": "Staff Engineer - Model Registry",
        "question": "Question:\nYou are designing a model registry for an ML platform using MLflow, backed by S3 for artifacts and PostgreSQL for metadata. Describe a practical approach to implement governance and versioning that ensures:\n- Each model version is immutable once registered.\n- Provenance is captured (training data version, code, and hyperparameters).\n- Deployment workflow promotes a model from stage (e.g., staging) to production with minimum downtime and rollback capability.\n\nIn your answer, outline:\n- The schema/metadata you would store in PostgreSQL and how it links to MLflow runs/artifacts.\n- How you would version models, including handling multiple ensembles or non-unique model names.\n- A high-level deployment flow (CI/CD-ish) with a promotion mechanism and rollback plan.\n- How you would ensure artifact integrity in S3 and traceability from registry to deployment.",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Relevance: Directly tests core role skills (model registry, versioning, governance, deployment) using specified tech (MLflow, S3, PostgreSQL); no red flags. Clarity: Well-scoped with specific requirements and outline; minor deduction for lacking explicit time/format constraints. Discriminative: Requires deep applied knowledge of trade-offs in ML ops (immutability, provenance, CI/CD, rollback), strongly separates experts from juniors.",
          "composite": 9.666666666666666
        }
      },
      {
        "role_id": "be-485",
        "role_title": "Junior Backend Engineer - Payment Webhooks",
        "question": "Question:\n\nYou are implementing a webhook receiver in Python (FastAPI) that must handle payment events from a provider. To prevent duplicate processing, you\u2019ll need a simple idempotency mechanism using Redis, keyed by a unique event_id provided in the webhook payload.\n\nRequirements:\n- Validate that the request includes a JSON body with fields: event_id (string) and event_type (string).\n- Implement a minimal handler that:\n  - Verifies the event_type is one of: \"payment_succeeded\", \"payment_failed\".\n  - Uses Redis to ensure each event_id is processed at most once. If the same event_id is received again, return 200 OK without re-processing.\n  - For the first-time event, simulate processing by printing \"Processed event {event_id}: {event_type}\".\n- Return appropriate HTTP status codes: 200 on success (including idempotent duplicates), 400 for invalid payload, 405 if method not allowed (no GET), etc.\n- Provide a short code snippet showing the FastAPI endpoint function and the core idempotency check (you can assume a Redis client named redis_client is already connected).\n\nSample payload:\n{\n  \"event_id\": \"evt_12345\",\n  \"event_type\": \"payment_succeeded\"\n}\n\nAnswer should be concise and only include the code snippet and brief explanation.",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role (payment webhooks, idempotency, Redis) with exact tech stack (Python/FastAPI). Clarity: Crystal clear requirements, well-scoped, sample payload helps; minor deduction for not specifying Redis commands explicitly. Discriminative: Tests applied skills in validation, idempotency, HTTP handling\u2014distinguishes those who can implement vs. theorize; concise scope fits screening.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-487",
        "role_title": "Senior Backend Engineer - Search Ranking",
        "question": "Question:\nYou are building a learning-to-rank (LTR) pipeline for a product search engine. You have a dataset of queries and candidate documents with the following features:\n- tf-idf similarity between query and document\n- PageRank-like popularity score\n- click-through-rate (CTR) feature from prior rankings\n- availability flag (in stock)\n- numeric length of the document title\n\nYou plan to train a pairwise RankNet-style model (or any pairwise ranking objective) to optimize ranking quality.\n\nPlausible quick tasks:\n- Propose a minimal but effective feature preprocessing/normalization strategy for these features before feeding them into a neural/LTR model (briefly justify).\n- Outline how you would construct training pairs from a labeled dataset (assume you have user click data indicating preferred documents for some queries).\n- Describe a simple evaluation metric and how you would validate improvements in an A/B test.\n\nProvide concrete steps or pseudocode snippets (2\u20134 bullet points) for preprocessing, pair construction, and evaluation. Keep it high-level and implementable within a few minutes.",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's LTR pipeline focus (features, model training, A/B testing) with exact tech (pairwise RankNet); no red flags. Clarity: Crystal clear, well-scoped with specific features/tasks, concrete instructions, minor vagueness on exact pseudocode depth (-1). Discriminative: Tests applied LTR knowledge (preprocessing trade-offs, pair construction, eval/A/B), requires reasoning over recall, strong separation; concise scope avoids time penalty.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-489",
        "role_title": "Backend Engineer - Data Quality",
        "question": "Question:\nYou have a daily Airflow DAG that loads a table fact_sales in Snowflake. A data quality check should ensure that:\n- the record count is within \u00b15% of the previous day\u2019s count, and\n- there are no nulls in the important dimension key column dim_store_id in the loaded data.\n\nProvide a compact Python snippet (to be used in a lightweight test) that:\n1) retrieves yesterday\u2019s row count from Snowflake for fact_sales,\n2) compares it to today\u2019s count and prints \"OK\" if within \u00b15% or raises an exception if not,\n3) checks that today\u2019s loaded data has no NULL values in dim_store_id and raises an exception if any NULLs exist.\n\nAssume you have a Snowflake connection object (ctx) from the Snowflake Python connector, and you can execute SQL via ctx.cursor().execute(sql). You may also assume a simple helper function run_sql(sql) that returns a list of tuples for results. Keep the snippet concise (roughly 15-25 lines).",
        "scores": {
          "relevance": 9,
          "clarity": 10,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core data quality skills (volume anomalies, null checks) for warehouse pipelines using exact tech stack (Airflow, Snowflake); minor -1 for no freshness/schema drift but hits main requirements. Clarity: Crystal clear with precise requirements, assumptions, constraints, and scope. Discriminative: Requires applied SQL/Python integration, threshold logic, and error handling; distinguishes those who can implement concisely from trivia knowers.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-490",
        "role_title": "Senior Backend Engineer - Environment Management",
        "question": "Question:\nYou are designing a simple Go utility that ensures each developer preview deployment gets a unique Kubernetes namespace with a 24-hour TTL. Given a base namespace prefix \"team-a-\" and a deployment name \"ui-review-123\", describe (with code or pseudocode) how you would:\n\n- Generate a deterministic, unique namespace name that avoids collisions.\n- Create a Kubernetes namespace object with a 24-hour TTL annotation and a ResourceQuota limiting CPU and memory (e.g., 100m CPU, 128Mi memory) for this namespace.\n- Implement a lightweight TTL cleanup check that you could run periodically to delete namespaces older than 24 hours, without enumerating all namespaces every time.\n\nNotes:\n- Assume you have a Kubernetes client-go client configured.\n- Focus on the critical logic, not full error handling or client setup.",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core needs (namespace isolation, TTL cleanup, resource quotas) using Go and Kubernetes directly. Clarity: Crystal clear, well-scoped with specifics (prefix, example, limits, assumptions); minor deduction for no exact time constraint. Discriminative: Tests applied Kubernetes API knowledge, deterministic naming trade-offs, TTL annotation/expiry logic, and efficient cleanup strategy\u2014strong separation of strong vs weak candidates.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-493",
        "role_title": "Staff Engineer - Reliability Standards",
        "question": "Question:\nYou are tasked with evaluating a production service running on Kubernetes with a PostgreSQL database. The service experiences a 5\u201310 minute BLIP once per week where latency spikes and error rates rise temporarily, then recover without human intervention.\n\n- Outline a concise, practical incident response plan you would expect the team to follow during such an event (include roles, data to collect, and steps to mitigate).\n- Propose two concrete reliability improvements (with minimal implementation effort) that would reduce the likelihood or impact of this type of incident, leveraging Go, Kubernetes, Prometheus, and PostgreSQL.\n- Explain how you would measure the impact of these changes and what success metrics you would track after deployment.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests SRE practices, incident response, reliability improvements, and production readiness\u2014core to role; uses relevant tech stack (Kubernetes, PostgreSQL, Go, Prometheus) without extras (-0 red flags). Clarity: Well-scoped with specific scenario, clear sub-parts, no ambiguity or missing constraints. Discriminative: Requires applied SRE knowledge, trade-offs in low-effort fixes, and metrics\u2014distinguishes deep understanding from basics; not recall or trivial.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-486",
        "role_title": "Backend Engineer - Span Processing",
        "question": "Question:\n\nYou are implementing a lightweight span processor in Go that performs tail sampling based on a simple rule: if a trace contains at least one span with an attribute \"error\" set to true, or if the trace duration exceeds a configurable threshold (in milliseconds), then the entire trace should be kept; otherwise, discard it. The processor receives spans from a Kafka topic as they arrive, buffers them per trace_id until the root span arrives (or until a timeout of 2 minutes), then decides whether to forward the trace to ClickHouse for storage or drop it.\n\n- Outline a minimal, idiomatic Go implementation approach (data structures and flow) to:\n  - Buffer spans by trace_id with a per-trace timeout.\n  - Detect the two tail-sampling conditions efficiently.\n  - Forward only qualifying traces to an output channel for downstream ingestion.\n- Provide a concise code snippet (pseudo-Go with real constructs) showing:\n  - The per-trace buffering mechanism using a map and a background cleaner.\n  - How you would evaluate the two conditions once the trace is complete or the timeout fires.\n- Explain how you would ensure this is safe for concurrent access and scalable under moderate load. No need to implement Kafka/ClickHouse I/O; just the in-memory logic.\n\nConstraints:\n- Assume spans arrive as: type Span struct { TraceID string; SpanID string; ParentID string; DurationMs int; Attributes map[string]interface{} }\n- The root span has ParentID == \"\".\n- The input stream guarantees spans for a given trace arrive in a single process, but multiple traces can be in flight concurrently.\n- Keep the answer concise (2\u20135 minutes of coding thought).",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (span processing, tail sampling, buffering by trace_id) in Go with exact tech (distributed tracing concepts, ClickHouse mention); minor ding for no Kafka/ClickHouse code but role-appropriate. Clarity: Crystal clear scope, precise constraints, unambiguous tasks with time guide; no bundling or vagueness. Discriminative: Requires applied Go knowledge (concurrency, maps, timeouts, idiomatic patterns), trade-offs in buffering/cleaning, distinguishes strong candidates via scalability/safety explanations.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-488",
        "role_title": "Backend Engineer - Gaming Backend",
        "question": "Question:\nYou\u2019re implementing a real-time lobby for a multiplayer game using Go, WebSocket, Redis, PostgreSQL, and Protocol Buffers.\n\nScenario: Players join a matchmaking queue. When at least 4 players are ready, they should be grouped into a game room, and all participants must receive a single, synchronized start message with a unique game session ID. The system must be resilient to a single player disconnecting before the match starts, in which case the remaining players should either be reassigned or the match should be canceled and players refunded to the queue if no enough players remain.\n\nTask:\n- Outline a small Go function or set of functions (pseudocode acceptable) that demonstrates:\n  - How you would atomically move 4 players from a Redis-based waiting queue into a Redis-backed \u201clobby\u201d or \u201croom\u201d with a unique session ID.\n  - How you would broadcast a synchronized start message to all 4 connected WebSocket clients using Protocol Buffers, ensuring all clients receive it at roughly the same time.\n  - How you would handle a disconnect of one player before the start, including which data structures you\u2019d modify and how you\u2019d decide to cancel or re-match.\n- Briefly describe how you\u2019d persist the final match metadata to PostgreSQL for auditing (e.g., session_id, player_ids, start_time).\n\nConstraints:\n- Emphasize atomicity and consistency in the matchmaking step.\n- Mention any Redis structures (e.g., lists, sorted sets, Lua scripts) you would use.\n- Include a short note on how you would generate and serialize the Protocol Buffers start message.\n\nProvide only the code-level outline or pseudo-code with concise explanations. time: 2\u20135 minutes.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (matchmaking, state sync, resilience) with exact tech stack (Go, WebSocket, Redis, PostgreSQL, Protobuf); minor deduction as anti-cheat not covered. Clarity: Crystal clear scenario, precise tasks, explicit constraints, scoped time limit. Discriminative: Requires applied knowledge of atomic Redis ops (Lua/scripts), WebSocket broadcast sync, disconnect handling trade-offs, and persistence\u2014strong separation of strong vs weak candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-492",
        "role_title": "Backend Engineer - CD Pipelines",
        "question": "Question:\nYou have a Kubernetes cluster managed with ArgoCD. A new feature branch has updated a Helm release for a microservice named \u201cpayments-service,\u201d and you want this change to be automatically promoted to the staging environment after a successful commit, but require a human approval before promoting to production.\n\nDescribe, at a high level, the minimal steps and configurations you would implement in ArgoCD and GitHub Actions (or your CI/CD tool) to achieve:\n- Automatic deployment to staging upon merge to the feature branch\n- Manual approval gate before promotion to production\n- A rollback plan if the staging deployment fails health checks\n\nIn your answer, include:\n- Brief outline of ArgoCD Application definitions or annotations you would use\n- An example of the GitHub Actions workflow triggers and a single job step that performs the promotion with a manual gate\n- How you would configure health checks or sync waves to enable safe promotion\n- How rollback would be triggered automatically if health checks fail on production\n\nTime: 2\u20135 minutes to answer.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core GitOps, ArgoCD, multi-env promotion, approval gates, and rollback\u2014perfect match to role; minor ding for assuming GitHub Actions. Clarity: Crystal clear with specific requirements, scoped by time, no ambiguity. Discriminative: Requires applied knowledge of integrations, configs, trade-offs in health checks/rollbacks\u2014strong separation of strong vs weak candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-495",
        "role_title": "Senior Backend Engineer - Audit Logging",
        "question": "Question:\nYou are designing a tamper-evident audit log in Go using a PostgreSQL-backed append-only table and a separate hash chain to detect tampering. Describe, at a high level, how you would implement:\n\n- The data model (tables/columns) to store log entries and their integrity proofs.\n- How you would compute and store a hash chain (e.g., per-entry hash chaining) and how you would verify integrity on read.\n- A simple API contract (Go function signatures) for appending a log entry and for querying a range of entries with integrity verification.\n- A basic retention/purging strategy that preserves integrity guarantees.\n\nAssume you must:\n- Ensure that any modification to past entries is detectable.\n- Maintain efficient writes (high throughput) and fast reads for verification.\n- Be able to recover integrity after downtime.",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Relevance: Perfectly matches role's audit logging, tamper-proof storage, query interfaces, and retention needs using specified Go/PostgreSQL stack. Clarity: Crystal clear with precise scope, assumptions, and components; minor deduction for no explicit time constraint. Discriminative: Requires deep applied knowledge of hashing, data modeling, API design, and integrity trade-offs, strongly separating strong candidates.",
          "composite": 9.666666666666666
        }
      },
      {
        "role_id": "be-496",
        "role_title": "Junior Backend Engineer - Model Serving",
        "question": "Question:\nYou have a FastAPI app that serves a machine learning model loaded from disk at startup. To improve throughput, you want to batch incoming requests per second before passing them to the model, and you want to implement a simple per-request latency measurement.\n\n- Outline a minimal FastAPI snippet (pseudo-code acceptable) that:\n  - Loads a PyTorch/NumPy model once at startup.\n  - Uses a simple in-process batching mechanism that collects requests arriving within a 100 ms window and processes them together.\n  - Returns individual results to each client, mapping the batch results back to the corresponding requests.\n  - Measures and logs per-request latency.\n\n- Briefly explain how you would test this locally (e.g., with curl or a small script) and what you would watch for in logs to ensure correctness and performance.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (FastAPI ML model deployment, batching, monitoring); minor ding as PyTorch/NumPy not explicitly listed but fits ML context (-1). Clarity: Crystal clear, well-scoped with specific requirements, pseudo-code flexibility. Discriminative: Requires applied knowledge of async batching, request mapping, latency tracking, and testing\u2014strong separation of strong entry-level candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-491",
        "role_title": "Junior Backend Engineer - Read Replicas",
        "question": "Question:\nYou have a Python application using PostgreSQL with a read-replica setup (primary and one read replica). The app uses SQLAlchemy and should route read queries to the replica when the replica is healthy, but fall back to the primary if the replica is lagging or unavailable.\n\nDescribe a lightweight approach (pseudo-code or actual Python snippet) to:\n- Detect replica lag before routing a read query.\n- Route the read query to the replica when lag is below a defined threshold (e.g., 5 seconds).\n- Fall back to the primary if the replica is lagging or unreachable.\n- Ensure writes always go to the primary.\n\nAssume you have:\n- Two SQLAlchemy engines: engine_primary and engine_replica.\n- A function get_replica_lag() that returns the replication lag in seconds, or raises an exception if the replica is unreachable.\n\nProvide the code for a small utility function get_read_connection() that returns the appropriate connection/engine for a read query, and explain how you would integrate it into query execution.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (replica routing, lag handling) with exact tech stack (Python, PostgreSQL, SQLAlchemy); minor deduction as entry-level but perfectly matches focus (-1 generic potential). Clarity: Crystal clear with explicit assumptions, scoped to one utility function, no ambiguity. Discriminative: Requires applied knowledge of routing logic, error handling, and integration; tests trade-offs in lag thresholds and fallbacks, distinguishes strong practical understanding.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-494",
        "role_title": "Backend Engineer - API Mocking",
        "question": "Question:\nYou are building a simple API mocking service in Node.js using TypeScript. The mock should read a predefined contract that specifies an endpoint, an HTTP method, a request body pattern, and a canned response. Implement a small function that, given:\n\n- method: string (e.g., \"POST\")\n- path: string (e.g., \"/users\")\n- body: any (the incoming request body)\n\nreturns the canned response if the incoming request matches the contract, or null if it does not.\n\nContract example (in code form):\n[\n  {\n    \"method\": \"POST\",\n    \"path\": \"/users\",\n    \"bodyPattern\": { \"email\": \".*@example.com\" },\n    \"response\": {\n      \"status\": 201,\n      \"headers\": { \"Content-Type\": \"application/json\" },\n      \"body\": { \"id\": 123, \"email\": \"user@example.com\" }\n    }\n  }\n]\n\nNotes:\n- The bodyPattern should be treated as a simple subset match using RegExp on any string field values (e.g., email) within the request body. If a field is not present in the incoming body, treat as non-match.\n- The function should be fast and side-effect free.\n- You can assume the incoming body is a plain JSON object with string values only for fields you need to evaluate.\n\nProvide the TypeScript implementation of this function (you can hard-code the contract array in the file). Include a few inline comments explaining the matching approach. The function signature should be something like:\nfunction matchMock(method: string, path: string, body: any): { status: number; headers: Record<string, string>; body: any } | null\n\nAim for concise, clear, and correct matching logic suitable for a mid-level engineer.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core API mocking skills (request matching, response templating) in Node.js/TS, matching role focus (-0 generic red flags). Clarity: Crystal clear with precise spec, example, constraints, and signature; well-scoped for screening. Discriminative: Requires applied knowledge of pattern matching (RegExp subset), recursion, TS typing; distinguishes mid-level engineers via trade-offs in fast/side-effect-free impl.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-500",
        "role_title": "Staff Engineer - Streaming Architecture",
        "question": "Question:\nYou need to design a real-time event path from a producer service to multiple downstream consumers (analytics, alerting, and a live dashboard). The system uses Rust, Kafka, Redis, and WebSocket, deployed on Kubernetes. Briefly describe a concrete, code-ish approach to ensure:\n- At-least-once delivery semantics from producer to Kafka\n- Efficient fan-out to Redis pub/sub channel and to a WebSocket feed for connected clients\n- Backpressure handling when downstream consumers lag\n- A minimal schema and serialization choice suitable for both Kafka and Redis, with versioning\n\nProvide:\n- A high-level data flow diagram (textual is fine)\n- The core Rust components or modules you would implement (names and responsibilities)\n- How you would implement idempotency and deduplication across retries\n- How you would monitor and observe lag and backpressure\n\nKeep it concise but show concrete, implementable ideas. The answer should be doable in 2\u20135 minutes.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core streaming architecture skills (event schemas, delivery guarantees, fan-out) and exact tech stack (Rust, Kafka, Redis, WebSocket, K8s); minor deduction for not emphasizing schema design as much (-1). Clarity: Crystal clear, well-scoped with explicit requirements, time guidance, and output format; no bundling or ambiguity. Discriminative: Excellent at separating candidates via trade-offs in delivery, backpressure, idempotency, fan-out, schema/versioning, and observability; concrete 'code-ish' ask tests deep applied knowledge without being too broad.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-498",
        "role_title": "Senior Backend Engineer - Profiling Infrastructure",
        "question": "Question:\nYou are adding a new feature to a continuous profiling service (Pyroscope-based) that collects CPU profiles from multiple Go services and stores them in ClickHouse for long-term analysis. A user should be able to compare their current baseline profile against a recent profile and view a simple flame-graph-like summary.\n\nProvide a concise, concrete plan (pseudo-commands or steps) to implement this:\n\n- How you would generate and store a short-term baseline profile alongside recent profiles, ensuring minimal storage impact.\n- How you would implement a quick, approximate comparison between the baseline and a target profile to identify the top hot functions (without rendering a full flame graph).\n- What data schema or minimal ClickHouse table structure you would use to support baseline + recent comparisons and efficient querying.\n\nConstraints:\n- Assume Go services emit profiles in the standard pprof-like format and Pyroscope is used for collection.\n- The solution should be executable within a 2\u20135 minute screening window (high-level steps, not full code).\n- Focus on observability and storage optimization considerations suitable for a senior backend engineer.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (profiling, flame graphs, baseline comparison, ClickHouse storage optimization) with exact tech stack (Pyroscope, Go, pprof); minor deduction as role doesn't explicitly mention Go/Pyroscope but aligns perfectly (-1 generic feel avoided). Clarity: Crystal clear with precise scope, constraints, and expected format; well-scoped for screening. Discriminative: Requires applied knowledge of profiling formats, storage trade-offs, schema design, and approximations\u2014distinguishes seniors via optimization reasoning and tech integration.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-497",
        "role_title": "Backend Engineer - Payout Systems",
        "question": "Question:\nYou are designing a payout batch job in Go that processes a daily settlements file and updates PostgreSQL with payout statuses. The file contains lines: payer_id, recipient_id, amount_cents, bank_account, status (PENDING/PAID/FAILED). The job must:\n- Read the file, validate that amount is > 0, and bank_account matches a simple Luhn check (you can assume a helper function isValidBankAccount(account string) bool exists).\n- Upsert a payout record for each line with columns: id (serial), payer_id, recipient_id, amount_cents, bank_account, status, processed_at (timestamp).\n- If any line is invalid, the entire batch should fail (atomicity). Use a transaction to ensure that either all lines are applied or none.\n- After successful commit, publish a message to a Kafka topic \"payouts.completed\" with the payout IDs array.\n\nProvide a concise Go code snippet (pseudo-implementation as needed) that:\n- Opens a DB transaction, reads lines from a provided reader, performs validation (including the bank account check), collects valid payout rows, and on success inserts them in a single batch within the transaction, then commits.\n- Demonstrates how you would handle rollback on validation failure and how you would publish to Kafka after commit.\n- Uses prepared statements where appropriate.\n\nAssume you have:\n- db *sql.DB\n- a function isValidBankAccount(account string) bool\n- a kafkaProducer with a method Produce(topic string, key string, value []byte) error\n- payouts table with columns id, payer_id, recipient_id, amount_cents, bank_account, status, processed_at\n\nKeep it concise (2-5 minutes for a candidate).",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (batch processing, validation, PostgreSQL upserts, atomicity) in exact tech stack (Go, PostgreSQL, Kafka); minor deduct for not explicitly mentioning reconciliation. Clarity: Crystal clear with precise requirements, constraints, assumptions, and scope (2-5 min, concise snippet). Discriminative: Requires applied knowledge of transactions, batch inserts, validation, rollback, Kafka integration, and prepared statements\u2014tests trade-offs in error handling and atomicity, strongly separates strong Go/backend engineers.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-499",
        "role_title": "Principal Engineer - Search Platform",
        "question": "Question:\nYou are designing a centralized search platform for an enterprise with multiple microservices emitting events to Kafka and storing indexed data in Elasticsearch. A new requirement is to support near-real-time search across customer records, but with strict field-level access control (row-level security) per department.\n\n- Outline the high-level architecture you would propose (components, data flow, and how you would achieve near-real-time indexing) using Java, Elasticsearch, Kafka, Kubernetes, and Redis.\n- Describe how you would implement field-level access control at query time without duplicating data or compromising performance. Include data model considerations, security boundaries, and a brief outline of the code or configuration approach you would use.\n- Identify potential operational challenges and how you would address monitoring, scaling, and fault tolerance.\n\nProvide concise bullet points you would share in a design review. (2\u20135 minutes to answer)",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core search platform architecture, strategy, and enterprise-scale ops matching role; uses exact tech stack implied (ES, Kafka) with no red flags (-0). Clarity: Crystal clear, well-scoped with time limit, single focused scenario, specific components listed; minor multi-part but cohesively tied (-0). Discriminative: Excellent separation via trade-offs (perf vs security), deep applied knowledge, ops challenges; not trivial recall, scoped for screening.",
          "composite": 9.0
        }
      }
    ]
  }
}