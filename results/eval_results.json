{
  "gpt-5-nano": {
    "model": "gpt-5-nano",
    "num_samples": 100,
    "avg_relevance": 9.21,
    "avg_clarity": 9.1,
    "avg_discriminative": 9.12,
    "avg_composite": 9.143333333333334,
    "results": [
      {
        "role_id": "be-401",
        "role_title": "Backend Engineer - Training Infrastructure",
        "question": "Question:\nYou need to submit a simple Ray-based training task on a single node with multiple GPUs. Write a minimal Python snippet that:\n\n- Uses Ray to launch 2 parallel train tasks.\n- Each task should simulate a training step by burning 0.5 seconds and then return a unique scalar loss value (e.g., 0.9 and 0.8) based on the task id.\n- After launching, collect the results and print the average loss.\n\nConstraints:\n- Use ray.init() local mode or standard mode as you prefer.\n- Do not rely on external files or GPUs in the snippet (you can simulate the GPU with sleeps).\n- Provide clear comments explaining how you would extend this to multiple nodes and GPUs in a real cluster.",
        "scores": {
          "relevance": 9,
          "clarity": 10,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests Ray (core for distributed ML training orchestration, job scheduling, GPU allocation per role); minor deduction as role emphasizes multi-node but question scopes to single node with extension (-1). Clarity: Crystal clear, well-scoped with precise constraints, comments requirement, no ambiguity. Discriminative: Tests applied Ray usage (ray.init, remote tasks, result collection), simulation forces understanding of parallelism, extension comments probe deeper cluster knowledge and trade-offs.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-402",
        "role_title": "Backend Engineer - Model Deployment",
        "question": "Question: You have a trained ML model you want to deploy on KServe with a rolling deployment strategy that can automatically rollback on performance cliff.\n\n- Provide a concise Python snippet (pseudo-code acceptable) that:\n  - Creates a KServe InferenceService for a model served from a container image.\n  - Configures a rolling rollout split of 20% canary and 80% stable.\n  - Adds a basic automated rollback condition: if external monitoring indicates AUC drops by more than 5% compared to the stable baseline for the last N minutes, trigger rollback to 100% stable.\n- Briefly explain how you would integrate a Redis-based metric flag as the trigger (what data you\u2019d store and how the service would read it).\n\nConstraints:\n- Focus on the core deployment definitions and rollback trigger logic (no need to implement full monitoring system).\n- Keep the code concise enough to fit in 2-5 minutes of reading.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (model deployment, KServe, rolling strategies, rollback automation); minor ding as KServe isn't explicitly named but fits MLOps focus (-1). Clarity: Crystal clear with precise scope, constraints, and time guide; no ambiguity. Discriminative: Requires applied KServe YAML/Python knowledge, rollout config, and integration design with trade-offs, strongly separating experienced MLOps engineers.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-410",
        "role_title": "Backend Engineer - Bulk Operations",
        "question": "Question:\nDesign a small Go function signature and outline an in-memory approach to implement idempotent bulk upsert operations (insert or update) for a batch of records, using a unique client-provided operation_id to ensure exactly-once semantics. Assume records have fields: id (UUID), data (string), and operation_id (string). Your answer should include:\n\n- The function signature (inputs/outputs) for processing a batch of records.\n- How you would handle partial failures within the batch (e.g., some records succeed, some fail) and report per-record status.\n- A high-level outline of how you\u2019d achieve idempotency using operation_id, including any data structures or storage considerations (no need for full code, but show critical steps and error handling).\n\nConstraints to consider:\n- Must work with a PostgreSQL-backed store and a Redis cache per the stack.\n- Emphasize practicality for bulk operations in a mid-senior Go role.",
        "scores": {
          "relevance": 9,
          "clarity": 10,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core role skills (bulk ops, idempotency, partial success) with exact Go/PostgreSQL/Redis stack (-1 minor for not deeply testing batch processing internals). Clarity: Crystal clear, well-scoped with specific requirements, constraints, and output expectations. Discriminative: Tests applied design knowledge, trade-offs (partial failure, storage), and mid-senior depth without being trivial or overly broad.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-407",
        "role_title": "Staff Backend Engineer - ML Infrastructure",
        "question": "Question: You are asked to design a simple, cost-efficient training job launcher for a Python-based ML workload using Ray on Kubernetes. The workload runs a PyTorch training script with a single node and 4 GPUs. Describe (in ~3\u20135 sentences or bullet points) how you would:\n\n- package and submit the job so it can be executed reproducibly on a Kubernetes cluster using Ray.\n- ensure the driver and workers are scaled appropriately (e.g., when using Ray with a small budget vs. when more resources are available).\n- minimize idle GPU and pod costs between runs (e.g., using preemption, autoscaling, or job cancellation).\n\nInclude any key Ray or Kubernetes components you would leverage and any minimal YAML or command-line snippet you would use to launch a single-trainer, multi-GPU job.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core ML infra skills (Ray on K8s, cost optimization, training platforms) matching role; minor deduction as PyTorch is specified but not core to infra leadership (-1). Clarity: Crystal clear, well-scoped with constraints (single node/4 GPUs, 3-5 sentences, specific subpoints, YAML snippets), no ambiguity. Discriminative: Requires applied knowledge of Ray/K8s integration, scaling trade-offs, and cost strategies, strongly separates experienced ML infra engineers from juniors.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-408",
        "role_title": "Junior Backend Developer - Health Endpoints",
        "question": "Question:\nIn a Go-based backend service, implement a simple HTTP health check endpoint and a readiness check endpoint. The readiness check should return 200 OK only when an in-memory slice of \"jobs\" is non-empty and a boolean variable isReady is true. The health check should always return 200 OK with a JSON body {\"status\":\"ok\"}. Provide a minimal Go code snippet (no external packages beyond the standard library) that wires these two endpoints using the http package, and show how you would simulate toggling readiness and jobs in code comments.\n\nWhat to include:\n- A main function setting up two HTTP handlers: /health and /ready\n- /health returns status 200 with body {\"status\":\"ok\"}\n- /ready returns 200 only when len(jobs) > 0 && isReady is true; otherwise returns 503 with body {\"status\":\"not-ready\"}\n- A simple in-memory declaration of the variables: var jobs []string and var isReady bool, with comments on how they would be updated in real scenarios.",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core focus on health/liveness/readiness endpoints in Go backend, directly testing exact skills needed (no red flags). Clarity: Crystal clear with precise requirements, paths, responses, and constraints; minor deduction for lacking explicit time/format limits. Discriminative: Requires applied Go knowledge (HTTP handlers, JSON, logic, state), distinguishes candidates able to write functional code vs. superficial knowledge; tests trade-offs in endpoint wiring.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-406",
        "role_title": "Senior Backend Engineer - LLM Serving",
        "question": "Question:\nYou are deploying a vLLM-based inference service behind a Kubernetes cluster. You want to implement a simple batching strategy that groups incoming requests for the same model to maximize throughput, while ensuring streaming responses are delivered to clients in the correct order and with minimal latency.\n\n- Describe how you would design a lightweight Python component (no external frameworks required) that:\n  - Buffers incoming requests for a given model/version.\n  - Creates batches up to a maximum batch size or a max wait time.\n  - Dispatches batches to vLLM and streams results back to the respective client connections in the order of their original requests.\n  - Maintains per-request KV cache warm-start where possible to reduce latency (conceptually, you can describe how KV cache indices would be reused across requests in the same batch).\n\n- Provide a concise pseudocode or code sketch (\u224820-40 lines) illustrating the batching loop, request tracking, and streaming of results.\n\n- Highlight potential race conditions or edge cases and how you would mitigate them.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Relevance: Directly targets core role responsibilities (batching optimization, KV cache management, streaming) using vLLM in K8s context; no red flags. Clarity: Well-scoped with specific requirements, constraints (lightweight Python, no frameworks, concise code), unambiguous despite multiple subparts. Discriminative: Requires applied systems knowledge, trade-offs in batching/KV reuse, race condition analysis, and practical code sketch\u2014excellent separation of strong vs weak candidates.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-409",
        "role_title": "Junior Backend Engineer - File Upload",
        "question": "Question:\nIn a FastAPI app, implement a small endpoint that accepts a file upload via multipart/form-data and saves it to an S3 bucket with a generated UUID filename. The answer should include:\n\n- A FastAPI route definition that accepts a file upload (UploadFile) and uses appropriate validation (e.g., max file size of 5 MB).\n- Reading the file content in chunks and streaming it to S3 without loading the entire file into memory.\n- Generation of a unique S3 object key using UUID.\n- Minimal error handling for common failures (e.g., missing file, upload error).\n- Pseudocode or actual code snippet showing the AWS S3 client usage (you can assume credentials are configured).\n\nProvide just the code snippet (no explanations).",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core file upload API skills (multipart, validation, S3 streaming) matching role; minor deduction as FastAPI/S3 not explicitly listed but implied by context (-1). Clarity: Precise scope, constraints, and format ('just code') make it unambiguous and well-scoped. Discriminative: Requires applied integration of FastAPI UploadFile, streaming, UUID, S3 client, and error handling\u2014tests practical skills with trade-offs (memory efficiency), distinguishing strong entry-level candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-403",
        "role_title": "Backend Engineer - Inference Optimization",
        "question": "Question:\nYou have a PyTorch model that you want to deploy with TensorRT for low-latency inference. You can only use a single GPU and must support dynamic input batch sizes at inference time.\n\n- Outline a concise, step-by-step approach to convert the model to TensorRT with minimal latency impact, including handling dynamic batch sizes.\n- Provide a minimal Python snippet that:\n  - Loads a TorchScript or ONNX model.\n  - Creates a TensorRT engine with a dynamic batch dimension.\n  - Performs an inference with a batch size that can vary between 1 and 32.\n- Highlight at least two pitfalls you would watch for (e.g., memory usage, dynamic shapes, or calibration needs) and how you would mitigate them.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core inference optimization skills (TensorRT conversion, dynamic batching, low-latency) matching role's quantization/batching/GPU focus; minor ding as PyTorch/TensorRT not explicitly listed but perfectly aligned (-1). Clarity: Well-scoped steps with precise constraints, unambiguous phrasing. Discriminative: Requires applied knowledge of workflow, code, and trade-offs/pitfalls for strong separation; not mere recall.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-404",
        "role_title": "Senior Backend Engineer - Feature Platform",
        "question": "Question:\n\nYou are designing a simple online feature store surface for a ML team. You have a streaming source of user events (user_id, event_time, event_type) and a batch table of user features (user_id, feature_a, feature_b, as_of_time). The online serving layer must return the latest values of feature_a and feature_b for a given user_id at a requested query_time.\n\n- Outline (in bullets or a short snippet) how you would implement this using Python, considering:\n  - How you would structure the data (e.g., which data stores, keys, and timestamps).\n  - How to ensure consistency between batch features and streaming updates.\n  - How to handle late-arriving events and out-of-order feature updates.\n\n- Provide a minimal code sketch (pseudo-Python) showing:\n  - A function get_features(user_id, query_time) that returns the latest feature_a and feature_b as of query_time.\n  - How you would update the online store with a new streaming event to keep features current.\n\nAssume you are using a simple in-memory structure for the sketch, but describe the data flow and any assumptions you make about Feast-like schemas or typical feature registry considerations.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core feature store skills (online serving, batch/streaming, timestamps) matching role's feature computation/serving/discovery; minor ding for assuming Feast-like without role spec (-1). Clarity: Well-scoped with specific data, clear asks (outline + code), constraints provided. Discriminative: Probes applied knowledge of consistency, late events, point-in-time queries\u2014reveals trade-offs and depth beyond basics.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-405",
        "role_title": "Senior Backend Engineer - Model Monitoring",
        "question": "Question:\nYou have a Python service that serves a simple binary classifier and logs model output to Prometheus metrics: prediction probability (float 0-1) and a binary decision (0/1). Write a short Python snippet (\u226420 lines) that calculates a simple drift indicator by comparing the current batch\u2019s mean prediction probability to a baseline mean, and emits a Prometheus gauge metric named model_drift_score with the absolute difference. Assume:\n- baseline_mean is already computed and available as a float.\n- current_batch_probs is a list of prediction probabilities for the current batch.\n- You already have a Prometheus Gauge named model_drift_score created (from prometheus_client import Gauge) as model_drift_score.\n- You should not rely on external services beyond Prometheus client.\n\nEdge cases: handle empty current_batch_probs gracefully (no emission). Provide a brief note on how you would trigger an alert when drift_score exceeds a threshold in a real system.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 8,
          "reasoning": "Relevance: Directly tests core ML monitoring skills (drift detection via prediction probs, Prometheus metrics) matching role's focus on model monitoring/alerting; minor ding for not specifying exact tech stack but assumes Python/Prometheus reasonably (-1). Clarity: Crystal clear with precise constraints (\u226420 lines, assumptions listed, edge cases specified), unambiguous scope. Discriminative: Tests applied ML ops knowledge (drift calc, metrics emission, edge handling) with good separation; brief alert note adds insight without being recall (-2 for short answer component).",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-413",
        "role_title": "Senior Backend Engineer - API Gateway Logic",
        "question": "Question:\nYou are implementing a simple request transformation in an Envoy-based API gateway using Lua filter. Given an incoming HTTP request with a JSON body and headers, implement a Lua snippet that:\n- Reads the JSON body, extracts the field \"user_id\".\n- Adds a new header \"X-User-ID\" with that value.\n- If \"user_id\" is missing, returns a 400 Bad Request with a JSON body: {\"error\":\"user_id required\"} and stops further processing.\n\nAssume the body is a JSON object and the content-type is application/json. Provide:\n- The Lua code snippet (to be placed in the Envoy Lua filter).\n- A brief explanation of how it ensures 400 response early if missing.\n- Any caveats you\u2019d consider in a real deployment (e.g., streaming bodies, large payloads, idempotency).",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core API gateway skills (request transformation, Lua filters) matching role's Envoy-based extensions (-0 red flags). Clarity: Precise scope, clear requirements, well-constrained for screening. Discriminative: Requires applied Lua/Envoy knowledge, error handling, and real-world trade-offs, strongly separating skilled candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-414",
        "role_title": "Senior Backend Engineer - Long-Polling",
        "question": "Question: \n\nYou are designing a long-polling endpoint in Go that clients poll with a session ID. If there is no new data, the server should hold the request up to a maximum timeout (e.g., 30 seconds) and then respond with either new data or a timeout indication. Implement a concise, self-contained pseudo-implementation or outline (you can describe code blocks or provide small Go snippets) that demonstrates:\n\n- How you would manage per-request lifecycle (context cancellation, timeouts) and avoid goroutine leaks.\n- How you would integrate a lightweight in-memory notification mechanism (e.g., a per-session channel or pub/sub) to wake up the poll when new data becomes available.\n- How you would handle a client disconnect (HTTP connection closed) and ensure resources are released promptly.\n- How you would fall back to a max-timeout response if no data arrives.\nAssume Go, no external dependencies beyond standard library (you may mention Redis as a backend concept but focus on in-process approach for the long-poll). Provide a brief explanation of the trade-offs.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (long-polling APIs, connection mgmt, timeouts, fallbacks) in Go with exact tech match; minor Redis mention is conceptual only (-0). Clarity: Crystal clear, well-scoped with specific bullets, constraints (std lib, concise), no ambiguity. Discriminative: Requires applied Go knowledge (context, channels, goroutines, leaks), trade-offs distinguish deep understanding; not trivial recall, appropriately scoped for screening.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-411",
        "role_title": "Backend Engineer - Admin APIs",
        "question": "Question:\nDesign a small FastAPI endpoint that allows an admin to deactivate a user account. The system uses PostgreSQL for user data and Redis for short-lived admin session tokens. Provide a concise code snippet (you can assume SQLAlchemy-like ORM and Redis client) that:\n\n- Validates that the request includes a valid admin session token in a header (e.g., X-Admin-Token).\n- Checks that the token is present in Redis with a non-expired TTL.\n- Deactivates the user by setting an is_active flag to False in PostgreSQL for a given user_id path parameter.\n- Writes a simple audit log entry to stdout (or a provided audit_log function) with: timestamp, admin_id, action=\"deactivate_user\", target_user_id.\n\nAssumptions you can make:\n- User model has id (int) and is_active (bool).\n- Redis client is available as redis_client and stores tokens as key: \"admin:token:<token>\" with value containing admin_id.\n- The function should be self-contained within a FastAPI route handler, focusing on the flow rather than full app setup.\n\nProvide the code snippet.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests FastAPI, PostgreSQL, Redis, auth validation, audit logging - core to admin APIs (minor -1 for assuming ORM/Redis client detail). Clarity: Precise scope, clear requirements, well-constrained for screening. Discriminative: Requires integrating auth, DB ops, Redis TTL check, auditing with error handling - tests applied role skills and trade-offs effectively.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-417",
        "role_title": "Junior Backend Developer - Event Sourcing",
        "question": "Question:\nYou are building a simple event-sourced order service. An Order can be created and then each item addition is represented as an event. There is a separate Read Model (projection) that maintains the total order value.\n\nGiven the following events:\n- OrderCreated { orderId: \"A1\", customerId: \"C1\", createdAt: 12345 }\n- ItemAdded { orderId: \"A1\", productId: \"P1\", price: 10, quantity: 2, occurredAt: 12350 }\n- ItemAdded { orderId: \"A1\", productId: \"P2\", price: 5, quantity: 1, occurredAt: 12360 }\n\nDescribe:\n1) A simple in-memory projection update method that keeps track of the total order value (sum of price * quantity) when ItemAdded events are received for an order.\n2) How you would handle the case where ItemAdded for a non-existent order is received (e.g., ensuring idempotency and consistency).\n3) The minimal state you would store per order in the projection to answer: \"What is the current total value for order A1?\", and why.\n\nProvide code-like pseudocode or concise Java-ish snippets to illustrate your solution. Limit to 2-5 minutes of reading time.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core event sourcing skills (event handlers, projections, aggregate state) for the role; minor ding for not explicitly covering full aggregate state (-1). Clarity: Crystal clear with specific events, scoped tasks, time limit, and pseudocode request; no bundling or ambiguity. Discriminative: Requires applied knowledge of projections, idempotency, trade-offs in state minimization; distinguishes understanding from rote recall.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-416",
        "role_title": "Staff Backend Engineer - External APIs",
        "question": "Question:\nYou are designing a new external REST API for partner integrations. You have a Go backend exposing endpoints behind Kong API Gateway, with OpenAPI documentation. Partners must be able to access a limited set of resources, with per-partner rate limits and a simple monetization model (first 1000 requests per month free, then throttle).\n\nDescribe, at a high level, the minimal implementation plan to support:\n- per-partner authentication and identification (without modifying backend business logic)\n- per-partner rate limiting with a 1-second burst cap and a monthly quota\n- clean exposure of an OpenAPI spec through Kong, including versioning and deprecation strategy\n- a lightweight monetization hook that counts usage and enforces the quota (without heavy infra)\n\nIn your answer, outline:\n- which components you would implement or configure (Go middleware, Kong plugins, OpenAPI changes, Kubernetes considerations)\n- the data model (how you identify a partner, store usage, and plan)\n- high-level flows for a single API call from partner to backend\n- how you would test this end-to-end in 2\u20135 minutes of hands-on work (minimal script or command steps)\n\nKeep it concise and concrete.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core role skills (API strategy, monetization, partner enablement, integrations) with exact tech (Go, Kong, OpenAPI); minor ding for no Kubernetes mention in role. Clarity: Crystal clear, well-scoped with explicit requirements and outline; no ambiguity or bundling issues. Discriminative: Tests deep applied knowledge of auth, rate limiting, versioning, monetization trade-offs, plus practical testing; strong separation via implementation choices and flows.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-418",
        "role_title": "Junior Backend Engineer - Service Clients",
        "question": "Question:\n\nYou are implementing a simple gRPC client in Go that calls a remote UserService.GetUser(ctx, req). The service is flaky: sometimes it returns gRPC errors with codes Unavailable or DeadlineExceeded. You want to implement a minimal retry with backoff and a circuit breaker using a counter in memory.\n\n- Constraints:\n  - Implement a function getUserWithRetry(code: string) -> (User, error) that:\n    - Retries up to 3 times on Unavailable or DeadlineExceeded.\n    - Uses exponential backoff starting at 100ms, max 1s.\n    - If there have been 3 consecutive failures (on any of those errors), trip a simple in-memory circuit breaker for 5 seconds, during which all calls fail fast with a specific error ErrCircuitOpen.\n    - After the circuit breaker is open for 5 seconds, the next attempt should be allowed to try once (half-open), and if it succeeds, the circuit closes; if it fails, it reopens.\n  - You can assume the gRPC client call is: resp, err := userClient.GetUser(ctx, &GetUserRequest{Id: reqId})\n  - You must not use external libraries; implement the retry and circuit-breaker logic with standard Go primitives.\n  - The function should be safe for concurrent use (multiple goroutines can call it). Use a simple mutex to guard shared state.\n\n- Provide:\n  - A concise Go code snippet implementing getUserWithRetry as described, including necessary types and state, and comments explaining the behavior.\n\nNote: You do not need to implement the entire gRPC client or server. Focus on the retry/backoff and circuit breaker logic around the given call.",
        "scores": {
          "relevance": 10,
          "clarity": 10,
          "discriminative": 10,
          "reasoning": "Perfect match to role: directly tests entry-level implementation of retries, exponential backoff, and in-memory circuit breaker in Go for gRPC clients. Crystal clear with precise constraints, scope, and concurrency requirements. High discriminative power tests applied knowledge, trade-offs (e.g., half-open state, mutex safety), and clean stdlib implementation\u2014no red flags.",
          "composite": 10.0
        }
      },
      {
        "role_id": "be-420",
        "role_title": "Backend Engineer - Graceful Shutdown",
        "question": "Question: You are running a Go gRPC service behind Kubernetes. When Kubernetes sends a SIGTERM for a pod during a rolling update, implement a minimal graceful shutdown flow that:\n\n- stops accepting new gRPC requests\n- drains in-flight requests within 2 seconds, then shuts down\n- ensures the Pod exits with a non-zero code only if there are undrained requests after the timeout\n\nProvide a concise code snippet (Go) showing:\n- how you listen for OS signals and trigger shutdown\n- how you stop accepting new connections without closing existing ones\n- how you track in-flight RPCs (hint: use a sync.WaitGroup or similar)\n- how you use context with a timeout to enforce the 2-second drain window\n\nAssume a standard gRPC server setup; focus on the shutdown logic and integration points.",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Relevance: Perfectly matches role's core focus on graceful shutdown, connection draining, signals, and zero-downtime deploys in exact context (Go gRPC + K8s). No red flags.\nClarity: Crystal clear, well-scoped to minimal code snippet with explicit requirements and integration points; assumes standard setup without ambiguity.\nDiscriminative: Requires deep applied knowledge of Go concurrency (WaitGroup), gRPC server lifecycle (GracefulStop), signals, contexts/timeouts, and trade-offs in pod exit codes\u2014distinguishes strong engineers exceptionally well.",
          "composite": 9.666666666666666
        }
      },
      {
        "role_id": "be-415",
        "role_title": "Senior Backend Engineer - Multipart APIs",
        "question": "Question:\n\nDesign a minimal Python FastAPI endpoint (just the handler logic, no full app is required) that supports resuming a large file upload using chunked multipart uploads. Assume:\n\n- Each chunk is uploaded via POST /upload/{upload_id}/chunk with JSON body: { \"chunk_index\": int, \"chunk_data\": \"<base64-encoded bytes>\" }.\n- Uploads are tracked in memory with a simple dict: uploads[upload_id] = { \"total_chunks\": int, \"received\": {chunk_index: data_bytes}, \"complete\": bool }.\n- When a chunk is received, store it in the in-memory map, and return a JSON response indicating the current progress: { \"uploaded_chunks\": n, \"total_chunks\": m, \"complete\": bool }.\n- If all chunks have been received, set complete = True and return { \"uploaded_chunks\": m, \"total_chunks\": m, \"complete\": true }.\n- You may assume the client knows total_chunks from a separate initialization step (not part of this question). For simplicity, you can ignore authentication, disk writes, and persistence.\n\nTask:\n- Provide the FastAPI endpoint function (including router decorator) that handles POST /upload/{upload_id}/chunk.\n- Include minimal in-function in-memory state management to track chunk receipt and completion.\n- Include basic input validation (e.g., chunk_index non-negative, chunk_data present).\n- Return the progress as described.\n\nNote: Focus on correctness and clarity of the chunk-resume logic suitable for a senior backend engineer.",
        "scores": {
          "relevance": 9,
          "clarity": 10,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (resumable chunked uploads, multipart handling) with exact FastAPI/Python stack (-0 red flags). Clarity: Crystal clear specs, precise constraints, unambiguous endpoint/task (-0 red flags). Discriminative: Requires applied FastAPI knowledge, state management, validation, resumption logic with trade-offs (e.g., in-memory dict handling), distinguishes seniors effectively (not recall, ~5-8min answer).",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-412",
        "role_title": "Backend Engineer - Analytics APIs",
        "question": "Question:\nYou are designing a simple analytics API endpoint in Go that returns daily user activity counts for the last 7 days. Data is stored in ClickHouse with a table schema:\n- events(event_id UUID, user_id UUID, event_time DateTime, event_type String)\n\nRequirements:\n- The API should return a JSON object: { \"dates\": [\"YYYY-MM-DD\", ...], \"counts\": [int, ...] } for each of the last 7 days in UTC.\n- Use a single ClickHouse query to fetch the counts per day, even if some days have zero events.\n- Handle the case where there are days with no events by returning 0 for those days.\n- The API should be implemented as a function signature in Go (no need for full server setup): fetchLast7DaysActivity(db *sql.DB) ([]string, []int, error)\n\nProvide:\n1) The Go function body implementing the query, data mapping, and handling of zero-event days.\n2) The ClickHouse SQL you would use inside the query (as a string), with appropriate functions to generate the last 7 dates and left-join to ensure zero counts for missing days.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (analytics APIs, ClickHouse aggregation, time-series) with exact tech stack; minor deduction as Go web framework not specified (-1). Clarity: Precise scope, clear requirements, function signature, and deliverables; no bundling or ambiguity. Discriminative: Requires applied knowledge of ClickHouse date generation/left-join for zeros, Go DB integration, and edge cases\u2014strong separation of strong vs weak candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-419",
        "role_title": "Backend Engineer - Retry Logic",
        "question": "Question:\n\nYou are designing a retry helper in Go for distributed operations with the following constraints:\n- Exponential backoff with a maximum cap.\n- Add jitter to avoid thundering herd.\n- Implement a simple retry budget: each operation has a budget that allows up to N retries per window (e.g., per minute). If the budget is exhausted, no further retries should be attempted until the window resets.\n- The question focuses on implementing a single function that determines whether a retry should proceed and computes the next backoff duration.\n\nTask:\nImplement a Go function with the signature:\n\ntype BackoffConfig struct {\n    MinDelay   time.Duration // base delay\n    MaxDelay   time.Duration // saturation cap\n    BackoffFactor float64     // e.g., 2.0\n    Jitter     time.Duration  // max jitter to apply\n    Budget     int              // retries allowed per window\n    Window     time.Duration    // window length for the budget\n}\n\ntype BackoffState struct {\n    Attempts   int\n    StartWindow time.Time\n}\n\nfunc ShouldRetry(cfg BackoffConfig, st *BackoffState, err error) (bool, time.Duration)\n\nBehavior requirements:\n- If err is nil, return (false, 0).\n- Maintain per-operation state (Attempts, window start). If the current time is past StartWindow+Window, reset Attempts to 0 and StartWindow to now.\n- Compute exponential backoff: delay = minDelay * (BackoffFactor)^(Attempts-1). If this exceeds MaxDelay, cap at MaxDelay.\n- Apply jitter: finalDelay = delay + random between [-Jitter, +Jitter].\n- Before returning true, ensure that the budget permits another retry: If Attempts >= Budget, return (false, 0) without updating state. If allowed, increment Attempts and return (true, finalDelay).\n- Ensure the random jitter uses a deterministic source if a seed is provided (you can sketch or describe how to inject a rand.Rand or use crypto/rand for safety).\n\nProvide a concise Go implementation snippet that compiles and demonstrates the core logic. No need for full package or tests, just the function and necessary types.",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Relevance: Perfectly matches role's core requirements (retry infra with backoff, jitter, budgets) using Go; no red flags. Clarity: Crystal clear signature, precise behavior specs, well-scoped for screening; minor jitter randomness detail is unambiguous. Discriminative: Requires deep applied knowledge of retry patterns, state management, concurrency safety, and Go idioms, forcing trade-offs in jitter/crypto impl; excellent separation.",
          "composite": 9.666666666666666
        }
      },
      {
        "role_id": "be-426",
        "role_title": "Junior Backend Engineer - DevOps Scripting",
        "question": "Question:\n\nYou need to create a small script in Python (3.x) that will be used in a CI environment to prepare a deployment workspace. The script should:\n\n- Create a directory named deploy_workspace in the current working directory.\n- Within that directory, create two files:\n  - config.json containing {\"env\":\"production\",\"version\":\"0.1.0\"}\n  - run.sh which prints \"Starting deployment...\" when executed.\n- Ensure the script is safe to run multiple times (idempotent) and will not overwrite existing files without a prompt (you can simulate the prompt by checking if files exist and exiting with a non-zero status if they already exist).\n- Make run.sh executable from the Python script after creation.\n\nProvide the Python script that accomplishes this. It should be concise (2-4 lines of Python code or a small, readable block) and work in a typical Linux environment.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 8,
          "reasoning": "Relevance: Directly tests Python scripting for CI/DevOps automation, idempotency, and Linux env setup\u2014core to role (-1 minor for generic JSON/shell vs specific pipelines). Clarity: Precise scope, constraints, and expectations; well-formed for screening. Discriminative: Tests applied Python (file ops, JSON, chmod, conditionals); distinguishes practical scripting skills from theory.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-424",
        "role_title": "Backend Engineer - Message Serialization",
        "question": "Question:\nYou have an Avro-encoded message schema updated from a current version to a new version. The old schema (v1) is used by producers, and the new schema (v2) is used by consumers. The field \"user_id\" (string, required) exists in v1 but has been renamed to \"user_identifier\" (string, optional) in v2, and a new optional field \"timestamp\" (long) has been added in v2.\n\nAssume the Avro schema is registered in Schema Registry with compatibility set to Backward and you cannot change existing producers/consumers logic.\n\na) Describe how you would configure the Avro schemas and the serialization/deserialization to ensure backward compatibility. Specifically address: schema naming/versioning, field defaults, and how a consumer using v2 reads messages produced with v1.\n\nb) Given an actual v1 message: {\"user_id\":\"alice\"} with only the required field, explain what happens when a v2-aware consumer deserializes it. What will be the value of user_identifier and timestamp in the deserialized object?\n\nc) If you instead configure compatibility to Forward, what changes (if any) would you need to make to the v1 producer or v2 consumer to successfully deserialize messages?",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Relevance: Directly tests core schema evolution, backward/forward compatibility, and Avro specifics from role (-1 minor for assuming Schema Registry not explicitly listed). Clarity: Well-structured with clear parts a/b/c, precise scenario, no ambiguity. Discriminative: Excellent at separating via applied Avro knowledge, trade-offs, and precise prediction of deserialization behavior with defaults/missing fields.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-423",
        "role_title": "Staff Engineer - Incident Response Platform",
        "question": "Question:\nDescribe a minimal Go function design for collecting and tracking an on-call incident in a PostgreSQL-backed SLO-aware system. Specifically:\n\n- Outline a Go function that receives an incident event (e.g., alert triggered, acknowledged, resolved) and updates a PostgreSQL table with at least: incident_id, status, severity, oncall_message, start_time, end_time, and an updated_slo_status.\n- Include what SQL tables and indexes you would create to support fast lookups for active incidents and SLO status, plus a brief note on how you would compute a simple SLO breach (e.g., % incidents resolved within target time).\n- Provide a concise code sketch (only the function body and essential types) showing:\n  - how you would upsert the incident record,\n  - how you would update an on-call message (Slack/PagerDuty) if needed,\n  - how you would update a derived SLO status field.\n\nAssume you have:\n- Go with a PostgreSQL driver (pgx or database/sql),\n- Basic incident events: { incident_id string, event string: \"triggered\" | \"acknowledged\" | \"resolved\", severity string, started_at, resolved_at },\n- A single table incidents(id, status, severity, oncall_message, started_at, resolved_at, slo_status),\n- An external helper to send Slack/PagerDuty updates via a message string.\n\nProvide linkage comments on how this would fit into an event-driven flow and emphasize correctness for staff-level reliability focus.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests Go, PostgreSQL, and SLO/incident tracking core to role; no red flags (-0). Clarity: Well-scoped with specifics, assumptions, and constraints; single focused question (-0). Discriminative: Requires applied design, trade-offs (indexes, upsert, SLO calc), code sketch, and system integration; strong separation for staff-level skills (-0).",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-421",
        "role_title": "Senior Backend Engineer - Security Automation",
        "question": "Question:\nYou have a Python script that reads a list of AWS IAM roles and their attached policies, identifies roles with attached policies granting broad access (e.g., actions: \"*\", or Resource: \"*\"), and prints the role ARN for remediation. The script should also avoid flagging roles that are explicitly exempted via a provided EXEMPT_ROLES list.\n\nGiven:\n- A JSON file roles.json with structure:\n  [\n    {\"role_name\": \"app-readonly\", \"arn\": \"arn:aws:iam::123456789012:role/app-readonly\", \"policies\": [{\"name\": \"ReadOnlyPolicy\", \"statements\": [{\"Effect\": \"Allow\", \"Action\": [\"ecs:*\"], \"Resource\": \"*\"}]}]},\n    ...\n  ]\n- A Python function is_broad_access(stmt) that returns True if a statement grants broad access (Action contains \"*\" or Resource contains \"*\"), and False otherwise.\n- A set EXEMPT_ROLES = {\"arn:aws:iam::123456789012:role/infra-maintenance\"}\n\nWrite the core logic (2-5 minutes) to:\n- Load roles.json\n- For each role, skip if ARN is in EXEMPT_ROLES\n- For each statement in each policy, use is_broad_access to detect broad access\n- If any statement is broad, print the role ARN (one per line)\n\nProvide only the essential Python snippet (no file-walking or error handling boilerplate beyond the minimal necessary), assuming roles.json is in the current directory.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 8,
          "reasoning": "Relevance: Directly tests Python scripting for AWS IAM policy analysis, core to security automation pipelines (-1 for no red flags). Clarity: Precise structure, constraints, and scope (2-5 min, essential snippet only). Discriminative: Tests practical data processing, iteration, conditionals, and JSON handling with exemptions; distinguishes applied Python skills (-2 for not probing edge cases or is_broad_access nuances).",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-425",
        "role_title": "Senior Backend Engineer - Graph Analytics",
        "question": "Question:\nYou have a social network graph stored in Neo4j where nodes are users and edges are \u201cFOLLOWS\u201d relationships with a property weight representing interaction strength (0.0 to 1.0). You want to build a lightweight influence score for a given user U by considering the top-3 most influential followers (neighbors who follow U directly) where influence is defined as the neighbor\u2019s own weighted out-going engagement score. The engagement score for a user N is the average of weights on all edges from N to their followers (i.e., N\u2019s followers are the incoming neighbors to N if you think in graph terms, but you\u2019ll compute from N to their followees). In Cypher, outline (pseudo-code is fine) how you would:\n\n- Retrieve the top-3 followers of U by the follower\u2019s own engagement score (computed as average weight of edges from that follower to their own followees).\n- Return U\u2019s influence score as the sum (or average) of those top-3 followers\u2019 engagement scores, limited to 3.\n\nAssume:\n- Graph uses relationships :FOLLOWS with property weight on both follower->followee edges.\n- U is a parameter userId.\n- You can use APOC or user-defined procedures if needed, but keep it concise. Provide the Cypher query and brief explanation of each major step.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests Neo4j/Cypher for graph traversal and analytics at scale, core to role's graph pipelines and billion-edge optimization (-1 for not explicitly mentioning scale handling). Clarity: Well-scoped with precise definitions, assumptions, and output requirements. Discriminative: Requires multi-hop traversal, aggregation, ranking, and Cypher fluency to distinguish strong graph DB experts from generalists.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-422",
        "role_title": "Junior Backend Engineer - API Testing",
        "question": "Question:\nYou have a FastAPI endpoint that creates a new user and returns the created user object with status 201. Write a pytest test function that:\n- sends a POST request to /users with JSON body {\"name\": \"Alice\", \"email\": \"alice@example.com\"}\n- asserts the response status is 201 and the response contains an id, name, and email matching the request\n- uses a TestClient from fastapi.testclient\n- uses a simple in-memory fixture for the app (no real DB)\n\nAssume the FastAPI app instance is named app and defined as:\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\n\napp = FastAPI()\n\nclass User(BaseModel):\n    id: int\n    name: str\n    email: str\n\n@app.post(\"/users\", response_model=User, status_code=201)\ndef create_user(user: User):\n    # pretend to save and assign an id\n    return User(id=1, name=user.name, email=user.email)\n\nProvide the complete test function code snippet including any necessary imports and fixture.",
        "scores": {
          "relevance": 9,
          "clarity": 10,
          "discriminative": 8,
          "reasoning": "Relevance: Directly tests FastAPI, pytest, and TestClient for API testing role (-1 for not covering fixtures/advanced patterns explicitly). Clarity: Crystal clear with exact requirements, code context, and constraints. Discriminative: Tests practical applied knowledge of testing FastAPI endpoints, distinguishing those who can write functional tests from those who can't; not trivial but not deeply trade-off heavy.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-428",
        "role_title": "Senior Backend Engineer - Payment Orchestration",
        "question": "Question:\nDescribe and sketch a simple Go function (or small module) for routing a payment request to multiple providers with failover. The function should:\n\n- Accept a payment request (amount, currency, customerId) and a list of provider endpoints with reliability weights.\n- Attempt to process the payment through the highest-weight provider first, and on failure, automatically retry with the next provider in the prioritized list, up to all providers.\n- Include how you would record a lightweight success/failure metric (e.g., using Redis or in-memory map) to influence future routing decisions.\n- Be mindful of idempotency: how would you avoid duplicate charges if a retry occurs?\n- Provide pseudo-Go code (or real Go code snippets) and brief explanation of the core logic and edge cases.\n\nWhat would your routing function look like, and what are the key failure/edge cases you would handle?",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core payment routing, failover, metrics, and idempotency from role (-1 minor for not explicitly mentioning cost optimization). Clarity: Well-scoped with specific inputs/requirements, clear pseudo-code expectation, no bundling. Discriminative: Requires applied Go knowledge, trade-offs (weights, metrics, idempotency), edge cases; strong separation of strong vs weak candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-427",
        "role_title": "Backend Engineer - Feature Store",
        "question": "Question:\n\nYou are building a simple feature store for real-time ML inference. You have a feature \u201cuser_age_bucket\u201d that should be computed from a user table with fields: user_id (string), birth_year (int), and a streaming events table with events (user_id, event_time). The feature should be a bucketed age group (e.g., 0-17, 18-24, 25-34, \u2026) based on the current date, and it must be available for online serving within a small freshness window (e.g., last 24 hours). The offline training will use a batch dataset with the latest computed bucket per user as of train_time.\n\nTask:\n- Describe a compact approach (data model + computation flow) to compute and serve this feature using Python and typical components (e.g., Feast-like feature registry, Redis for online store, PostgreSQL for offline store, and Kafka for streaming).\n- Include:\n  - How you would define the feature in the feature registry (name, type, dependencies).\n  - How you would compute and update the online feature value when new user_birth_year or user events arrive.\n  - How you would ensure the online feature is consistent with offline training (training with a fixed timestamp notion).\n- Provide a minimal code snippet (pseudo-Python is fine) showing:\n  - a small feature definition, and\n  - a simple online-to-offline sync step that would update the online store.\n\nAssumptions:\n- You may use Python-like pseudocode and simple function signatures.\n- Do not rely on external libraries beyond standard data structures; focus on the logic.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core feature store skills (computation, versioning, real-time serving) matching role; minor ding for assuming specific tools (Feast/Redis) not in desc (-1). Clarity: Crystal clear with precise scope, constraints, and deliverables; assumptions resolve ambiguities. Discriminative: Requires applied ML infra knowledge, trade-offs in sync/consistency, and code logic; strong separation of capable candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-429",
        "role_title": "Backend Engineer - Log Aggregation",
        "question": "Question:\nYou are building a log ingestion path in Go that reads raw log lines from Kafka, parses them into a structured event, and writes to ClickHouse for long-term storage. The pipeline must handle at least 1e6 events/second with minimal tail latency and support simple enrichment (e.g., adding hostname and a processed_at timestamp). Describe a concrete, minimal design (high-level architecture and key data structures) and provide a short Go snippet that:\n\n- Consumes messages from a Kafka topic (use a mock interface if you prefer)\n- Parses a log line in the following format: [LEVEL] 2024-07-01T12:34:56Z hostname: message content\n- Enriches with hostname and processed_at\n- Writes a structured row to ClickHouse using a batch write approach\n\nWhat considerations would you make to ensure low latency and high throughput (e.g., batching, concurrency, backpressure), and how would you handle potential parsing errors without dropping valid events? Provide the core code snippet (parsing + enrichment + batch assembly) and a brief note on how you\u2019d integrate with ClickHouse in production.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core log pipeline skills (Go, high-throughput processing, parsing/enrichment) matching role's millions/sec focus; minor deduction as ClickHouse/Kafka specifics unconfirmed but highly relevant (-1). Clarity: Crystal clear scope, format, constraints, and expectations; well-scoped for screening. Discriminative: Requires applied design (architecture, trade-offs like batching/backpressure), concrete code, error handling; strong separation of deep vs shallow candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-430",
        "role_title": "Junior Backend Engineer - Search Indexing",
        "question": "Question:\n\nYou have a PostgreSQL table \"articles\" with columns id (int), title (text), body (text), and published_at (timestamp). You want to build a simple Python job that indexes each article into Elasticsearch under an index named \"articles\". The index should map the fields as:\n- title: text with.Keyword subfield for exact matches\n- body: text\n- published_at: date\n\nTask:\n1) Write a concise Python snippet (using any Elasticsearch Python client) that:\n   - Connects to Elasticsearch\n   - Creates the index \"articles\" with the above mapping (ignore if it already exists)\n   - Iterates through a small batch (e.g., 100 at a time) of rows from PostgreSQL (you can mock the data with a list of dicts if you prefer) and bulk indexes them into Elasticsearch\n2) Show how you would structure the bulk request payload for Elasticsearch\n3) Briefly explain how you would adjust the mapping if you wanted to support a simple \"search in title\" use case with a fast exact match option.\n\nTimeboxed: answer in ~2\u20135 minutes. Focus on practical correctness and clear code structure.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core search indexing skills (Elasticsearch mapping, bulk indexing) matching role focus (-1 for minor PostgreSQL assumption not in role). Clarity: Precise scope, clear tasks, timebox, mock data option. Discriminative: Requires applied knowledge of mappings, bulk API structure, trade-offs (text+keyword), distinguishes practical skills from trivia.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-434",
        "role_title": "Principal Engineer - Database Strategy",
        "question": "Question: In a multi-region deployment using PostgreSQL and Redis, you must ensure low-latency reads, strong consistency for critical writes, and cost-effective failover. Describe a concrete, step-by-step plan for: \n\n- data placement and replication strategy across regions (including any role of PostgreSQL vs Redis, and how you handle consistency guarantees). \n- a failover scenario during regional outage (what components failover to which regions, how do you minimize data loss, and how do you verify correctness after failover). \n- metrics and monitoring you would track to maintain operational excellence, and a brief outline of a dark/chaos test you would run to validate the plan. \n\nKeep your answer high-level but with specific configuration choices or trade-offs you would consider (e.g., synchronous vs asynchronous replication, read replicas, Redis caching patterns, Kafka as a change data capture pipe).",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Relevance: Directly tests core multi-region database strategy, replication, failover, and ops excellence; matches PostgreSQL/Redis mention with perfect role alignment (-0 red flags). Clarity: Precise scope, well-structured subpoints, clear high-level depth with examples; no ambiguity or bundling issues. Discriminative: Requires deep applied knowledge of trade-offs (sync/async, caching, CDC), planning, and chaos testing; excellent separation of strong vs weak candidates.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-435",
        "role_title": "Junior Backend Engineer - Queue Consumers",
        "question": "Question:\nYou are implementing a simple RabbitMQ consumer in Python that processes messages from a queue named \"orders\". Each message contains an order_id. The processing is idempotent and can be retried safely, but you must ensure a message is not reprocessed if it has already been completed successfully.\n\nIn pseudo-code or Python-like code, sketch a minimal consumer loop that:\n- Acks a message only after successful processing\n- Uses a PostgreSQL table \"processed_orders\" to track completed order_ids\n- Handles at-least-once delivery by detecting duplicates and rejecting/acking appropriately\n- Includes a basic error path that requeues on transient failure and routes permanently failed messages to a dead-letter queue (dlx) named \"orders.dlq\"\n\nProvide:\n- The core logic (snippets) for consuming, checking the database for duplicates, committing the transaction, and acknowledging/nacking messages\n- Brief explanation of how idempotency is achieved in this setup\n\nNote: You don\u2019t need to provide full runnable code or boilerplate, just the essential control flow and database interactions.",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role (RabbitMQ consumers, ack patterns, dead letter handling, idempotency, PostgreSQL); no red flags. Clarity: Crystal clear scope, explicit requirements, well-constrained for screening; minor -1 for lacking exact ack/nack API details but note mitigates. Discriminative: Tests applied knowledge of core patterns (idempotency via DB, at-least-once, DLQ), trade-offs in ack/requeue/nack; strong separation without being overly broad.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-432",
        "role_title": "Backend Engineer - Data Validation",
        "question": "Question:\n\nYou have a Python function that processes a data batch and writes results to a PostgreSQL table. After processing, you want to quickly validate two things using Great Expectations (GE) before the batch is considered successful:\n\n1) The output table should have no NULLs in the columns: user_id (int, non-null), event_type (str, non-null), and event_timestamp (datetime, non-null).\n2) The mean value of a numeric column revenue should be within [0, 1000] for the batch.\n\nWrite a small GE validation script snippet (using a Batch and a simple Expectation Suite or inline expectations) that:\n- Creates a batch of the target PostgreSQL table\n- Applies the two checks above\n- Reads the results and returns a boolean indicating whether the batch passed validation\n\nAssume:\n- SQLAlchemy/PostgreSQL connection is available as an engine object named engine\n- The target table is named public.events_YYYYMMDD\n- You can instantiate a GE context in memory (no need to persist a suite)\n\nProvide only the code snippet and brief explanation (1\u20132 sentences).",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Directly tests core data validation skills (schema/NULL checks, statistical ranges) using Great Expectations on PostgreSQL, matching role's pipeline quality focus; crystal clear with precise scope, constraints, and assumptions; requires applied GE knowledge and trade-offs (inline vs suite), distinguishing strong candidates without being trivial or overly broad.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-431",
        "role_title": "Staff Engineer - Real-time Collaboration",
        "question": "Question:\nYou\u2019re designing a real-time text editor using CRDTs and operational transforms. Describe a concise approach to handling concurrent edits from two users that arrive almost simultaneously over WebSocket connections. Specifically:\n- How would you model the incoming operations (CRDT vs OT) and their ordering?\n- What is your strategy to resolve conflicts and maintain a consistent document state across clients and the server?\n- Outline minimal data structures (types or interfaces) you would define to represent an operation and an edit batch, and how you would apply them on both server and client.\n- Briefly explain how presence information and vector clocks (or causal metadata) integrate with your approach to ensure correct synchronization.",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Relevance: Perfectly matches role's core focus on real-time collaboration, conflict resolution, presence, and operational transforms; directly tests CRDT/OT which are central. Clarity: Crystal clear, well-scoped subpoints with specific asks, no ambiguity or bundling issues. Discriminative: Excellent at separating experts (requiring deep applied knowledge of trade-offs, data modeling, integration) from superficial knowers; tests synthesis over recall.",
          "composite": 9.666666666666666
        }
      },
      {
        "role_id": "be-433",
        "role_title": "Senior Backend Engineer - Platform APIs",
        "question": "Question:\nYou are designing a simple internal platform API to provision \"compute\" resources for developers. The API should be a gRPC service with two methods:\n\n- CreateResource(CreateResourceRequest) -> CreateResourceResponse\n- GetResource(GetResourceRequest) -> GetResourceResponse\n\nRequirements:\n- Each resource has: id (string), owner (string), type (\"compute\"), and status (\"provisioning\" | \"ready\" | \"error\"). The resource state should be stored in PostgreSQL.\n- Provisioning is simulated: when CreateResource is called, the resource is inserted with status \"provisioning\", and after a short async delay (you can describe the approach, not implement), it becomes \"ready\" or \"error\" based on an arbitrary condition you define (e.g., if owner contains \"fail\", it becomes \"error\"; otherwise \"ready\").\n- Use Redis to debounce duplicate CreateResource requests by id within a 1-minute window (idempotency key).\n- Provide a concise Go interface (protobuf-like) for the service and the messages, and outline the core logic for handling idempotency, persistence, and the provisioning state transition without full boilerplate.\n\nTask:\n- Sketch the proto definitions (messages and service) and provide a brief high-level Go pseudo-implementation outline for CreateResource, focusing on:\n  - idempotent handling with Redis\n  - persisting initial state to PostgreSQL\n  - queuing/triggering the asynchronous state transition\n  - id, owner, and status fields management\n\nYou can use pseudo-code or simplified snippets. Explain your approach in 2\u20134 short bullets.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core API design, gRPC, PostgreSQL, Redis, async workflows, and Go for internal platform provisioning (-1 for not explicitly listing gRPC/Redis in role but perfect conceptual match). Clarity: Crystal clear scope, precise requirements, focused task with pseudo-code allowance. Discriminative: Requires applied knowledge of idempotency, async patterns, DB integration, and trade-offs in real-time systems, strongly separates strong candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-436",
        "role_title": "Backend Engineer - API Documentation",
        "question": "Question: You\u2019re building an API documentation tool that auto-generates OpenAPI specs for a Node.js/TypeScript backend. You have an endpoint:\n\nPOST /users\nRequest body:\n{\n  \"name\": string (required),\n  \"email\": string (required, unique),\n  \"age\": number (optional, minimum 0)\n}\nResponse:\n201 Created with body:\n{ \"id\": string, \"name\": string, \"email\": string, \"age\": number | null }\n\na) Sketch how you would implement a small OpenAPI 3.0 spec snippet for this endpoint, including request body schema, response schema, and a 400 error if email is invalid. Show at least one example value.\n\nb) Describe how you would ensure this spec stays in sync with a TypeScript type used in your Express route handler, and provide a minimal code snippet illustrating the mapping between the TS type and the OpenAPI schema (no external tools).",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests OpenAPI spec generation, TypeScript/Node.js integration, and sync mechanisms\u2014core to API doc tooling role (-1 for not explicitly mentioning versioning/SDK but highly on-target). Clarity: Precise endpoint details, scoped parts a/b, clear expectations without ambiguity. Discriminative: Requires schema authoring, error handling, TS-OpenAPI mapping with code\u2014tests practical applied knowledge and trade-offs in type safety/sync, strongly separates skilled candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-439",
        "role_title": "Backend Engineer - Billing Integration",
        "question": "Question:\nYou are implementing a monthly subscription with Stripe. A customer downgrades from Pro to Basic mid-cycle, and prorations should apply to the current period. Describe (and sketch code) how you would:\n\n- compute the prorated credit for the remaining days of the current period\n- apply that credit to the next invoice rather than issuing a separate credit note\n- ensure idempotency and avoid duplicate proration when a retry occurs\n\nAssume:\n- Python backend, Stripe API, PostgreSQL for state\n- You have the Stripe customer_id, current_subscription_id, current_period_end, and plan prices\n- The system uses Celery for async tasks and Redis for idempotency keys\n\nWhat would your approach be, and provide a compact code snippet illustrating the core Stripe calls and idempotency check.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core role skills (proration, subscription changes) with exact tech stack (Stripe, Python); minor ding for not mentioning PostgreSQL usage explicitly (-1). Clarity: Precise scope, clear asks, well-defined assumptions, compact code request. Discriminative: Tests deep applied knowledge of Stripe proration/idempotency/tradeoffs, separates strong engineers; not trivial recall.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-437",
        "role_title": "Senior Backend Engineer - Secrets Management",
        "question": "Question:\nYou are designing a Go service that rotates database credentials stored in HashiCorp Vault every 24 hours. The credentials are used by a PostgreSQL client in your app, and you must ensure the app always uses the latest credentials without restarting the process.\n\n- Outline a minimal design with:\n  - How you would fetch and refresh credentials from Vault in a running Go application.\n  - How you would propagate rotated credentials to the PostgreSQL client without downtime.\n  - A brief note on handling encryption-at-rest and audit considerations in Vault for these credentials.\n\n- Provide a concise pseudocode snippet (or real Go-ish code) showing:\n  - Initialization (initial fetch and cache setup).\n  - A background refresh loop that updates the in-memory credentials.\n  - A safe way to replace the credentials for new DB connections while existing connections are closed gracefully or reused.\n\nAssume: Vault is using AppRole authentication, credentials have a short TTL, and you use a standard Go PostgreSQL driver. The answer should reflect practical implementation details and edge cases.",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core focus on secrets rotation, Vault, encryption-at-rest, and audit logging in Go; no red flags. Clarity: Crystal clear with precise scope, constraints, and assumptions; minor deduction only for not specifying exact response length. Discriminative: Tests deep applied knowledge of Vault integration, Go concurrency, zero-downtime credential refresh, and edge cases like TTL/TTL expiry, strongly separating strong candidates.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-440",
        "role_title": "Junior Backend Engineer - Metrics Collection",
        "question": "Question:\nYou have a small Go HTTP service that processes user requests. Add minimal code snippets to demonstrate proper metrics instrumentation using Prometheus client_golang. Specifically:\n\n1) Expose a /metrics endpoint with a Counter named requests_total that increments on every request.\n2) Expose a Gauge named in_progress_requests that tracks the number of in-flight requests (increment on request start, decrement on done), and ensure it does not go negative.\n3) Expose a Histogram named request_duration_seconds to observe the time taken to handle each request (seconds).\n\nProvide:\n- The Go code snippets showing how you would declare and update these metrics and hook them into a simple http.Handler that sleeps for 10-50ms to simulate work.\n- How you would start the HTTP server and expose /metrics.\n\nKeep the code concise and runnable, focusing on the metric usage.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core metrics instrumentation (counter/gauge/histogram) with Prometheus client_golang in Go for entry-level role; matches tech stack perfectly (-0 red flags). Clarity: Precise scope, numbered tasks, clear deliverables, well-constrained for screening. Discriminative: Requires applied knowledge of metric patterns, concurrency safety (gauge), timing (histogram), and integration; distinguishes candidates understanding trade-offs from rote memorizers.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-438",
        "role_title": "Staff Engineer - ML Serving Infrastructure",
        "question": "Question:\nYou are deploying a real-time inference service using NVIDIA Triton Inference Server behind a Kubernetes ingress. The model requires sub-50ms latency for 95th percentile at a 100 qps peak load. You have two optimization knobs: (A) dynamic batching at the Triton level, and (B) TensorRT optimization for the model. Describe a concise, practical plan to achieve the target latency, including:\n- How you would configure Triton dynamic batching (batching window, max batch size, and any constraints) and how to determine their values.\n- How you would enable and verify TensorRT optimization for the model within Triton.\n- What metrics and instrumentation you would monitor (including Prometheus metrics) to validate latency targets during rollout.\n- A minimal rollback or safe-guard strategy in case latency regresses.",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Relevance: Directly tests core ML serving skills (Triton, batching, TensorRT, latency optimization, Kubernetes) from role description, no red flags. Clarity: Crystal clear, well-scoped with specific constraints, minor deduction for lacking exact time limit. Discriminative: Requires deep applied knowledge, trade-offs, and practical ML infra experience to distinguish experts.",
          "composite": 9.666666666666666
        }
      },
      {
        "role_id": "be-442",
        "role_title": "Backend Engineer - Event Streaming",
        "question": "Question:\nYou are building a real-time event streaming service in Rust using NATS for the transport and Redis for ephemeral state. You need a simple fan-out mechanism with per-subscription filtering and a short replay capability.\n\nTask:\n- Outline a minimal Rust function (or code snippet) that subscribes a consumer to a NATS subject, maintains a per-consumer filter predicate (e.g., only events where event.type == \"order\" and event.country == \"US\"), and delivers only matching events to the consumer.\n- Include how you would implement:\n  - A lightweight in-memory filter state per subscriber (without using Redis for filtering logic).\n  - A basic replay capability that, upon subscription, replays the last N events stored in Redis (assume events are serialized as JSON and stored with keys like \"event:{id}\").\n- Briefly explain how you would wire this with NATS subscriptions and what data structures you would use to ensure O(1) filtering after receiving an event.\n\nNotes:\n- You do not need a full runnable program; provide concise code sketches and rationale.\n- Focus on practical design decisions and potential performance considerations for mid-level backend engineers.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (event streaming, fan-out, filtering, replay) with exact tech stack (Rust, NATS, Redis); no red flags (-0). Clarity: Crystal clear task, well-scoped with constraints, no ambiguity or bundling (-0). Discriminative: Requires applied Rust/NATS/Redis knowledge, data structure design for O(1) filtering, trade-offs, and performance reasoning; strong separation for mid-level engineers (-0).",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-444",
        "role_title": "Backend Engineer - Service Templates",
        "question": "Question:\n\nYou are given a minimal Go service template that uses a standard HTTP server and a Dockerfile. Describe and provide a short code snippet to add the following in 2 minutes or less:\n\n- A health check endpoint at /health that returns 200 OK with a JSON body { \"status\": \"ok\" }.\n- A basic request metric: increment a Prometheus counter named \"requests_total\" for every incoming HTTP request.\n- A simple cookiecutter-style template variable we could replace to set the service name (e.g., {{cookiecutter.service_name}}) and show how you would expose it in the Go code (without modifying build config). \n\nBonus (optional): outline what minimal Kubernetes manifest would be updated to deploy this new service using ArgoCD, focusing only on the deployment name and container image tag referencing a placeholder that would be substituted by the CI/CD process.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (service templating, observability via Prometheus, best practices for health checks) using Go/Docker; minor ding for cookiecutter (not mentioned) but fits templating (-1). Clarity: Well-scoped with time limit, specific requirements, unambiguous. Discriminative: Requires applied knowledge of Go HTTP/Prometheus integration, templating, and K8s/ArgoCD basics; tests practical skills with trade-offs in minimalism, strong separation.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-443",
        "role_title": "Senior Backend Engineer - Data Lake Architecture",
        "question": "Question:\nYou have an Iceberg table in a Spark job stored on S3. The table was created with a nested schema that includes a struct field user.info with a timestamp field last_login. A new requirement is to evolve the schema to add a new field user.info.nickname (string) and to enable time travel to query the state of the table as of yesterday.\n\n- Describe the minimal code changes you would make in Spark to:\n  a) perform a safe schema evolution to add user.info.nickname without breaking existing reads, and\n  b) run a time-travel query to retrieve data as of yesterday.\n\n- Include:\n  - the key Iceberg/Spark options you would rely on,\n  - how you would reference the historical snapshot in Spark SQL or DataFrame API,\n  - any considerations for nested fields (structs) during evolution,\n  - and how this interacts with partition pruning or performance.\n\nNote: No more than a few short code snippets, and focus on the practical steps and options rather than theory.",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core skills (Iceberg schema evolution, time travel, partitioning) with exact tech stack (Spark, S3, Iceberg nested schemas). No red flags. Clarity: Crystal clear, well-scoped with specific subparts, constraints (short snippets, practical focus), and context; minor deduction for bundled a/b but tightly related. Discriminative: Tests deep applied knowledge of Iceberg APIs, options, nested evolution trade-offs, and performance interactions, strongly separating experienced candidates.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-446",
        "role_title": "Staff Engineer - Distributed Tracing",
        "question": "Question:\nYou are designing a sampling strategy for a high-traffic distributed service stack using OpenTelemetry, Jaeger, and Kafka. The system emits traces with a 128-bit Trace ID. You want to implement a probabilistic sampling policy that:\n\n- Keeps head-of-line latency low for user requests.\n- Ensures stable traces for a given service path (i.e., the same trace should be either sampled consistently or not sampled across its span).\n- Allows easy reconfiguration without redeploying services.\n\nDescribe (briefly) how you would implement this in Go, focusing on:\n- How you would compute a per-request sample decision without expensive hashing of the entire trace.\n- How you would ensure consistency of sampling for a trace across different services/processes (trace-id based sampling).\n- How you would propagate the sampling decision through the pipeline (from client to collector via Kafka) and how you would expose the configurability (sampling rate) for runtime changes.\n\nProvide a small code snippet (pseudocode or Go-like) illustrating the per-request sampling decision and the propagation of the sampling flag.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Relevance: Directly tests core distributed tracing/sampling skills with exact tech stack (OpenTelemetry, Jaeger, Kafka); minor deduction as role emphasizes SLO correlation less (-1). Clarity: Crystal clear, well-scoped with specific focuses and code request; no bundling or ambiguity. Discriminative: Excellent at separating strong candidates via trade-offs (low latency, consistency, propagation), applied Go knowledge, and runtime config\u2014deep, role-specific, not recall/trivia.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-448",
        "role_title": "Senior Backend Engineer - Webhook Infrastructure",
        "question": "Question:\n\nYou are designing a webhook delivery worker in Go with guaranteed delivery. Describe a simple, end-to-end approach (no code required) to ensure that a webhook payload is delivered at least once, including how you would:\n\n- store the retry state and avoid duplicate deliveries on retry,\n- handle exponential backoff with jitter,\n- verify the request signature on the receiver side and enforce delivery guarantees when a signature check fails,\n- surface basic delivery analytics (success rate, retry count, oldest pending webhook) using PostgreSQL and Redis.\n\nBe concise but concrete enough to implement in a real system.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (webhook delivery, retries, signatures, analytics) with exact tech (Go, PostgreSQL, Redis); minor deduction for not being Go-specific in depth (-1). Clarity: Well-scoped, unambiguous, clear sub-bullets with 'simple end-to-end' and 'concise but concrete' constraints. Discriminative: Requires applied knowledge of trade-offs (at-least-once, idempotency, backoff, DB/Redis partitioning), distinguishes strong architects from weak ones.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-450",
        "role_title": "Principal Engineer - ML Platform Strategy",
        "question": "Question:\nYou are tasked with deploying a reusable, multi-tenant ML model serving platform on Kubernetes that integrates Kubeflow for experimentation, MLflow for model governance, and Ray for distributed training/inference. In 2\u20135 minutes, outline a high-level design (no code) that addresses:\n- How you would isolate workloads and data for multiple teams/models securely on a shared cluster.\n- How you would standardize model lifecycle from experimentation to production with governance and lineage.\n- How you would enable cross-team collaboration while preventing resource contention and ensuring scalability.\n\nInclude: key components/services you'd deploy, main data/model lineage artifacts, and a simple 2\u20133 sentence rationale for your choices.",
        "scores": {
          "relevance": 7,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Tests core ML platform strategy, governance, and tooling (Kubernetes, Kubeflow/MLflow/Ray align with ML platform needs), but deduct -2 for focusing only on these specific tools (potentially not main role focus) and -1 for generic design without role's org-wide adoption emphasis. Clarity: Crystal clear, well-scoped with time limit, specific asks, and format; no bundling or ambiguity. Discriminative: Excellent at testing applied trade-offs in multi-tenancy, lifecycle, and scalability, distinguishing strategic thinkers from rote learners.",
          "composite": 8.333333333333334
        }
      },
      {
        "role_id": "be-445",
        "role_title": "Junior Backend Engineer - Database Migrations",
        "question": "Question: You have a PostgreSQL table users(id UUID PK, email TEXT UNIQUE, created_at TIMESTAMP). You need to add a new non-nullable column last_login_at TIMESTAMP with default to current_timestamp, but existing rows must be valid. Outline a safe Alembic migration that:\n\n- Adds the column without locking the table for long (consider PostgreSQL behavior).\n- Fills the column for existing rows with a sensible value (e.g., created_at) without downtime.\n- Sets a server-side default for future inserts.\n- Includes a rollback plan.\n\nProvide the SQL statements or Alembic operations you would include in the upgrade() and downgrade() functions, plus any considerations you would document in comments. Assume SQLAlchemy ORM models may not be fully in sync during migration.",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Relevance: Perfectly matches role (database migrations, zero-downtime, rollback) with PostgreSQL/Alembic exactly as described. Clarity: Crystal clear with precise scope, constraints, and expectations; minor deduction for no explicit time limit. Discriminative: Tests deep applied knowledge of zero-downtime patterns, PG behaviors, backfill strategies, and trade-offs, strongly separating capable candidates.",
          "composite": 9.666666666666666
        }
      },
      {
        "role_id": "be-447",
        "role_title": "Backend Engineer - Load Shedding",
        "question": "Question:\nDesign a simple backpressure-driven request limiter in Go for a Redis-backed queue used by a Go service. Requirements:\n- Each incoming HTTP request should either be accepted to enqueue a task or rejected with a 503 if the system is overloaded.\n- The limiter should use a fixed-size in-memory semaphore (counting semaphore) to cap concurrent processing, and use Redis to coordinate a priority queue where higher-priority tasks are enqueued before lower-priority ones.\n- Explain how you would:\n  -- check current load and decide accept/reject without blocking callers\n  -- implement graceful degradation so that when the queue fills, lower-priority tasks are dropped before higher-priority ones\n  -- expose a simple API to signal backpressure (e.g., a Redis pub/sub or metric-based cue) to upstream services\n\nProvide concise code sketches (Go snippets) for:\n- a request handler that decides to enqueue or reject\n- a basic in-memory semaphore for concurrency\n- a function stub showing Redis LPUSH/LZ ADD (with priorities) and how you\u2019d pull the next task for workers\n\nAssume Go, Redis client, and a HTTP server context. No need for production-ready error handling, just demonstrate the core logic.",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core focus on backpressure, priority queues, and graceful degradation using exact tech (Go, Redis). No red flags. Clarity: Crystal clear with specific requirements, scoped code sketches, and assumptions; minor deduction for no explicit time constraint. Discriminative: Tests deep applied knowledge of semaphores, Redis priority queues (LPUSH/LZADD implies sorted sets), backpressure signals, and trade-offs in load shedding\u2014distinguishes strong systems thinkers.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-449",
        "role_title": "Junior Backend Engineer - Auth Integration",
        "question": "Question:\nYou have a Node.js TypeScript backend that accepts an OAuth2 authorization code from a frontend login flow. Outline and briefly implement (pseudo-code or real code) the minimal steps to exchange the authorization code for tokens with an OAuth provider, validate the ID token if present, and securely store the access token in PostgreSQL. Include how you would handle token refresh logic and error cases in a way suitable for a junior engineer.\n\nDeliverables:\n- A concise code snippet (TypeScript) showing:\n  - Exchange of authorization code for tokens via a POST request to the provider\u2019s token endpoint.\n  - Optional validation of an ID token (e.g., JWT verification) if provided.\n  - Storing the access token (and refresh token if returned) in PostgreSQL with a simple schema.\n  - A basic token refresh flow (using the refresh token to obtain new tokens) and how you would update the DB.\n  - Minimal error handling and logging.\n\nNotes:\n- Do not rely on provider-specific libraries; show generic HTTP calls and JSON handling.\n- Emphasize security aspects: use HTTPS, proper error handling, not logging sensitive tokens, and safe storage.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 8,
          "reasoning": "Relevance: Directly tests OAuth code exchange, token handling, refresh flows, and DB storage\u2014core to role; minor deduction for assuming Node.js/TS/PostgreSQL not explicitly listed (-1). Clarity: Crystal clear with precise deliverables, constraints, and security emphasis; well-scoped for screening. Discriminative: Tests applied knowledge of OAuth flow, security, and implementation with trade-offs (error handling, refresh logic), distinguishing juniors who understand vs. those who don't; slightly broad but constrained by 'minimal/concise'.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-452",
        "role_title": "Senior Backend Engineer - Faceted Search",
        "question": "Question:\n\nYou are implementing a faceted search feature on a product catalog using Elasticsearch. You have two requirements:\n\n1) A user can filter by category, brand, price range, and a boolean \u201cin_stock\u201d flag.\n2) The UI shows facet counts that reflect the number of results after applying all active filters (including nested category paths) and must be accurate at scale.\n\nProvide a succinct, concrete approach (no code necessary) that answers:\n- How you would structure the Elasticsearch query (including aggregations and filters) to ensure correct facet counts under multiple concurrent filters.\n- How you would handle performance considerations for large catalogs (e.g., optimizing aggregations, using composable aggregations, or shard-level strategies).\n- Any trade-offs or caveats you would consider (e.g., count accuracy vs. performance, nested vs. parent aggregations, or reverse-lookup of category paths).\n\nBriefly describe what the resulting query would do and why this approach is sound for a senior backend engineer.",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Relevance: Perfectly matches role's faceted search, dynamic filtering, aggregation optimization, and scale requirements; no red flags. Clarity: Crystal clear with specific requirements, well-scoped prompts, minor deduction for lacking explicit time constraint. Discriminative: Tests deep applied knowledge of ES query structure, performance trade-offs, and scale strategies, strongly separating senior engineers.",
          "composite": 9.666666666666666
        }
      },
      {
        "role_id": "be-454",
        "role_title": "Junior Backend Engineer - ETL Scripts",
        "question": "Question:\nYou have a CSV file \"sales.csv\" with columns: order_id (int), customer_id (int), order_date (YYYY-MM-DD), amount (float), category (text). Write a short Python function using pandas that:\n\n- Reads the CSV from a given file path.\n- Filters rows where amount > 100.\n- Groups by category and computes total revenue and the number of orders.\n- Returns a DataFrame with columns: category, total_revenue, order_count, sorted by total_revenue descending.\n\nAlso include a minimal example call to demonstrate it works (you can use a small in-memory CSV via StringIO). No need to handle missing values or errors beyond basic.",
        "scores": {
          "relevance": 9,
          "clarity": 10,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests pandas ETL skills core to role (-1 for minor generic programming elements). Clarity: Crystal clear with precise requirements, constraints, and example. Discriminative: Tests applied pandas knowledge (filter, groupby, agg, sort) distinguishing capable entry-level candidates from weak ones via practical implementation.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-451",
        "role_title": "Backend Engineer - Subscription Billing",
        "question": "Question:\nYou implement a simple monthly subscription with usage-based pricing. Each user has a plan with a monthly base price and a per-unit rate for overages. At the end of the billing period, you must compute the invoice amount as:\n- base_price + max(0, usage_units - included_units) * per_unit_rate\nAdditionally, support plan changes mid-cycle with proration:\n- If a user upgrades on day 15 in a 30-day month, the new plan\u2019s base price starts immediately and is prorated for the remaining days of the cycle; while the old plan is prorated accordingly for the time used. The total charge for the cycle should reflect: (old plan days remaining) and (new plan days used).\n\nGiven:\n- Month length = 30 days\n- User A on Plan Basic: base 10, included 100 units, overage 0.5 per unit, usage 120 units for the cycle\n- On day 15, User A upgrades to Plan Pro: base 25, included 200 units, overage 0.4 per unit\n- No dunning, no taxes\n\nQuestion: Write the high-level calculation steps (or pseudocode) you would implement to produce the final invoice amount for User A for that cycle, including proration. Explain the formulas you use and the final amount. You have 2\u20135 minutes.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core subscription billing with proration and usage metering from role (-1 for generic pseudocode vs specific stack). Clarity: Precise scenario, constraints, and time limit; unambiguous formulas expected. Discriminative: Requires applied math/trade-off understanding of proration logic, distinguishes those grasping proportional billing from rote learners.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-458",
        "role_title": "Backend Engineer - API Gateway Plugins",
        "question": "Question:\nYou are writing a Kong Lua plugin that must add a custom header X-User-Role to the upstream request based on a token in the Authorization header. The token is a simple string in the format \"Role:<roleName>\". If the header is missing or the token is invalid, respond with 401 and a JSON body {\"error\":\"invalid_token\"}.\n\n- Describe the minimal Lua code snippet (within the access phase) to:\n  - Read the Authorization header from the incoming request.\n  - Validate the token format and extract roleName.\n  - Set kong.response.set_header(\"X-User-Role\", roleName) and allow the request to proceed if valid.\n  - Return a 401 with the specified JSON body if invalid.\n- Explain how you would handle the case where multiple plugins might set the same header, ensuring the token takes precedence.\n\nProvide only the essential code snippet and concise explanation.",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's API gateway plugin development for Kong with auth/transformation (no red flags). Clarity: Crystal clear, well-scoped to access phase with precise requirements, minor deduction for lacking explicit code length constraint. Discriminative: Tests applied Lua/Kong knowledge, parsing, error handling, and thoughtful trade-offs on header precedence, strongly separates skilled candidates.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-453",
        "role_title": "Backend Engineer - Notification Delivery",
        "question": "Question:\nYou are implementing a Go function to batch and enqueue device tokens for push notification delivery. The system must:\n\n- Accept a list of device tokens with their platform (iOS or Android) and an associated message payload.\n- Batch tokens into groups of up to 500 for FCM and up to 100 for APNS per batch (these are typical provider limits).\n- Deduplicate tokens within the batch to avoid duplicate sends.\n- Return a list of batches, where each batch includes the platform, a token list, and the payload.\n\nGiven the following input type:\ntype DeviceToken struct {\n  Token   string\n  Platform string // \"ios\" or \"android\"\n}\n\ntype PushPayload struct {\n  Title string\n  Body  string\n}\n\nImplement a Go function:\n\nfunc CreateDeliveryBatches(tokens []DeviceToken, payload PushPayload) ([]Batch, error)\n\nWhere Batch is:\ntype Batch struct {\n  Platform string   // \"ios\" or \"android\"\n  Tokens   []string\n  Payload  PushPayload\n}\n\nConstraints:\n- Remove duplicate tokens across the entire input (not just within a platform).\n- Partition into batches such that:\n  - iOS batches have at most 100 tokens.\n  - Android batches have at most 500 tokens.\n- If a token appears with both platforms (which should not happen in valid data), keep the token only in the first platform encountered.\n\nProvide just the function implementation. You may assume the input is small enough to process in memory. Focus on correctness and clarity over performance micro-optimizations.",
        "scores": {
          "relevance": 9,
          "clarity": 10,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core batching/deduplication for push notifications in Go, matching role's device registration/batching focus (-1 for no explicit role tech stack match). Clarity: Crystal clear with precise types, constraints, scoping, and focus on correctness. Discriminative: Requires applied data structures (maps for dedup), grouping logic, and edge case handling (cross-platform dupes), strongly separates skilled Go devs.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-455",
        "role_title": "Staff Engineer - Developer Portal",
        "question": "Question:\nYou are asked to implement a lightweight, extensible \u201cService Catalog\u201d widget within a Backstage-based developer portal. The widget should display a list of services from a PostgreSQL-backed catalog and support a simple plugin hook that allows teams to add a custom field to each service (e.g., owner or service owner email) without altering the core schema.\n\nRequirements:\n- Describe the minimal database schema changes (if any) and the backend API endpoints (CRUD not required, but read + extensible field is fine). Include sample SQL for the schema and a brief explanation of how the dynamic field would be stored.\n- Outline how you would architect the React component in Backstage to render the catalog, fetch data via the backend, and allow a plugin to inject an additional field into each service item without modifying the main component code.\n- Explain how Backstage\u2019s plugin architecture would facilitate onboarding a new team to add their own fields and views to the catalog.\n\nProvide:\n- A concise schematic (textual is fine) showing data flow from PostgreSQL to API to React, and where the plugin hook hooks in.\n- A short code snippet (pseudo or TS) demonstrating:\n  - A backend route that returns the catalog with a dynamic extra field if provided by a plugin.\n  - A React hook or component method that consumes the API and renders the dynamic field, delegating rendering of that field to the plugin.\n\nTime: 2\u20135 minutes to answer.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests Backstage plugin architecture, PostgreSQL integration, and developer portal extensibility\u2014core to the role (-0 red flags). Clarity: Precise scope, explicit requirements, time-bound, unambiguous despite minor bundling. Discriminative: Probes schema design trade-offs, React/Backstage patterns, and plugin extensibility, separating strong architects from basics.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-456",
        "role_title": "Backend Engineer - Query Optimization",
        "question": "Question:\nYou have a PostgreSQL table orders(id serial primary key, user_id int, product_id int, amount numeric, created_at timestamptz). A frequent query is:\n\nSELECT count(*) FROM orders WHERE user_id = $1 AND created_at >= now() - interval '30 days';\n\nAssume the table has 1e8 rows and a btree index on (user_id, created_at). The query planner often chooses a sequential scan with a high cost, and the actual query runs slowly enough to notice.\n\n- What is the most likely reason the planner prefers a sequential scan over the index for this query?\n- Propose a concrete, minimal change (one or two steps) to PostgreSQL configuration or schema/indexing that would make this query use an index scan again and improve performance, with justification.\n- How would you validate your change after applying it?\n\nAnswer should be concise and focused on practical steps.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests PostgreSQL query optimization, indexing, and planner behavior - core to role's slow query detection and index recommendations (-0 red flags). Clarity: Precise scenario with schema/index/query, clear sub-questions, well-scoped for screening. Discriminative: Requires deep understanding of Postgres planner heuristics (e.g., selectivity, costs), practical fixes (e.g., index tweaks), and validation - distinguishes experts via trade-offs and real-world application.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-459",
        "role_title": "Junior Backend Engineer - Container Security",
        "question": "Question:\nYou have a Docker image built for a Go web service. You want to add a quick, practical check to ensure that your image includes no known critical vulnerabilities before deployment.\n\n- Describe the exact steps and commands you would run using Trivy to scan the image locally.\n- Include how you would interpret the results and decide whether to fail or proceed.\n- Show how you would integrate this into a simple CI snippet (bash) that runs the scan and exits with a non-zero status if any vulnerability has a severity of Critical or High.\n\nProvide the commands and brief rationale.",
        "scores": {
          "relevance": 7,
          "clarity": 9,
          "discriminative": 8,
          "reasoning": "Relevance: Tests container scanning with Trivy (core to role), but penalize -2 for focusing only on one minor tool ignoring broader role areas like policy enforcement/base image mgmt, and -1 for generic scanning approach applicable to any container role. Clarity: Crystal clear, well-scoped steps with specific asks. Discriminative: Tests applied knowledge of scanning, result interpretation, and CI integration with trade-offs in severity thresholds, separating practical candidates.",
          "composite": 8.0
        }
      },
      {
        "role_id": "be-457",
        "role_title": "Senior Backend Engineer - SLO Automation",
        "question": "Question:\nSuppose you have a service with an SLO of 99.9% availability per 5-minute burn window. The service receives requests already instrumented with success/failure outcomes. You have a burn-rate alerting system implemented in Go that stores a sliding window of the last 7 days and computes burn rate as the ratio of used error budget to total available error budget in the current window.\n\nGiven:\n- Error budget per 5-minute window = 0.000833... (i.e., 0.1% of 1, 0.1% over 5 minutes)\n- You observe for the current 5-minute window: 2 failed requests out of 1000 total requests.\n\na) Compute the burn rate for this 5-minute window.\nb) Explain how you would design a simple alert condition (logic) to trigger a burn-rate alert when the current burn rate exceeds a threshold for at least two consecutive windows. What threshold would you choose for a high-severity alert, and why?\nc) Outline a minimal Go snippet (pseudo-code or working logic) that updates the burn rate with a new 5-minute window result and determines whether an alert should fire, using a sliding window approach. Focus on core logic (no I/O or external systems).",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core SLO/burn rate skills for the role (-1 for no explicit Go requirement in desc but matches infra focus). Clarity: Precise scenario, well-scoped parts a-c, clear constraints. Discriminative: a) math application, b) design/trade-offs, c) Go logic distinguish deep vs superficial knowledge.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-460",
        "role_title": "Backend Engineer - Experiment Tracking",
        "question": "Question:\nYou are integrating an experiment tracking feature using MLflow in a Python project. A researcher logs two metrics for each run: \"accuracy\" and \"loss\". They want to compare two runs and automatically determine which one is better based on a combined score where higher accuracy is better and lower loss is better. The score should be defined as: score = accuracy - 0.5 * loss. \n\nProvide a minimal Python snippet (not a full app) that:\n- Loads two MLflow runs by their run_ids (assume they exist in the same tracking URI).\n- Retrieves the latest logged values for \"accuracy\" and \"loss\" for each run.\n- Computes the score for each run using the formula above.\n- Prints the run_id with the higher score and its score.\n\nAssume:\n- MLflow is already installed and configured.\n- You can use MLflow's tracking API to fetch metrics.\n- If a metric is missing for a run, you should skip that run and print a message indicating the missing metric.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests MLflow (core to ML experiment tracking role), metric handling, and comparison logic - perfect match (-0 red flags). Clarity: Precise scope, clear requirements, explicit assumptions, handles edge cases well. Discriminative: Requires applied MLflow API knowledge, error handling, and computation - distinguishes strong candidates via trade-offs like latest values and skipping invalids.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-461",
        "role_title": "Senior Backend Engineer - Fraud Detection",
        "question": "Question: You have a stream of transaction events coming in real-time via Kafka. Each event includes: user_id, transaction_id, amount, timestamp, location, and device_id. You need to score fraud risk using a simple feature-based approach in Python and publish scores to a Redis sorted set for downstream routing.\n\nTask:\n- Outline a minimal Python function that:\n  - consumes a single transaction dict,\n  - computes a simple risk score using: (a) amount > 1000, (b) same device_id as last transaction for the same user (persisted in-memory), and (c) if location differs from last known location for the user.\n  - stores/updates the user's last transaction info in an in-memory dict.\n  - returns the risk score (0-100) and a reason string describing active rules.\n- Explain how you would publish this score to Redis as a sorted set \"fraud:scores\" with score as the score and member as transaction_id, ensuring idempotence on replayed events.\n- Briefly discuss how you would adapt this to real-time streaming with Kafka and what considerations you would add for production.\n\nNote: You do not need to implement full Kafka consumer/producer code, just the core scoring function and Redis interaction outline. Provide concise code snippets.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core fraud detection pipeline skills (real-time scoring, rule engines) with exact tech stack (Kafka, Redis, Python); minor ding for in-memory state not production-scale. Clarity: Crystal clear task breakdown, precise scope, constraints explicit. Discriminative: Requires applied coding (features, state, idempotence), trade-offs (streaming/production), distinguishes practical engineers from theorists.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-441",
        "role_title": "Senior Backend Engineer - Autocomplete Systems",
        "question": "Question:\n\nYou are building a high-throughput autocomplete service. A request contains a userId and a prefix string. You have an index of previous search queries stored in Elasticsearch with fields: userId, query, timestamp. To support personalization, you want to bias results toward queries the user has searched recently, while still returning relevant global prefix matches.\n\nDesign a minimal but concrete approach (conceptual steps and data flows) to implement this within a sub-50ms latency target at ~100k QPS. Specifically address:\n\n- How you would structure the Elasticsearch query to combine prefix matching with a lightweight personalization boost (e.g., using a per-user recent-query weight).\n- How you would store or maintain the per-user recency signal (data model and eviction strategy) without causing hotspots.\n- How you would ensure low latency (caching, routing, and index shard considerations) and what metrics you would monitor to keep latency under 50ms.\n\nProvide a rough query snippet (pseudo-SQL or DSL) and a brief justification of the design choices. You do not need to implement code, just outline the approach.",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Relevance: Perfectly matches role's core requirements (autocomplete, prefix matching, personalization, 50ms@100K QPS) with exact tech (Elasticsearch); no red flags. Clarity: Crystal clear, well-scoped with specific sub-bullets, pseudo-code request, and constraints; minor deduction for no explicit time limit. Discriminative: Tests deep applied knowledge, trade-offs (latency, hotspots, caching), and role-specific design; excellent separation of strong vs weak candidates.",
          "composite": 9.666666666666666
        }
      },
      {
        "role_id": "be-462",
        "role_title": "Backend Engineer - Alert Routing",
        "question": "Question:\nYou are building an alert routing microservice in Go that ingests alerts from a source (e.g., Prometheus) and routes them to responders via PagerDuty. Given a simplified alert object:\n\ntype Alert struct {\n  ID        string\n  Status    string // \"firing\" or \"resolved\"\n  Severity  string // \"critical\" or \"warning\"\n  Labels    map[string]string\n  Annotations map[string]string\n  StartsAt  time.Time\n  EndsAt    time.Time\n}\n\nRequirements:\n- Implement a function routeAlert(a Alert) string that returns the ID of the responder target (e.g., \"pagerduty-oncall-foo\" or \"pagerduty-esc-1\") or \"\" if no routing applies.\n- Routing rules (hard-coded in the function for this exercise):\n  - If Status == \"firing\" and Severity == \"critical\", route to \"pagerduty-oncall-urgent\".\n  - If Status == \"firing\" and Severity == \"warning\", route to \"pagerduty-oncall-standard\".\n  - If Status == \"resolved\", always route to \"\" (no routing).\n  - If Labels has key \"team\" with value \"infra\", route to \"pagerduty-oncall-infra\" regardless of severity.\n- Deduplicate rapid repeated alerts: if the same alert (same ID) has Status unchanged for more than 5 minutes, return \"\" to simulate deduplication.\n- Provide a minimal, self-contained Go implementation of routeAlert, including handling for a simple in-memory state to track the last status time per alert ID.\n\nEdge cases to consider:\n- When an alert fires, but a matching exception label is present (e.g., Labels[\"ignore\"] == \"true\"), route to \"\".\n\nKeep the implementation succinct (2-5 minutes to write) and focused on core logic. You can assume necessary imports. Provide only the code snippet.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests Go coding for alert routing, deduplication, and escalation logic matching role (Prometheus/PagerDuty style); minor deduction as rules are simplified/hard-coded (-1). Clarity: Crystal clear spec, well-scoped with time estimate, precise requirements, no ambiguity. Discriminative: Requires applied Go knowledge (structs, maps, time logic, state management), edge cases, and deduplication trade-offs; distinguishes strong coders effectively.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-464",
        "role_title": "Staff Engineer - Presence Systems",
        "question": "Question:\nDesign a lightweight heartbeat protocol for a real-time presence system used by a chat app. You have a large fleet of WebSocket connections (Go backend) and need to detect dead/idle clients quickly while minimizing network and CPU overhead.\n\nRequirements:\n- Each client maintains a heartbeat with the server and vice versa.\n- The server should detect a dead connection within a configurable window (e.g., 5\u201315 seconds) and mark the user as offline if no heartbeat is received.\n- Consider how to handle network latency, NAT, and bursty traffic without flooding the system with heartbeats.\n- Outline the data format (JSON or binary), the message types, and the server-side logic to:\n  - register a new connection,\n  - refresh or update the last-seen timestamp,\n  - clean up and notify others when a user goes offline,\n  - scale across multiple instances (e.g., via Redis or Kafka).\n\nProvide a concrete example of the heartbeat flow (sequence of messages and timeouts) and a brief discussion of potential edge cases and how you would mitigate them.",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Relevance: Directly targets core role skills (heartbeat protocols, WebSocket connection management, state sync at scale) with exact tech mentions (Go, Redis/Kafka); no red flags. Clarity: Crystal clear, well-scoped with specific requirements, constraints, and outputs; minor deduction for no explicit time limit. Discriminative: Tests deep applied knowledge, trade-offs (latency/NAT/bursty traffic), scaling, edge cases\u2014strong separation of strong vs weak candidates.",
          "composite": 9.666666666666666
        }
      },
      {
        "role_id": "be-465",
        "role_title": "Backend Engineer - CDC Pipelines",
        "question": "Question:\nYou are configuring a PostgreSQL source using Debezium to stream changes into Kafka for a data lake. A table has an integer primary key id and a TEXT column name. You notice that during a full initial snapshot, Debezium is taking a long time, and after the snapshot, downstream consumers occasionally receive duplicate events if a transient failure occurs, even though you have configured Kafka exactly-once semantics.\n\n- Explain a practical approach to ensure idempotent processing and at-least-once vs exactly-once guarantees in this setup.\n- Include (a) what Debezium configuration or mode you would use for the initial snapshot, (b) how you would structure Kafka topics or keys to achieve idempotence, and (c) what downstream consumer logic (in Python) would look like to deduplicate events and maintain exactly-once semantics across restarts.\n\nProvide concise snippets or pseudo-code for the consumer deduplication portion and describe how you would handle schema changes/DDL in this pipeline. The answer should demonstrate how you would diagnose and fix issues in a real-time CDC pipeline end-to-end.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Relevance: Directly tests core CDC pipeline skills (Debezium, PostgreSQL, Kafka, snapshots, exactly-once, schema changes) matching role; minor deduction as Debezium isn't explicitly listed but is standard for described tasks (-1). Clarity: Well-scoped with specific parts (a,b,c), clear expectations for code/snippets, no bundling issues. Discriminative: Excellent at separating experts via trade-offs (at-least vs exactly-once, idempotence, consumer logic, schema handling, diagnosis), requires deep applied knowledge not rote recall.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-466",
        "role_title": "Senior Backend Engineer - GitOps Tooling",
        "question": "Question:\nYou are building a simple GitOps reconciliation loop in Go. Given a desired state stored in a Git repository (a YAML manifest) and a live Kubernetes cluster state, outline and implement a minimal reconcile function that:\n\n- Reads a YAML manifest with a single Deployment (name, namespace, image, replicas).\n- Compares it to the corresponding Deployment in the cluster.\n- If the live state differs (image or replicas), prints the needed Kubernetes patch operations (instead of applying them) to bring the live state in line with the desired state.\n\nConstraints:\n- Use a Go function signature: func Reconcile(desiredYAML []byte) ([]string, error)\n- Do not make actual API calls to Kubernetes; simulate the cluster state with an in-memory map of deployments (keyed by namespace/name) that you initialize inside the function.\n- Return a list of human-readable patch messages, e.g., \"Update deployment ns/app: replicas from 2 to 3\", \"Update deployment ns/app: image from foo:1.0 to foo:1.1\".\n- If the desired state perfectly matches the live state, return an empty slice with nil error.\n\nProvide:\n- A compact Go code snippet implementing Reconcile, including:\n  - YAML parsing into a DeploymentSpec struct\n  - In-memory live state initialization\n  - Diff logic for replicas and image\n  - Proper error handling for malformed YAML\n- Brief explanation (1-2 sentences) of how this would fit into a real controller, and one potential extension for drift detection.",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly targets core GitOps reconciliation loops, YAML manifests, Kubernetes Deployments, and Go\u2014directly matching role's declarative infra tooling focus (no red flags). Clarity: Crystal clear with precise signature, constraints, simulation, and scoped outputs; minor deduction for not specifying exact YAML structure. Discriminative: Requires applied Go/YAML/K8s knowledge, diff logic, error handling, and controller insight\u2014strong separation of strong vs weak candidates.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-468",
        "role_title": "Junior Backend Engineer - CI Pipelines",
        "question": "Question:\nYou are setting up a GitHub Actions workflow to build and test a Python project in a CI pipeline. The repository uses a Docker-based test environment and has several Python test modules. You want to optimize the workflow so that:\n\n- The Python dependencies are installed only once per workflow run and cached for subsequent jobs.\n- Tests can run in parallel across modules to speed up the pipeline.\n- The final step publishes a test coverage report if tests pass.\n\nProvide a concise YAML snippet (GitHub Actions workflow) showing:\n- a job that caches Python dependencies across the workflow using actions/cache\n- parallelizable test jobs (one job per test module directory, e.g., tests/moduleA, tests/moduleB)\n- a final job that runs after the test jobs to generate and upload a coverage report (assume coverage tool is pytest-cov)\n\nKeep the snippet focused on the core logic and include minimal commands to illustrate the steps. No external explanations.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core GitHub Actions skills (caching, parallelization, workflows) from role description (-0 red flags). Clarity: Crystal clear scope, constraints, and deliverables; no ambiguity or bundling. Discriminative: Requires applied YAML knowledge, trade-offs in caching/parallelization, and integration; distinguishes strong entry-level candidates able to produce functional snippet.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-467",
        "role_title": "Backend Engineer - Connection Pooling",
        "question": "Question:\nYou are tasked with designing a simple, in-process connection pool for PostgreSQL in Go to be used alongside PgBouncer in a Kubernetes environment. The pool should:\n\n- Limit the maximum number of active connections to PostgreSQL to N (configurable).\n- Reuse connections efficiently for sequential queries within a request.\n- Support a single transaction mode per connection (i.e., when a transaction starts, it must finish before releasing the connection).\n- Detect and gracefully handle a failed connection by removing it from the pool and creating a new one.\n\nProvide a concise, high-level outline (pseudo-code or bullet steps) for:\n1) Pool data structures and synchronization primitives you would use in Go.\n2) The logic for AcquireConnection and ReleaseConnection, including how you\u2019d handle a transaction boundary (BEGIN/COMMIT/ROLLBACK) and error cases.\n3) How you would ensure safe interaction with PgBouncer in a Kubernetes environment (e.g., connection multiplexing considerations, idle timeouts, and pool sizing hints).\n\nAssume you already have a working Go PostgreSQL driver (database/sql with pq or pgx) available. No need to write full code, but include enough detail to assess correctness and practicality.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core role skills (pool sizing N, transaction modes, failover) in exact tech stack (Go, PostgreSQL) with role context (PgBouncer, K8s); no red flags. Clarity: Crystal clear, well-scoped to high-level outline with numbered parts and assumptions; minor jargon but contextualized. Discriminative: Tests deep applied knowledge of Go concurrency, transaction handling, error recovery, and PgBouncer/K8s trade-offs; strong separation without being too broad.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-469",
        "role_title": "Senior Backend Engineer - Chaos Engineering",
        "question": "Question:\nYou are deploying a Go-based microservice on a Kubernetes cluster managed with Chaos Mesh. You want to ensure that when a network partition occurs between two service pods, the system maintains at least 99.9% availability for a 5-minute window, and you should observe recovery within 2 minutes after the partition ends.\n\n- Outline a concise plan (including the specific Chaos Mesh experiment you would use, how you would configure it, and what metrics you would monitor).\n- Provide a minimal Go snippet (or pseudo-code) showing how your service would gracefully degrade or retry to maintain availability during the partition.\n- Describe how you would validate the blast radius and recovery time using Prometheus/Grafana dashboards.\n\nAssume you have:\n- A Kubernetes cluster with Chaos Mesh installed.\n- A service mesh or standard Kubernetes networking setup.\n- Basic Go HTTP service with a retryable upstream call.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests chaos engineering (Chaos Mesh, failure injection), Go microservices, K8s, resilience, blast radius, and recovery\u2014core to role (-0 red flags). Clarity: Well-scoped with explicit parts, clear assumptions, no ambiguity. Discriminative: Requires applied knowledge of Chaos Mesh config, Go resilience coding, and observability setup with trade-offs, strongly separates candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-470",
        "role_title": "Backend Engineer - GraphQL Federation",
        "question": "Question:\nYou are building a GraphQL gateway using Apollo Federation to compose two subgraphs: Users and Orders. The Users subgraph exposes a User type with key field id, and the Orders subgraph exposes an Order type with a field userId that references the User. You want to enable cross-service entity resolution so a query can fetch a User along with their Orders in a single federated query.\n\nTask:\n1) Describe (pseudo-code or concise steps) how you would configure the User and Order types to be representable as federated entities, including:\n   - How to mark the entity types, keys, and reference resolvers in each subgraph.\n   - How to implement a federated resolver in the gateway to fetch an aggregated User with their related Orders.\n\n2) Provide an example of a GraphQL query that a client could run against the gateway to fetch a user by id along with their orders, and outline how the gateway would resolve this query across the two subgraphs.\n\nConstraints:\n- Use TypeScript-style pseudocode for the resolver signatures.\n- Focus on the core federation concepts: type definitions, _entities, and reference resolution, not full server setup.",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core requirements (federated GraphQL gateway, schema stitching, entity resolution) with exact tech (Apollo Federation). No red flags. Clarity: Crystal clear with numbered tasks, specific constraints, focused scope; minor deduction for no explicit time limit. Discriminative: Tests deep applied knowledge of federation setup, resolvers, query planning with trade-offs in entity resolution; strong separation between experts and juniors. Minor deduction as slightly broad but well-constrained (<10min feasible).",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-471",
        "role_title": "Staff Engineer - Zero Trust Architecture",
        "question": "Question:\nYou are designing a microservices platform using SPIFFE/SPIRE and mTLS. A service A in Namespace ns-a needs to call service B in Namespace ns-b. Outline, in concrete steps, how you would:\n\n- issue and validate mTLS certificates end-to-end between A and B using SPIFFE IDs\n- enforce a policy that only service A can call service B from within the cluster\n- handle certificate rotation without downtime\n\nAssume you are using Kubernetes, Envoy as the sidecar proxy, and SPIRE as the CA/identity agent. Be specific about SPIRE workload registration, Envoy config hints (e.g., SDS, TLS validation), and how you would configure the SPIRE policy/attestation to achieve the zero-trust requirement.",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Relevance: Perfectly matches role's core focus on zero-trust microservices, mTLS automation, and policy enforcement with exact tech stack (SPIFFE/SPIRE, mTLS, Kubernetes). No red flags.\nClarity: Crystal clear, specific components and scope defined; minor deduction only for not specifying response time/format.\nDiscriminative: Tests deep applied knowledge of integration, trade-offs (e.g., rotation without downtime), and configuration specifics, strongly separating architects from juniors.",
          "composite": 9.666666666666666
        }
      },
      {
        "role_id": "be-472",
        "role_title": "Backend Engineer - Vector Search",
        "question": "Question:\nYou have a FastAPI service that receives a text query and should return the top-3 most similar documents from a Pinecone index. Each document has a precomputed embedding stored in Pinecone with the external ID equal to the document_id. The service should also fallback to Redis-stored keyword matches if Pinecone latency exceeds 120ms on average.\n\nSketch a minimal, runnable Python snippet (no full app required) that:\n- Accepts a string query.\n- Generates its embedding using a provided function generate_embedding(text) -> List[float].\n- Queries a Pinecone index named \"docs-index\" to retrieve top-3 matches by cosine similarity.\n- If the Pinecone query latency exceeds 120ms on average (simulated), fall back to performing a simple Redis-based keyword match using a Redis key pattern \"doc:keywords:<document_id>\" where the value is a space-separated string of keywords; return the top-3 documents with the highest keyword overlap with the query.\n- Returns a list of document_ids for the final results.\n\nNotes:\n- You can assume the Pinecone client is already initialized as pinecone_client and the index as pinecone_index.\n- You can simulate latency threshold by checking a provided variable pinecone_latency_ms.\n- Focus on the control flow and integration logic, not on full error handling or deployment concerns.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core vector search (Pinecone embeddings, cosine similarity), hybrid search fallback (Redis keywords), and FastAPI integration - matches role focus perfectly (-0 red flags). Clarity: Crystal clear scope, precise constraints, simulation for latency, minimal viable snippet (-0 red flags). Discriminative: Requires applied knowledge of Pinecone/Redis APIs, embedding integration, latency-based fallback logic, and ranking trade-offs - strong separation of strong vs weak candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-473",
        "role_title": "Senior Backend Engineer - Refund Processing",
        "question": "Question:\nDesign a high-level approach and provide a concise code sketch (Java/Spring) for handling a partial refund in a refund processing service that integrates with a ledger system.\n\nRequirements:\n- Accept a refund request with orderId, totalAmount, and refundAmount (where refundAmount <= totalAmount).\n- Update an in-memory ledger (simulated with a Map) to reflect the partial refund against the original charge line item, ensuring idempotency for repeated refund requests with the same refundId.\n- Publish a refund event to Kafka (topic: refunds.events) after successful ledger update.\n- Record an audit trail entry (in-memory list) with timestamp, refundId, orderId, refundedAmount, and status.\n- Include basic error handling for over-refund attempts and idempotent behavior for duplicate refundId.\n\nWhat is the core data model, method signature, and a compact implementation sketch (pseudocode or real Java) that demonstrates these aspects? Focus on correctness and clarity suitable for a senior engineer.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core refund processing, ledger integration, idempotency, events, and audits from role; Java/Spring matches implied stack (-0 red flags). Clarity: Precise requirements, scoped constraints, clear deliverables; no bundling or vagueness. Discriminative: Requires applied design, trade-offs (idempotency, error handling), and code synthesis distinguishing seniors; not recall/trivia, well-scoped <10min.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-476",
        "role_title": "Backend Engineer - Live Updates",
        "question": "Question:\nYou are implementing a live dashboard using Socket.io with multiple rooms representing different data feeds. A user can subscribe to a subset of feeds and may disconnect/reconnect (reconnect should automatically rejoin the previously subscribed rooms). Describe or provide a minimal TypeScript/Node.js snippet that demonstrates:\n\n- How you would manage user subscriptions to rooms (subscribe/unsubscribe) using Socket.io namespaces or rooms.\n- How you would ensure that on reconnection, the client automatically re-enters the same set of rooms they were in before disconnect.\n- A brief outline or code snippet showing how you would broadcast an update only to users subscribed to a specific feed (room) without leaking to others.\n\nConstraints for answer:\n- Use TypeScript with Socket.io (server-side).\n- Include how you would store per-user subscription state (in-memory or Redis) and how you would rehydrate it on reconnect.\n- Keep the code snippet concise (approx 15-25 lines of core logic).",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests Socket.io rooms, reconnection, selective broadcasting - core to role's live update system; minor ding for not specifying storage tech but covers role focus perfectly (-1 generic feel avoided).\nClarity: Crystal clear with specific requirements, constraints, scope (15-25 lines), no ambiguity or bundling.\nDiscriminative: Tests applied Socket.io knowledge, state management trade-offs (in-memory/Redis), reconnection logic - distinguishes strong candidates via practical implementation depth.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-463",
        "role_title": "Junior Backend Engineer - Full-text Search",
        "question": "Question:\n\nYou have a PostgreSQL table articles(id serial primary key, title text, body text, tsvector_search tsvector). You want to implement a simple full-text search that:\n\n- updates tsvector_search when articles are inserted or updated\n- searches using a query string, returning the top 5 most relevant articles by simple ranking (ts_rank_cd with normalization 32D)\n\nTasks (do not need to write entire app, just the SQL and a minimal Python snippet):\n\n1) Write the SQL statements to:\n   - create a trigger function to automatically set the tsvector_search column from (title || ' ' || body) for inserts and updates.\n   - create a trigger on the articles table to call that function.\n   - create a GIN index on tsvector_search.\n\n2) Given a Python snippet using psycopg2 (or any psycopg3), show how you would execute a search for a user-provided query string q, returning id, title, and a ranking score, limited to 5 results, ordered by the rank. Use a simple ts_rank_cd with a tsquery built from plainto_tsquery on q.\n\nProvide only the necessary SQL and the minimal Python code to perform the search.",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's PostgreSQL full-text search focus (tsvector, triggers, ranking, GIN index). No red flags.\nClarity: Crystal clear tasks with precise specs (ts_rank_cd 32D, plainto_tsquery, top 5), minimal Python scope. Minor deduction for not explicitly stating Python library preference.\nDiscriminative: Tests hands-on applied skills (triggers, indexing, query building, integration) distinguishing implementers from theorists; not recall, strong separation for entry-level.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-475",
        "role_title": "Junior Backend Engineer - Search Analytics",
        "question": "Question:\nYou have a simple search log table in PostgreSQL with columns: id, user_id, query_text, timestamp, clicks, and result_rankings (a JSON array of document_id in the order shown to the user). Write a Python function that, given a user's search query and a list of observed document_ids returned in that order (as a JSON array), updates a SQLite/PostgreSQL table of per-query statistics to increment a \"total_queries\" counter for that exact query and append a new entry to a separate table \"query_results\" capturing query_text, document_id, rank, and timestamp. The function should:\n- Upsert the per-query statistics (create if not exists, then increment total_queries).\n- Insert all (query_text, document_id, rank, timestamp) rows for the observed results.\n- Do this efficiently for typical short result lists (e.g., 10 items).\n\nProvide a concise Python snippet (using psycopg2 or sqlalchemy) that performs this operation within a single transaction. Do not worry about schema setup, just show the function body and necessary imports.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests PostgreSQL, Python (psycopg2/SQLAlchemy), JSON handling, and query logging/updates core to search analytics role (-1 minor for mentioning SQLite option). Clarity: Crystal clear with precise schema, requirements, constraints, and scope. Discriminative: Requires applied SQL (upsert/insert), transaction handling, efficiency awareness, and Python DB integration, strongly separating capable entry-level engineers.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-477",
        "role_title": "Senior Backend Engineer - Data Catalog",
        "question": "Question:\nYou have a data pipeline that ingests raw JSON logs from Kafka, processes them with a Python Airflow DAG, and stores transformed records in PostgreSQL. You want to automatically capture metadata and lineage in Apache Atlas for both the input (Kafka topic) and output (PostgreSQL table) entities, including a simple data quality check (e.g., a field \"user_id\" must be non-null and a field \"event_ts\" must be within the last 30 days).\n\nDescribe, at a high level, the minimal steps and key code or configuration snippets you would implement to:\n- register Atlas entities for the Kafka topic and PostgreSQL table\n- create and attach lineage between the Kafka ingest step and the PostgreSQL write step\n- implement a basic data quality check that updates Atlas with a quality score or status\n\nExplain how you would trigger and ensure this metadata is captured automatically as part of the Airflow DAG execution.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (metadata extraction, lineage tracking) with exact tech stack (Airflow, Kafka, PostgreSQL) matching data catalog needs; minor ding for not mentioning profiling/governance explicitly (-1). Clarity: Well-scoped high-level question with precise requirements, no ambiguity or bundling. Discriminative: Requires applied knowledge of Atlas integration, lineage APIs, Airflow hooks, and DQ implementation with trade-offs, strongly separates experienced candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-474",
        "role_title": "Backend Engineer - Dashboards API",
        "question": "Question:\nYou are implementing a minimal dashboard rendering endpoint in Go for an observability platform. The frontend passes a dashboard template ID and a map of variable values (e.g., { \"host\": \"host-1\", \"region\": \"us-east\" }). The template contains placeholders in the form ${var} that should be substituted with the provided values. Additionally, some variables have default values if not supplied (e.g., \"region\" defaults to \"us-west\").\n\nDesign a concise approach and provide a small code snippet (Go) that:\n- Retrieves a template string from a mocked store (you can hardcode a template in the snippet).\n- Substitutes variables using the provided map, applying defaults where missing.\n- Returns the final rendered dashboard JSON string with all ${var} placeholders replaced.\n\nConstraints:\n- Do not rely on external libraries for template rendering; implement a simple substitution.\n- If a required variable is missing and has no default, return an error.\n- Include error handling for missing template or invalid input.\n\nExample:\nTemplate: { \"title\": \"Dashboard for ${host}\", \"region\": \"${region}\" }\nInput vars: { \"host\": \"db-01\" }\nDefaults: region -> \"us-west\"\nOutput: { \"title\": \"Dashboard for db-01\", \"region\": \"us-west\" }\n\nProvide just the code snippet and brief explanation (1-2 sentences).",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Directly tests Go API development, variable substitution, and error handling\u2014core to dashboard rendering in the observability role (-0 red flags). Crystal clear with precise scope, constraints, example, and output format. Strong separation via custom simple substitution implementation requiring applied Go knowledge, defaults, and error cases\u2014not trivial recall.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-479",
        "role_title": "Principal Engineer - Data Architecture",
        "question": "Question: You are designing a data platform that handles both transactional (OLTP) and analytical (OLAP) workloads in a single environment. You have PostgreSQL for OLTP, ClickHouse for analytics, Kafka for streaming, Redis for caching, and Kubernetes for orchestration.\n\nScenario: A new feature requires real-time user activity tracking with low-latency dashboards and periodic batch aggregations for BI reports. Outline a concrete high-level dataflow (components and data movement) that satisfies:\n\n- Real-time ingestion from a Kafka topic into the OLTP and/or analytical store\n- Efficient upserts and referential integrity in PostgreSQL for transactional data\n- Efficient, scalable analytics in ClickHouse without impacting OLTP performance\n- Caching and fast reads for dashboards\n\nProvide:\n1) The data routing/processing approach you would implement (include any adapters, stream processing concepts, and where you would materialize data).\n2) A succinct justification for chosen storage choices (why PostgreSQL vs ClickHouse for which data).\n3) A brief outline of deployment considerations on Kubernetes (scaling, schema migrations, and fault tolerance).\n\nLimit to 200-300 words.",
        "scores": {
          "relevance": 9,
          "clarity": 10,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core data architecture skills for mixed OLTP/OLAP workloads using exact tech stack from role (-0 red flags). Clarity: Crystal clear scenario, precise requirements, scoped with word limit and numbered parts. Discriminative: Requires applied trade-offs in dataflow design, storage choices, and K8s deployment, distinguishing strong architects from rote learners.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-480",
        "role_title": "Junior Backend Engineer - Deployment Automation",
        "question": "Question:\nYou have a Python script that installs a small web app on an Ubuntu server. The script should:\n- Create a virtual environment in /opt/myapp/venv\n- Install requirements.txt from the repository\n- Start the app with a simple Python HTTP server on port 8000\n- Ensure that if the server process dies, it is automatically restarted (basic restart policy)\n\nProvide a short Python snippet that:\n1) Creates the virtual environment and installs dependencies from requirements.txt\n2) Starts the HTTP server in the background in a way that can be restarted if it exits (you can assume the app is a simple script app.py in /opt/myapp)\n3) Demonstrates a minimal, portable way to implement a basic restart mechanism (without relying on systemd)\n\nNote: You don\u2019t need to implement full error handling, just show the core commands/logic to accomplish these steps.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 8,
          "reasoning": "Relevance: Directly tests Python scripting for Ubuntu deployment, virtualenvs, and restart logic\u2014core to entry-level deployment scripts role (-1 minor for not mentioning config mgmt/rollbacks explicitly). Clarity: Well-scoped with clear requirements, assumptions, and constraints; no ambiguity. Discriminative: Requires applied knowledge of venv, background processes, and restart loops to distinguish capable scripters from novices; not trivial recall.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-478",
        "role_title": "Backend Engineer - Cost Attribution",
        "question": "Question:\nYou are designing a cost attribution feature for a cloud platform. You receive resource usage data in a daily CSV feed with fields: resource_id, team_id, cost, and tags (a semicolon-delimited key=value string). Each resource is owned by exactly one team, but tags can include multiple attributes (e.g., env=prod;app=backend;region=us-east-1). The system must support generating a daily cost report that attributes each resource's cost to its team and also allows filtering by a tag key=value (e.g., app=backend).\n\n- Describe a minimal Go data model (structs) and a small in-memory processing snippet that:\n  - Parses a single CSV line into useful objects\n  - Aggregates total cost per team for those resources matching a given tag filter\n  - Produces a map[string]float64 where key is team_id and value is the total cost\n\n- Include edge-case handling (missing fields, malformed tags) and how you would extend this to handle large-scale data (without implementing full batch processing).\n\nYou may provide concise code snippets and brief explanations.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (cost attribution, resource tagging, team allocation, reports) with Go (common for cloud infra) and exact data model from role (-0 generic red flags). Clarity: Crystal clear scope, single focused task with explicit requirements/edges, well-constrained for screening. Discriminative: Requires applied Go (structs/parsing), aggregation logic, error handling, and scalable thinking with trade-offs, strongly separates strong candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-481",
        "role_title": "Backend Engineer - Canary Deployments",
        "question": "Question:\nYou are implementing a canary deployment for a Go microservice on Kubernetes with Istio. You deploy a new version (v2) alongside the current version (v1) and want to automatically promote to v2 only if the following conditions are met within 5 minutes:\n- The error rate of v2 remains <= 0.5% of requests.\n- The latency 95th percentile for v2 is within 10% of v1's 95th percentile.\n- The success rate (2xx/3xx/4xx/5xx) of v2 is not worse than v1 by more than 5%.\n\nDescribe in concrete terms:\n- How you would configure the Istio VirtualService/DestinationRule to route 5-10% of traffic to v2 initially and then shift traffic gradually.\n- What metrics you would collect in Prometheus, and how you would compute the three conditions above using Prometheus queries.\n- A high-level outline of an automated rollback/promotion decision loop (pseudocode or steps) that runs every 30 seconds and reverts if any condition fails, or promotes to 100% if all pass.\n\nAssume you have:\n- Service: \"orders\" with versions \"v1\" and \"v2\"\n- Istio metrics enabled and Prometheus scraping enabled\n- Go service emitting standard HTTP metrics (latency histograms, request counts, error counts) labeled by version\n\nKeep the answer concise but implementable.",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Relevance: Perfectly matches role's core focus on canary deployments, traffic shifting, metrics, and gates using exact tech (Kubernetes/Istio/Prometheus/Go); no red flags. Clarity: Crystal clear with precise constraints, assumptions, and scoping; minor -1 for no explicit time limit but well-contained. Discriminative: Tests deep applied knowledge of Istio config, PromQL queries, and automation logic with trade-offs, strongly separates experts.",
          "composite": 9.666666666666666
        }
      },
      {
        "role_id": "be-482",
        "role_title": "Senior Backend Engineer - API Monetization",
        "question": "Question:\nYou are implementing a simple quota system for an API service. Each API key has a monthly quota (e.g., 10,000 requests). When a request comes in, you need to check and increment usage, and return an error if the quota is exceeded. The system should be safe for concurrent requests and reset quotas at the start of each month.\n\nPropose a concise, implementable approach (in Go) using PostgreSQL and Redis that ensures:\n- Atomic check-and-increment of usage per API key within the month.\n- Fast reads for availability and minimal locking.\n- Reset of quotas at month boundary without data drift.\n\nProvide:\n- A brief data model (tables/columns) you would use.\n- The exact Redis or SQL operation(s) you would perform for a single request to enforce the quota.\n- How you would handle month rollover (e.g., a scheduled task or on-demand cleanup) and avoid double-counting or gaps.\n\nKeep code snippets minimal and focused on the critical path. Timebox your answer to 5 minutes.",
        "scores": {
          "relevance": 9,
          "clarity": 10,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core quota management and concurrency for API monetization role using relevant tech (Postgres/Redis), minor deduction for not mentioning billing integration (-1). Clarity: Crystal clear, well-scoped with explicit requirements, constraints, and timebox. Discriminative: Tests applied knowledge of atomic ops, caching, race conditions, and rollover strategies, strong separation of strong vs average candidates.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-483",
        "role_title": "Backend Engineer - WAF Integration",
        "question": "Question:\n\nYou are wiring up a Go service to manage ModSecurity rules for a commercial WAF integration. Given a request coming through Nginx, describe (in code or pseudocode) how you would:\n\n- Detect a simple rate-limiting pattern (e.g., more than 100 requests from the same IP within 60 seconds) using Redis as the counter store.\n- Trigger a ModSecurity rule from Go when the rate limit is exceeded, so the request is blocked at the WAF layer.\n- Ensure the detection is efficient and race-free in a concurrent Go environment.\n\nBriefly outline:\n- The Redis data structure and command sequence you would use.\n- How you would integrate with ModSecurity (e.g., via a rule push or API) and what data you would pass to trigger the block.\n- Any potential pitfalls and how you would mitigate them (e.g., clock drift, rate-limit reset, backoffs).",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Relevance: Directly tests Go, Redis, rate limiting, WAF/ModSecurity integration, and concurrency - core to role (-1 for no explicit Nginx mention in role). Clarity: Precise scope, clear components, well-constrained for screening. Discriminative: Requires applied knowledge of Redis Lua/scripts, Go concurrency (sync/atomic/context), ModSecurity triggering, and trade-offs/pitfalls like races/clock skew - excellent separation of strong candidates.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-484",
        "role_title": "Staff Engineer - Model Registry",
        "question": "Question:\nYou are designing a model registry for an ML platform using MLflow, backed by S3 for artifacts and PostgreSQL for metadata. Describe a practical approach to implement governance and versioning that ensures:\n- Each model version is immutable once registered.\n- Provenance is captured (training data version, code, and hyperparameters).\n- Deployment workflow promotes a model from stage (e.g., staging) to production with minimum downtime and rollback capability.\n\nIn your answer, outline:\n- The schema/metadata you would store in PostgreSQL and how it links to MLflow runs/artifacts.\n- How you would version models, including handling multiple ensembles or non-unique model names.\n- A high-level deployment flow (CI/CD-ish) with a promotion mechanism and rollback plan.\n- How you would ensure artifact integrity in S3 and traceability from registry to deployment.",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Relevance: Directly tests core role skills (model registry, versioning, governance, deployment) using specified tech (MLflow, S3, PostgreSQL); no red flags. Clarity: Well-scoped with specific requirements and outline; minor deduction for lacking explicit time/format constraints. Discriminative: Requires deep applied knowledge of trade-offs in ML ops (immutability, provenance, CI/CD, rollback), strongly separates experts from juniors.",
          "composite": 9.666666666666666
        }
      },
      {
        "role_id": "be-485",
        "role_title": "Junior Backend Engineer - Payment Webhooks",
        "question": "Question:\n\nYou are implementing a webhook receiver in Python (FastAPI) that must handle payment events from a provider. To prevent duplicate processing, you\u2019ll need a simple idempotency mechanism using Redis, keyed by a unique event_id provided in the webhook payload.\n\nRequirements:\n- Validate that the request includes a JSON body with fields: event_id (string) and event_type (string).\n- Implement a minimal handler that:\n  - Verifies the event_type is one of: \"payment_succeeded\", \"payment_failed\".\n  - Uses Redis to ensure each event_id is processed at most once. If the same event_id is received again, return 200 OK without re-processing.\n  - For the first-time event, simulate processing by printing \"Processed event {event_id}: {event_type}\".\n- Return appropriate HTTP status codes: 200 on success (including idempotent duplicates), 400 for invalid payload, 405 if method not allowed (no GET), etc.\n- Provide a short code snippet showing the FastAPI endpoint function and the core idempotency check (you can assume a Redis client named redis_client is already connected).\n\nSample payload:\n{\n  \"event_id\": \"evt_12345\",\n  \"event_type\": \"payment_succeeded\"\n}\n\nAnswer should be concise and only include the code snippet and brief explanation.",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role (payment webhooks, idempotency, Redis) with exact tech stack (Python/FastAPI). Clarity: Crystal clear requirements, well-scoped, sample payload helps; minor deduction for not specifying Redis commands explicitly. Discriminative: Tests applied skills in validation, idempotency, HTTP handling\u2014distinguishes those who can implement vs. theorize; concise scope fits screening.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-487",
        "role_title": "Senior Backend Engineer - Search Ranking",
        "question": "Question:\nYou are building a learning-to-rank (LTR) pipeline for a product search engine. You have a dataset of queries and candidate documents with the following features:\n- tf-idf similarity between query and document\n- PageRank-like popularity score\n- click-through-rate (CTR) feature from prior rankings\n- availability flag (in stock)\n- numeric length of the document title\n\nYou plan to train a pairwise RankNet-style model (or any pairwise ranking objective) to optimize ranking quality.\n\nPlausible quick tasks:\n- Propose a minimal but effective feature preprocessing/normalization strategy for these features before feeding them into a neural/LTR model (briefly justify).\n- Outline how you would construct training pairs from a labeled dataset (assume you have user click data indicating preferred documents for some queries).\n- Describe a simple evaluation metric and how you would validate improvements in an A/B test.\n\nProvide concrete steps or pseudocode snippets (2\u20134 bullet points) for preprocessing, pair construction, and evaluation. Keep it high-level and implementable within a few minutes.",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's LTR pipeline focus (features, model training, A/B testing) with exact tech (pairwise RankNet); no red flags. Clarity: Crystal clear, well-scoped with specific features/tasks, concrete instructions, minor vagueness on exact pseudocode depth (-1). Discriminative: Tests applied LTR knowledge (preprocessing trade-offs, pair construction, eval/A/B), requires reasoning over recall, strong separation; concise scope avoids time penalty.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-489",
        "role_title": "Backend Engineer - Data Quality",
        "question": "Question:\nYou have a daily Airflow DAG that loads a table fact_sales in Snowflake. A data quality check should ensure that:\n- the record count is within \u00b15% of the previous day\u2019s count, and\n- there are no nulls in the important dimension key column dim_store_id in the loaded data.\n\nProvide a compact Python snippet (to be used in a lightweight test) that:\n1) retrieves yesterday\u2019s row count from Snowflake for fact_sales,\n2) compares it to today\u2019s count and prints \"OK\" if within \u00b15% or raises an exception if not,\n3) checks that today\u2019s loaded data has no NULL values in dim_store_id and raises an exception if any NULLs exist.\n\nAssume you have a Snowflake connection object (ctx) from the Snowflake Python connector, and you can execute SQL via ctx.cursor().execute(sql). You may also assume a simple helper function run_sql(sql) that returns a list of tuples for results. Keep the snippet concise (roughly 15-25 lines).",
        "scores": {
          "relevance": 9,
          "clarity": 10,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core data quality skills (volume anomalies, null checks) for warehouse pipelines using exact tech stack (Airflow, Snowflake); minor -1 for no freshness/schema drift but hits main requirements. Clarity: Crystal clear with precise requirements, assumptions, constraints, and scope. Discriminative: Requires applied SQL/Python integration, threshold logic, and error handling; distinguishes those who can implement concisely from trivia knowers.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-490",
        "role_title": "Senior Backend Engineer - Environment Management",
        "question": "Question:\nYou are designing a simple Go utility that ensures each developer preview deployment gets a unique Kubernetes namespace with a 24-hour TTL. Given a base namespace prefix \"team-a-\" and a deployment name \"ui-review-123\", describe (with code or pseudocode) how you would:\n\n- Generate a deterministic, unique namespace name that avoids collisions.\n- Create a Kubernetes namespace object with a 24-hour TTL annotation and a ResourceQuota limiting CPU and memory (e.g., 100m CPU, 128Mi memory) for this namespace.\n- Implement a lightweight TTL cleanup check that you could run periodically to delete namespaces older than 24 hours, without enumerating all namespaces every time.\n\nNotes:\n- Assume you have a Kubernetes client-go client configured.\n- Focus on the critical logic, not full error handling or client setup.",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core needs (namespace isolation, TTL cleanup, resource quotas) using Go and Kubernetes directly. Clarity: Crystal clear, well-scoped with specifics (prefix, example, limits, assumptions); minor deduction for no exact time constraint. Discriminative: Tests applied Kubernetes API knowledge, deterministic naming trade-offs, TTL annotation/expiry logic, and efficient cleanup strategy\u2014strong separation of strong vs weak candidates.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-493",
        "role_title": "Staff Engineer - Reliability Standards",
        "question": "Question:\nYou are tasked with evaluating a production service running on Kubernetes with a PostgreSQL database. The service experiences a 5\u201310 minute BLIP once per week where latency spikes and error rates rise temporarily, then recover without human intervention.\n\n- Outline a concise, practical incident response plan you would expect the team to follow during such an event (include roles, data to collect, and steps to mitigate).\n- Propose two concrete reliability improvements (with minimal implementation effort) that would reduce the likelihood or impact of this type of incident, leveraging Go, Kubernetes, Prometheus, and PostgreSQL.\n- Explain how you would measure the impact of these changes and what success metrics you would track after deployment.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests SRE practices, incident response, reliability improvements, and production readiness\u2014core to role; uses relevant tech stack (Kubernetes, PostgreSQL, Go, Prometheus) without extras (-0 red flags). Clarity: Well-scoped with specific scenario, clear sub-parts, no ambiguity or missing constraints. Discriminative: Requires applied SRE knowledge, trade-offs in low-effort fixes, and metrics\u2014distinguishes deep understanding from basics; not recall or trivial.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-486",
        "role_title": "Backend Engineer - Span Processing",
        "question": "Question:\n\nYou are implementing a lightweight span processor in Go that performs tail sampling based on a simple rule: if a trace contains at least one span with an attribute \"error\" set to true, or if the trace duration exceeds a configurable threshold (in milliseconds), then the entire trace should be kept; otherwise, discard it. The processor receives spans from a Kafka topic as they arrive, buffers them per trace_id until the root span arrives (or until a timeout of 2 minutes), then decides whether to forward the trace to ClickHouse for storage or drop it.\n\n- Outline a minimal, idiomatic Go implementation approach (data structures and flow) to:\n  - Buffer spans by trace_id with a per-trace timeout.\n  - Detect the two tail-sampling conditions efficiently.\n  - Forward only qualifying traces to an output channel for downstream ingestion.\n- Provide a concise code snippet (pseudo-Go with real constructs) showing:\n  - The per-trace buffering mechanism using a map and a background cleaner.\n  - How you would evaluate the two conditions once the trace is complete or the timeout fires.\n- Explain how you would ensure this is safe for concurrent access and scalable under moderate load. No need to implement Kafka/ClickHouse I/O; just the in-memory logic.\n\nConstraints:\n- Assume spans arrive as: type Span struct { TraceID string; SpanID string; ParentID string; DurationMs int; Attributes map[string]interface{} }\n- The root span has ParentID == \"\".\n- The input stream guarantees spans for a given trace arrive in a single process, but multiple traces can be in flight concurrently.\n- Keep the answer concise (2\u20135 minutes of coding thought).",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (span processing, tail sampling, buffering by trace_id) in Go with exact tech (distributed tracing concepts, ClickHouse mention); minor ding for no Kafka/ClickHouse code but role-appropriate. Clarity: Crystal clear scope, precise constraints, unambiguous tasks with time guide; no bundling or vagueness. Discriminative: Requires applied Go knowledge (concurrency, maps, timeouts, idiomatic patterns), trade-offs in buffering/cleaning, distinguishes strong candidates via scalability/safety explanations.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-488",
        "role_title": "Backend Engineer - Gaming Backend",
        "question": "Question:\nYou\u2019re implementing a real-time lobby for a multiplayer game using Go, WebSocket, Redis, PostgreSQL, and Protocol Buffers.\n\nScenario: Players join a matchmaking queue. When at least 4 players are ready, they should be grouped into a game room, and all participants must receive a single, synchronized start message with a unique game session ID. The system must be resilient to a single player disconnecting before the match starts, in which case the remaining players should either be reassigned or the match should be canceled and players refunded to the queue if no enough players remain.\n\nTask:\n- Outline a small Go function or set of functions (pseudocode acceptable) that demonstrates:\n  - How you would atomically move 4 players from a Redis-based waiting queue into a Redis-backed \u201clobby\u201d or \u201croom\u201d with a unique session ID.\n  - How you would broadcast a synchronized start message to all 4 connected WebSocket clients using Protocol Buffers, ensuring all clients receive it at roughly the same time.\n  - How you would handle a disconnect of one player before the start, including which data structures you\u2019d modify and how you\u2019d decide to cancel or re-match.\n- Briefly describe how you\u2019d persist the final match metadata to PostgreSQL for auditing (e.g., session_id, player_ids, start_time).\n\nConstraints:\n- Emphasize atomicity and consistency in the matchmaking step.\n- Mention any Redis structures (e.g., lists, sorted sets, Lua scripts) you would use.\n- Include a short note on how you would generate and serialize the Protocol Buffers start message.\n\nProvide only the code-level outline or pseudo-code with concise explanations. time: 2\u20135 minutes.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (matchmaking, state sync, resilience) with exact tech stack (Go, WebSocket, Redis, PostgreSQL, Protobuf); minor deduction as anti-cheat not covered. Clarity: Crystal clear scenario, precise tasks, explicit constraints, scoped time limit. Discriminative: Requires applied knowledge of atomic Redis ops (Lua/scripts), WebSocket broadcast sync, disconnect handling trade-offs, and persistence\u2014strong separation of strong vs weak candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-492",
        "role_title": "Backend Engineer - CD Pipelines",
        "question": "Question:\nYou have a Kubernetes cluster managed with ArgoCD. A new feature branch has updated a Helm release for a microservice named \u201cpayments-service,\u201d and you want this change to be automatically promoted to the staging environment after a successful commit, but require a human approval before promoting to production.\n\nDescribe, at a high level, the minimal steps and configurations you would implement in ArgoCD and GitHub Actions (or your CI/CD tool) to achieve:\n- Automatic deployment to staging upon merge to the feature branch\n- Manual approval gate before promotion to production\n- A rollback plan if the staging deployment fails health checks\n\nIn your answer, include:\n- Brief outline of ArgoCD Application definitions or annotations you would use\n- An example of the GitHub Actions workflow triggers and a single job step that performs the promotion with a manual gate\n- How you would configure health checks or sync waves to enable safe promotion\n- How rollback would be triggered automatically if health checks fail on production\n\nTime: 2\u20135 minutes to answer.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core GitOps, ArgoCD, multi-env promotion, approval gates, and rollback\u2014perfect match to role; minor ding for assuming GitHub Actions. Clarity: Crystal clear with specific requirements, scoped by time, no ambiguity. Discriminative: Requires applied knowledge of integrations, configs, trade-offs in health checks/rollbacks\u2014strong separation of strong vs weak candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-495",
        "role_title": "Senior Backend Engineer - Audit Logging",
        "question": "Question:\nYou are designing a tamper-evident audit log in Go using a PostgreSQL-backed append-only table and a separate hash chain to detect tampering. Describe, at a high level, how you would implement:\n\n- The data model (tables/columns) to store log entries and their integrity proofs.\n- How you would compute and store a hash chain (e.g., per-entry hash chaining) and how you would verify integrity on read.\n- A simple API contract (Go function signatures) for appending a log entry and for querying a range of entries with integrity verification.\n- A basic retention/purging strategy that preserves integrity guarantees.\n\nAssume you must:\n- Ensure that any modification to past entries is detectable.\n- Maintain efficient writes (high throughput) and fast reads for verification.\n- Be able to recover integrity after downtime.",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Relevance: Perfectly matches role's audit logging, tamper-proof storage, query interfaces, and retention needs using specified Go/PostgreSQL stack. Clarity: Crystal clear with precise scope, assumptions, and components; minor deduction for no explicit time constraint. Discriminative: Requires deep applied knowledge of hashing, data modeling, API design, and integrity trade-offs, strongly separating strong candidates.",
          "composite": 9.666666666666666
        }
      },
      {
        "role_id": "be-496",
        "role_title": "Junior Backend Engineer - Model Serving",
        "question": "Question:\nYou have a FastAPI app that serves a machine learning model loaded from disk at startup. To improve throughput, you want to batch incoming requests per second before passing them to the model, and you want to implement a simple per-request latency measurement.\n\n- Outline a minimal FastAPI snippet (pseudo-code acceptable) that:\n  - Loads a PyTorch/NumPy model once at startup.\n  - Uses a simple in-process batching mechanism that collects requests arriving within a 100 ms window and processes them together.\n  - Returns individual results to each client, mapping the batch results back to the corresponding requests.\n  - Measures and logs per-request latency.\n\n- Briefly explain how you would test this locally (e.g., with curl or a small script) and what you would watch for in logs to ensure correctness and performance.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (FastAPI ML model deployment, batching, monitoring); minor ding as PyTorch/NumPy not explicitly listed but fits ML context (-1). Clarity: Crystal clear, well-scoped with specific requirements, pseudo-code flexibility. Discriminative: Requires applied knowledge of async batching, request mapping, latency tracking, and testing\u2014strong separation of strong entry-level candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-491",
        "role_title": "Junior Backend Engineer - Read Replicas",
        "question": "Question:\nYou have a Python application using PostgreSQL with a read-replica setup (primary and one read replica). The app uses SQLAlchemy and should route read queries to the replica when the replica is healthy, but fall back to the primary if the replica is lagging or unavailable.\n\nDescribe a lightweight approach (pseudo-code or actual Python snippet) to:\n- Detect replica lag before routing a read query.\n- Route the read query to the replica when lag is below a defined threshold (e.g., 5 seconds).\n- Fall back to the primary if the replica is lagging or unreachable.\n- Ensure writes always go to the primary.\n\nAssume you have:\n- Two SQLAlchemy engines: engine_primary and engine_replica.\n- A function get_replica_lag() that returns the replication lag in seconds, or raises an exception if the replica is unreachable.\n\nProvide the code for a small utility function get_read_connection() that returns the appropriate connection/engine for a read query, and explain how you would integrate it into query execution.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (replica routing, lag handling) with exact tech stack (Python, PostgreSQL, SQLAlchemy); minor deduction as entry-level but perfectly matches focus (-1 generic potential). Clarity: Crystal clear with explicit assumptions, scoped to one utility function, no ambiguity. Discriminative: Requires applied knowledge of routing logic, error handling, and integration; tests trade-offs in lag thresholds and fallbacks, distinguishes strong practical understanding.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-494",
        "role_title": "Backend Engineer - API Mocking",
        "question": "Question:\nYou are building a simple API mocking service in Node.js using TypeScript. The mock should read a predefined contract that specifies an endpoint, an HTTP method, a request body pattern, and a canned response. Implement a small function that, given:\n\n- method: string (e.g., \"POST\")\n- path: string (e.g., \"/users\")\n- body: any (the incoming request body)\n\nreturns the canned response if the incoming request matches the contract, or null if it does not.\n\nContract example (in code form):\n[\n  {\n    \"method\": \"POST\",\n    \"path\": \"/users\",\n    \"bodyPattern\": { \"email\": \".*@example.com\" },\n    \"response\": {\n      \"status\": 201,\n      \"headers\": { \"Content-Type\": \"application/json\" },\n      \"body\": { \"id\": 123, \"email\": \"user@example.com\" }\n    }\n  }\n]\n\nNotes:\n- The bodyPattern should be treated as a simple subset match using RegExp on any string field values (e.g., email) within the request body. If a field is not present in the incoming body, treat as non-match.\n- The function should be fast and side-effect free.\n- You can assume the incoming body is a plain JSON object with string values only for fields you need to evaluate.\n\nProvide the TypeScript implementation of this function (you can hard-code the contract array in the file). Include a few inline comments explaining the matching approach. The function signature should be something like:\nfunction matchMock(method: string, path: string, body: any): { status: number; headers: Record<string, string>; body: any } | null\n\nAim for concise, clear, and correct matching logic suitable for a mid-level engineer.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core API mocking skills (request matching, response templating) in Node.js/TS, matching role focus (-0 generic red flags). Clarity: Crystal clear with precise spec, example, constraints, and signature; well-scoped for screening. Discriminative: Requires applied knowledge of pattern matching (RegExp subset), recursion, TS typing; distinguishes mid-level engineers via trade-offs in fast/side-effect-free impl.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-500",
        "role_title": "Staff Engineer - Streaming Architecture",
        "question": "Question:\nYou need to design a real-time event path from a producer service to multiple downstream consumers (analytics, alerting, and a live dashboard). The system uses Rust, Kafka, Redis, and WebSocket, deployed on Kubernetes. Briefly describe a concrete, code-ish approach to ensure:\n- At-least-once delivery semantics from producer to Kafka\n- Efficient fan-out to Redis pub/sub channel and to a WebSocket feed for connected clients\n- Backpressure handling when downstream consumers lag\n- A minimal schema and serialization choice suitable for both Kafka and Redis, with versioning\n\nProvide:\n- A high-level data flow diagram (textual is fine)\n- The core Rust components or modules you would implement (names and responsibilities)\n- How you would implement idempotency and deduplication across retries\n- How you would monitor and observe lag and backpressure\n\nKeep it concise but show concrete, implementable ideas. The answer should be doable in 2\u20135 minutes.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core streaming architecture skills (event schemas, delivery guarantees, fan-out) and exact tech stack (Rust, Kafka, Redis, WebSocket, K8s); minor deduction for not emphasizing schema design as much (-1). Clarity: Crystal clear, well-scoped with explicit requirements, time guidance, and output format; no bundling or ambiguity. Discriminative: Excellent at separating candidates via trade-offs in delivery, backpressure, idempotency, fan-out, schema/versioning, and observability; concrete 'code-ish' ask tests deep applied knowledge without being too broad.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-498",
        "role_title": "Senior Backend Engineer - Profiling Infrastructure",
        "question": "Question:\nYou are adding a new feature to a continuous profiling service (Pyroscope-based) that collects CPU profiles from multiple Go services and stores them in ClickHouse for long-term analysis. A user should be able to compare their current baseline profile against a recent profile and view a simple flame-graph-like summary.\n\nProvide a concise, concrete plan (pseudo-commands or steps) to implement this:\n\n- How you would generate and store a short-term baseline profile alongside recent profiles, ensuring minimal storage impact.\n- How you would implement a quick, approximate comparison between the baseline and a target profile to identify the top hot functions (without rendering a full flame graph).\n- What data schema or minimal ClickHouse table structure you would use to support baseline + recent comparisons and efficient querying.\n\nConstraints:\n- Assume Go services emit profiles in the standard pprof-like format and Pyroscope is used for collection.\n- The solution should be executable within a 2\u20135 minute screening window (high-level steps, not full code).\n- Focus on observability and storage optimization considerations suitable for a senior backend engineer.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (profiling, flame graphs, baseline comparison, ClickHouse storage optimization) with exact tech stack (Pyroscope, Go, pprof); minor deduction as role doesn't explicitly mention Go/Pyroscope but aligns perfectly (-1 generic feel avoided). Clarity: Crystal clear with precise scope, constraints, and expected format; well-scoped for screening. Discriminative: Requires applied knowledge of profiling formats, storage trade-offs, schema design, and approximations\u2014distinguishes seniors via optimization reasoning and tech integration.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-497",
        "role_title": "Backend Engineer - Payout Systems",
        "question": "Question:\nYou are designing a payout batch job in Go that processes a daily settlements file and updates PostgreSQL with payout statuses. The file contains lines: payer_id, recipient_id, amount_cents, bank_account, status (PENDING/PAID/FAILED). The job must:\n- Read the file, validate that amount is > 0, and bank_account matches a simple Luhn check (you can assume a helper function isValidBankAccount(account string) bool exists).\n- Upsert a payout record for each line with columns: id (serial), payer_id, recipient_id, amount_cents, bank_account, status, processed_at (timestamp).\n- If any line is invalid, the entire batch should fail (atomicity). Use a transaction to ensure that either all lines are applied or none.\n- After successful commit, publish a message to a Kafka topic \"payouts.completed\" with the payout IDs array.\n\nProvide a concise Go code snippet (pseudo-implementation as needed) that:\n- Opens a DB transaction, reads lines from a provided reader, performs validation (including the bank account check), collects valid payout rows, and on success inserts them in a single batch within the transaction, then commits.\n- Demonstrates how you would handle rollback on validation failure and how you would publish to Kafka after commit.\n- Uses prepared statements where appropriate.\n\nAssume you have:\n- db *sql.DB\n- a function isValidBankAccount(account string) bool\n- a kafkaProducer with a method Produce(topic string, key string, value []byte) error\n- payouts table with columns id, payer_id, recipient_id, amount_cents, bank_account, status, processed_at\n\nKeep it concise (2-5 minutes for a candidate).",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (batch processing, validation, PostgreSQL upserts, atomicity) in exact tech stack (Go, PostgreSQL, Kafka); minor deduct for not explicitly mentioning reconciliation. Clarity: Crystal clear with precise requirements, constraints, assumptions, and scope (2-5 min, concise snippet). Discriminative: Requires applied knowledge of transactions, batch inserts, validation, rollback, Kafka integration, and prepared statements\u2014tests trade-offs in error handling and atomicity, strongly separates strong Go/backend engineers.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-499",
        "role_title": "Principal Engineer - Search Platform",
        "question": "Question:\nYou are designing a centralized search platform for an enterprise with multiple microservices emitting events to Kafka and storing indexed data in Elasticsearch. A new requirement is to support near-real-time search across customer records, but with strict field-level access control (row-level security) per department.\n\n- Outline the high-level architecture you would propose (components, data flow, and how you would achieve near-real-time indexing) using Java, Elasticsearch, Kafka, Kubernetes, and Redis.\n- Describe how you would implement field-level access control at query time without duplicating data or compromising performance. Include data model considerations, security boundaries, and a brief outline of the code or configuration approach you would use.\n- Identify potential operational challenges and how you would address monitoring, scaling, and fault tolerance.\n\nProvide concise bullet points you would share in a design review. (2\u20135 minutes to answer)",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core search platform architecture, strategy, and enterprise-scale ops matching role; uses exact tech stack implied (ES, Kafka) with no red flags (-0). Clarity: Crystal clear, well-scoped with time limit, single focused scenario, specific components listed; minor multi-part but cohesively tied (-0). Discriminative: Excellent separation via trade-offs (perf vs security), deep applied knowledge, ops challenges; not trivial recall, scoped for screening.",
          "composite": 9.0
        }
      }
    ]
  },
  "rl": {
    "model": "rl",
    "num_samples": 100,
    "avg_relevance": 9.04,
    "avg_clarity": 8.54,
    "avg_discriminative": 8.84,
    "avg_composite": 8.806666666666667,
    "results": [
      {
        "role_id": "be-401",
        "role_title": "Backend Engineer - Training Infrastructure",
        "question": "\"Imagine you're tasked with setting up a Ray cluster for distributed training of a large-scale ML model. You want to ensure that each training job is scheduled efficiently across available GPUs, and that resource utilization is balanced across the cluster. Describe how you would design the Ray cluster configuration (including cluster address, node configuration, and scheduling policies) to achieve this. Include specific details on how you would configure Ray's resource allocation, GPU scheduling, and job monitoring to prevent over-allocation or under-utilization of GPUs. Explain how you would verify that the cluster is working as intended.\"",
        "scores": {
          "relevance": 9,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests Ray (core for distributed ML orchestration, GPU scheduling), minus 1 for not explicitly listed in role but perfectly matches infrastructure needs. Clarity: Well-scoped with specific aspects, but lacks explicit constraints like cluster size/time (-1 red flag: missing constraints, -1 vague jargon context). Discriminative: Probes applied knowledge, trade-offs in config/scheduling/monitoring/verification, strong separation of experts.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-402",
        "role_title": "Backend Engineer - Model Deployment",
        "question": "\"Explain how you would implement a rollback strategy for a model deployed using KServe in Kubernetes, including how you would detect when a model is no longer performing acceptably and how you would automate the rollback to a previous version.\"",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core role skills (deployment strategies, rollback automation) with KServe/K8s fitting model deployment context; minor ding as KServe not explicitly listed (-1). Clarity: Precise, well-scoped, no ambiguity or missing constraints. Discriminative: Requires deep applied knowledge of monitoring, detection, and automation trade-offs, strongly separating experienced candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-403",
        "role_title": "Backend Engineer - Inference Optimization",
        "question": "\"Describe a scenario where you optimized model inference latency using quantization and batching in a production environment. Walk through the actual steps you took, including how you measured the improvement, and what challenges you faced (e.g., accuracy trade-offs, memory constraints, or model compatibility).\"",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Directly targets core role skills (quantization, batching, latency optimization) in production context; crystal clear with specific guidance on steps, measurement, challenges; elicits deep experiential knowledge with trade-offs, strongly separating candidates with hands-on expertise from those without.",
          "composite": 9.666666666666666
        }
      },
      {
        "role_id": "be-404",
        "role_title": "Senior Backend Engineer - Feature Platform",
        "question": "\"Design a feature computation pipeline using Feast and Spark to compute and store features for a user-based ML model. The pipeline should handle incremental feature updates, support feature discovery, and ensure data consistency. Walk through your design decisions, including how you'd structure the data flow, how you'd handle feature dependencies, and how you'd ensure consistency between online and offline feature stores. Provide a high-level diagram or pseudocode to illustrate your approach.\"",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (feature computation, online/offline stores, discovery) with exact tech (Feast); minor generic penalty avoided (-0). Clarity: Well-scoped design question with specific requirements, no bundling or ambiguity. Discriminative: Probes deep trade-offs in incremental updates, dependencies, consistency, and Feast/Spark integration, distinguishing strong ML platform engineers.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-405",
        "role_title": "Senior Backend Engineer - Model Monitoring",
        "question": "\"Design a drift detection system for a production ML model that monitors input feature distributions over time. Describe how you would implement this using Python, Prometheus, and PostgreSQL, including how you would collect, store, and visualize drift signals. Specifically, the system should detect concept drift in input features and trigger alerts when drift exceeds a configurable threshold. Include at least one example of a drift metric and explain how you would define the threshold for alerting.\"",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core drift detection for production ML monitoring using specified stack; minor ding for not explicitly covering performance tracking/alerting breadth (-1). Clarity: Crystal clear, well-scoped with specific tech, requirements, and example; no bundling or ambiguity. Discriminative: Tests applied system design, trade-offs in metrics/thresholds, and implementation depth, strongly separating strong ML engineers from weak ones.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-406",
        "role_title": "Senior Backend Engineer - LLM Serving",
        "question": "\"Explain how you would optimize a vLLM serving pipeline for low-latency inference with high throughput, focusing on KV cache management and batching strategies. Walk through your approach, including how you'd configure vLLM settings, handle KV cache sharing across requests, and ensure efficient batching for mixed workloads (e.g., long-context queries and short, fast prompts).\"",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core role responsibilities (batching optimization, KV cache management) and exact tech (vLLM) from description; no red flags. Clarity: Crystal clear, well-scoped with specific focus areas, minor deduction for lacking explicit time constraints. Discriminative: Tests deep applied knowledge, trade-offs (e.g., mixed workloads, sharing), and configuration expertise, strongly separating candidates.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-407",
        "role_title": "Staff Backend Engineer - ML Infrastructure",
        "question": "*\u201cYou're tasked with building a scalable training platform for large-scale ML models using Ray and Kubernetes. The platform must support distributed training across multiple GPUs, handle dynamic workloads, and optimize resource utilization. Describe how you would design the job scheduling and resource allocation strategy to ensure efficient use of cluster resources while minimizing job overhead and maximizing throughput. Be specific about how Ray\u2019s cluster mode and Kubernetes integrations would be used, and include considerations for scaling, fault tolerance, and cost optimization.\u201d*",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core focus on ML training platforms, distributed systems, and cost optimization; uses Ray/K8s directly relevant to scalable ML infra. Clarity: Crystal clear, well-scoped design question with specific tech and considerations; minor deduction for lacking explicit time/format constraints. Discriminative: Tests deep applied knowledge of Ray/K8s integrations, trade-offs in scheduling/scaling/fault tolerance, strongly separates experts from generalists.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-408",
        "role_title": "Junior Backend Developer - Health Endpoints",
        "question": "\"Design and implement a health check endpoint in Go that:\n1. Returns a 200 OK status for liveness and readiness probes\n2. Returns a 503 Service Unavailable when the service is not ready (e.g., during startup)\n3. Exposes metrics via a separate endpoint (e.g. /metrics) that can be scraped by Prometheus\n4. Uses Docker and Kubernetes to run the service and expose the health endpoints\"",
        "scores": {
          "relevance": 9,
          "clarity": 7,
          "discriminative": 8,
          "reasoning": "Relevance: Directly tests core role skills (health checks, liveness/readiness, metrics) and Go; minor ding for Docker/K8s not in role desc (-2 red flag overridden by core match). Clarity: Clear intent but bundles multiple endpoints + infra setup without time/format constraints (-2 for multiple questions). Discriminative: Tests applied design/implementation with trade-offs (startup logic, Prometheus), distinguishes strong implementers from weak ones.",
          "composite": 8.0
        }
      },
      {
        "role_id": "be-409",
        "role_title": "Junior Backend Engineer - File Upload",
        "question": "\"Design a FastAPI endpoint to handle file uploads for a user. The file should be validated for size (max 10MB), MIME type (only allow images), and stored in S3. The endpoint should return a success response with the file's upload ID and a message upon successful upload. Handle errors gracefully and provide meaningful error messages for common issues (e.g., invalid file type, file too large).\"",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (file upload API, multipart implied, validation, S3 storage) with exact tech (FastAPI). No red flags. Clarity: Crystal clear, well-scoped endpoint design with specific constraints; minor deduction as no explicit time/format limit. Discriminative: Requires applied knowledge of FastAPI uploads, validation, S3 integration, error handling, and trade-offs (e.g., async, security), strongly separating capable entry-level candidates.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-410",
        "role_title": "Backend Engineer - Bulk Operations",
        "question": "Imagine you're implementing a bulk import API for a user database where users are imported in batches of 1000 records. Each record has a unique `user_id` and a `name`. The API accepts a JSON array of user records and must:",
        "scores": {
          "relevance": 7,
          "clarity": 4,
          "discriminative": 6,
          "reasoning": "Relevance: Tests bulk API handling relevant to role but lacks explicit role description/tech stack and ignores idempotency/partial success (-3 generic). Clarity: Incomplete/incomplete prompt cuts off requirements, multiple interpretations, missing constraints (-4 for vagueness, bundled/missing scope -2). Discriminative: Scenario-based but superficial without full specs, tests basic knowledge not trade-offs.",
          "composite": 5.666666666666667
        }
      },
      {
        "role_id": "be-411",
        "role_title": "Backend Engineer - Admin APIs",
        "question": "You're building an admin API endpoint to manage user roles in a system. The endpoint needs to:\n- Accept a user ID and a new role (e.g., \"admin\", \"moderator\", \"viewer\")\n- Validate the role is in a predefined list of allowed roles\n- Check that the current user making the request has sufficient privileges (e.g., only users with \"admin\" role can modify roles)\n- Update the user's role in the database\n- Log the action with timestamp, user ID, role before/after, and a descriptive message\n- Return a 200 OK response if successful, or a 400/403/500 error with descriptive messages",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core admin API skills (privileged access, audit logging, workflows) from role description, no red flags. Clarity: Crystal clear requirements, well-scoped; minor deduction for unspecified implementation language/format. Discriminative: Requires applied knowledge of auth, validation, DB ops, logging, and error handling with trade-offs (e.g., privilege checks, audit details), strongly separates candidates.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-412",
        "role_title": "Backend Engineer - Analytics APIs",
        "question": "\"Design a Go-based analytics API endpoint that retrieves user session duration statistics for a given date range. The endpoint must support filtering by user ID, and it should efficiently handle time-series aggregation (e.g., average session duration per day). Use ClickHouse as the primary data store, Redis for caching, and PostgreSQL for metadata (e.g., user IDs). Assume the session data is stored in ClickHouse with a schema like: `user_id, session_start, session_end, session_duration`. The endpoint should return a structured response including daily averages and total counts. Explain your design decisions, including how you'd handle caching, query optimization, and data consistency.\"",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Relevance: Perfectly matches role's focus on analytics APIs, aggregation, time-series, and caching with exact tech stack (Go, ClickHouse, Redis, PostgreSQL). No red flags.\nClarity: Crystal clear with schema, requirements, and scope; minor deduction for lacking explicit time/format constraints.\nDiscriminative: Excellent at testing applied knowledge, trade-offs (caching, optimization, consistency), and deep design skills; strong separation of strong vs weak candidates.",
          "composite": 9.666666666666666
        }
      },
      {
        "role_id": "be-413",
        "role_title": "Senior Backend Engineer - API Gateway Logic",
        "question": "\"Explain how you would design a custom Envoy filter plugin in Go to modify HTTP headers before they are sent to downstream services, and describe how you would ensure it integrates securely and efficiently with Envoy's plugin system, including considerations for performance, memory, and security.\"",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Directly targets Envoy plugin development in Go for HTTP header modification and routing logic, core to API gateway extensions role. Crystal clear with specific scope (custom filter, integration, performance/security trade-offs). Tests deep applied knowledge and design decisions, strongly differentiating strong candidates familiar with Envoy's WASM/Go filter system from others.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-414",
        "role_title": "Senior Backend Engineer - Long-Polling",
        "question": "\"Design a long-polling API endpoint in Go that handles a 30-second timeout and supports a fallback to polling every 5 seconds if the long-polling request times out. Use Redis to store the result of the async operation and PostgreSQL to store the request context. The endpoint must manage client connections properly and ensure that when a client disconnects, the request is marked as failed. Include a method to verify the health of the long-polling service and a method to verify the health of the Redis and PostgreSQL connections.\"",
        "scores": {
          "relevance": 9,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core long-polling, connection management, timeouts, fallbacks, and async ops with role's tech (Go implied, Redis/PostgreSQL specified); minor ding for extra health checks. Clarity: Well-scoped and unambiguous, but bundles multiple elements (design + health checks) and lacks response format/time constraints (-2). Discriminative: Requires deep applied knowledge of trade-offs in connection handling, DB integration, fallbacks, and edge cases like disconnects, strongly separating strong candidates.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-415",
        "role_title": "Senior Backend Engineer - Multipart APIs",
        "question": "\"Design a resumable upload system using FastAPI and S3 that supports chunked uploads and resume from any point. Explain how you would structure the upload state, manage upload progress, and ensure data integrity when reassembling chunks on the server side. Include how you'd handle edge cases like network failures, partial uploads, and concurrent uploads from the same client.\"",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core role skills (resumable uploads, chunk management, S3 APIs) with specified tech (FastAPI/S3); no red flags. Clarity: Well-scoped design question with specific components and edge cases listed, minor deduction for lacking time/format constraints. Discriminative: Tests deep applied knowledge, trade-offs, and edge cases, strongly separating strong candidates who can reason through state/integrity issues from weak ones.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-416",
        "role_title": "Staff Backend Engineer - External APIs",
        "question": "\"Design a simple Go service that exposes an external API endpoint to validate a partner's API key. The service should:\n1. Accept a POST request with a `key` field in the request body.\n2. Store keys in memory (not persistence) for testing.\n3. Use OpenAPI to define the API spec and serve it via a built-in OpenAPI server.\n4. Validate the key against a predefined list of valid keys (hardcoded).\n5. Return a 200 OK with a JSON response body if valid, 401 Unauthorized if invalid.\n6. Include error handling for malformed requests (e.g., missing key field, invalid JSON).\"",
        "scores": {
          "relevance": 4,
          "clarity": 9,
          "discriminative": 7,
          "reasoning": "Relevance: Generic Go API coding tests superficially mention tech (Go, OpenAPI) but unrelated to core role of API strategy/monetization/enablement; misses role focus (-3 generic, -2 minor tech). Clarity: Crystal clear specs, well-scoped, unambiguous. Discriminative: Tests applied Go/web skills with error handling, differentiates solid coders but not deeply strategic.",
          "composite": 6.666666666666667
        }
      },
      {
        "role_id": "be-417",
        "role_title": "Junior Backend Developer - Event Sourcing",
        "question": "\"Design a simple event handler in Java that listens to a `UserRegisteredEvent` from Kafka, persists the event to PostgreSQL, and then emits a `UserReadyEvent` after a 1-second delay. Use only the provided technologies (Java, Kafka, PostgreSQL, Docker). Do not use Spring Boot or Spring Framework. You can use any Java 17+ library for Kafka and JDBC for PostgreSQL.\"",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core event handling, persistence, and event emission with exact tech stack (Java, Kafka, PostgreSQL); minor Docker mention doesn't detract (-0). Clarity: Crystal clear scope, explicit constraints, no ambiguity. Discriminative: Requires integrating streaming, DB ops, delays, and sequencing\u2014tests applied event sourcing skills and trade-offs (e.g., exactly-once, error handling), strongly separates candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-418",
        "role_title": "Junior Backend Engineer - Service Clients",
        "question": "\"Walk me through how you'd design a gRPC client in Go that automatically retries failed requests with exponential backoff, while also implementing a circuit breaker to prevent overwhelming the server during transient failures.\"",
        "scores": {
          "relevance": 9,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (retries, circuit breakers) in Go for service clients; minor deduction as gRPC not explicitly listed (-1). Clarity: Clear intent and well-scoped for screening, but lacks explicit constraints like max retries or thresholds (-1). Discriminative: Excellent at probing applied knowledge, trade-offs (backoff tuning, circuit states), and integration depth.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-419",
        "role_title": "Backend Engineer - Retry Logic",
        "question": "*Design a retry mechanism for a distributed system using Go, Redis, and PostgreSQL. The system must handle exponential backoff with jitter, respect a retry budget (e.g., max 3 retries), and avoid infinite loops. Explain how you would store retry state, manage backoff, and ensure the system does't retry indefinitely. Include a brief design for how you\u2019d handle failed operations that exceed the retry budget.*",
        "scores": {
          "relevance": 9,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core retry infrastructure skills (backoff, jitter, budgets) and specifies Go/Redis/PostgreSQL relevant to distributed ops (-1 for assuming specific DBs not in role desc). Clarity: Well-scoped with clear requirements, but 'design' lacks minor constraints like perf or scale (8). Discriminative: Probes applied knowledge, trade-offs (storage, budgeting, indefinite prevention), strong separation of strong vs weak candidates.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-420",
        "role_title": "Backend Engineer - Graceful Shutdown",
        "question": "How would you implement a graceful shutdown for a Go gRPC server that is running in a Kubernetes environment, ensuring that in-flight requests are completed before terminating the process, and that the server handles termination signals from Kubernetes (like SIGTERM) properly? Include considerations for connection draining and handling of client-side timeouts.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core role requirements (graceful shutdown, connection draining, signals) using Go gRPC and Kubernetes from context (-0 red flags). Clarity: Precise, well-scoped for screening, no ambiguity or bundling. Discriminative: Probes deep applied knowledge of signals, draining, trade-offs (e.g., timeouts), distinguishing strong candidates via reasoning over recall.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-421",
        "role_title": "Senior Backend Engineer - Security Automation",
        "question": "\"Describe how you would implement a Python-based pipeline to automatically detect and rotate secrets in AWS using HashiCorp Vault and Terraform. Include details on how the pipeline would detect outdated secrets, trigger rotation, and ensure compliance with security policies. Specifically, last rotated timestamp and rotation period are critical parameters. Provide a clear explanation of how this would work in practice, including any error handling, logging, or validation steps.\"",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core role skills (security automation, secrets rotation in AWS) with exact tech stack (Python, Vault, Terraform). No red flags. Clarity: Well-scoped with specific parameters and components; minor deduction for lacking explicit time/format constraints. Discriminative: Tests applied knowledge, trade-offs (detection, rotation, compliance), error handling; strong separation without being too broad or recall-based.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-422",
        "role_title": "Junior Backend Engineer - API Testing",
        "question": "Write a pytest test that verifies a FastAPI endpoint returns a 200 OK status and a JSON response with a specific field value (e.g., `{\"message\": \"success\"}`) when accessed with a GET request. Use a fixture to set up a test client and include error handling to catch any unexpected HTTP status codes or JSON parsing errors.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 8,
          "reasoning": "Relevance: Directly tests pytest fixtures, FastAPI testing, and error handling\u2014core to API tests role; minor ding as role emphasizes contract testing/fixtures more explicitly (-1). Clarity: Crystal clear, well-scoped with specific requirements and fixture use. Discriminative: Tests applied pytest/FastAPI knowledge with fixture and error handling, separates those who can write functional tests from trivia knowers.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-423",
        "role_title": "Staff Engineer - Incident Response Platform",
        "question": "\"Design a system to detect and respond to SLO violations in a Kubernetes environment using Go. Your solution should integrate with PagerDuty for alerts and Slack for notifications. It should monitor a critical service\u2019s SLO (e.g., 99.9% availability), store metrics and alerts in PostgreSQL, and handle alert deduplication and time-based suppression. Explain your architecture, how you'd implement alert deduplication and suppression, and how you'd handle failures gracefully.\"",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core SLO tracking, alerting, and incident response in Kubernetes/Go, matching role; minor ding for specifying PagerDuty/Slack/PostgreSQL not in desc (-1). Clarity: Well-scoped design question with specific integrations, components, and aspects to cover; no bundling or vagueness. Discriminative: Probes architecture, implementation details (dedup/suppression), and trade-offs (failures), separating strong engineers who can apply knowledge from weak ones.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-424",
        "role_title": "Backend Engineer - Message Serialization",
        "question": "\"Design a message serialization pipeline using Avro with schema evolution, where messages are produced to Kafka and consumed by multiple services. The pipeline must support backward and forward compatibility, ensure schema validation at both producer and consumer sides, and handle schema evolution gracefully. Explain how you would structure the schema registry integration, manage schema versioning, and handle schema conflicts during consumption. Provide a concrete example of a message schema evolution from v1 to v2, including how producers and consumers adapt to the change.\"",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Relevance: Perfectly matches role's core focus on message serialization, schema evolution, backward/forward compatibility, and validation; directly tests Avro/Kafka which align with infrastructure needs (no red flags). Clarity: Crystal clear and well-scoped with specific requirements, concrete example requested; minor deduction for no explicit time/format constraints. Discriminative: Excellent at testing deep applied knowledge, trade-offs in versioning/conflicts, and practical adaptation, strongly separating strong candidates.",
          "composite": 9.666666666666666
        }
      },
      {
        "role_id": "be-425",
        "role_title": "Senior Backend Engineer - Graph Analytics",
        "question": "\"Design a graph analytics pipeline to compute user influence scores in a social network using Neo4j and Apache Spark. The pipeline should handle a billion-edge-scale graph, support real-time updates via streaming, and allow for incremental computation of influence scores. Explain how you would structure the pipeline, the key challenges, and how you would balance between query performance and scalability.\"",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Relevance: Directly targets core role skills (graph analytics, billion-edge scale, social networks) with specified tech (Neo4j, Spark); minor ding for assuming Spark/Neo4j in role desc (-1). Clarity: Well-scoped design question with explicit constraints (scale, real-time, incremental); no bundling or vagueness. Discriminative: Excellent at testing trade-offs, deep understanding of challenges in distributed graph processing, streaming, and optimization\u2014strong separation of strong vs weak candidates.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-426",
        "role_title": "Junior Backend Engineer - DevOps Scripting",
        "question": "\"Write a Bash script that will automatically deploy a Python application to a production server using GitHub Actions. The script should:  \n- Clone the latest code from the main branch  \n- Install Python dependencies using pip  \n- Run the application with a specific environment variable  \n- Log all actions to a file called `deploy.log`\"",
        "scores": {
          "relevance": 4,
          "clarity": 8,
          "discriminative": 6,
          "reasoning": "Relevance: Generic Bash scripting with superficial DevOps elements (cloning/pip), but fundamentally mismatches role by asking for Bash script *within* GitHub Actions (a YAML CI workflow), not actual pipeline writing; generic programming (-3), misses core CI pipeline skills. Clarity: Well-formed requirements, unambiguous steps, good scope for screening. Discriminative: Tests basic Bash scripting and applied DevOps tasks (logging, env vars), differentiates weak scripters, but lacks trade-offs/depth; somewhat basic.",
          "composite": 6.0
        }
      },
      {
        "role_id": "be-427",
        "role_title": "Backend Engineer - Feature Store",
        "question": "\"Design a schema for a feature store that supports both batch and real-time feature serving using Feast. The schema should include tables for feature entities, features, and feature views. Include a description of how you would handle feature versioning and how you would ensure data consistency between batch and real-time feature serving.\"",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Directly tests core feature store skills (schema, versioning, batch/real-time serving) using Feast, matching role focus. Crystal clear with specific tables and aspects, well-scoped for screening. Probes applied knowledge of trade-offs in versioning and consistency, distinguishing strong ML infra candidates. Minor ding for lacking explicit time constraint.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-428",
        "role_title": "Senior Backend Engineer - Payment Orchestration",
        "question": "\"Design a system to route payment transactions across multiple providers (e.g., Stripe, Adyen, PayPal) using Go, PostgreSQL, Kafka, and Redis. Your design should include:  \n- How you would store provider configurations and health status in PostgreSQL  \n- How you would handle failover when a provider fails (e.g., due to timeout or error)  \n- How you would use Redis to cache provider latency or success rates  \n- How you would use Kafka to propagate transaction failures and trigger retries or alerts  \n- How you would ensure transaction consistency and avoid duplicate processing in a distributed system\"",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core needs (routing, failover, cost/success optimization) and exact tech stack. Clarity: Crystal clear with specific components scoped; minor deduction for no time limit. Discriminative: Tests deep applied knowledge, trade-offs (e.g., consistency in distributed systems), and integration; excellent separation of strong vs weak candidates.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-429",
        "role_title": "Backend Engineer - Log Aggregation",
        "question": "Explain how you would design a log aggregation pipeline using Go, Kafka, and ClickHouse to handle 1 million events per second. Include specific details on how you\u2019d handle log parsing, message enrichment, and retention policies in ClickHouse. Be sure to address scalability and data durability.",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core role skills (log pipelines, parsing, enrichment, retention) with exact tech stack implied by high-throughput needs (Go, Kafka, ClickHouse ideal). No red flags. Clarity: Crystal clear, well-scoped with specific components, throughput, and requirements; minor deduction for no explicit time/format constraints. Discriminative: Tests deep applied knowledge, trade-offs in scalability/durability, distinguishes strong architects from weak ones; appropriately scoped for screening.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-430",
        "role_title": "Junior Backend Engineer - Search Indexing",
        "question": "\"Explain how you would design and implement a bulk indexing job for a product catalog in Elasticsearch using Python, and describe how you would handle duplicates, ensure data consistency, and validate the indexing process.\"",
        "scores": {
          "relevance": 9,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core role skills (bulk indexing in Elasticsearch, mapping implied), matches tech stack; minor ding for not explicitly mentioning mapping design (-1). Clarity: Well-formed and unambiguous, but lacks explicit constraints like scale or time (-2). Discriminative: Excellent at testing applied knowledge, trade-offs (e.g., duplicate handling, consistency), and validation\u2014strong separation of candidates.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-431",
        "role_title": "Staff Engineer - Real-time Collaboration",
        "question": "\"Design a system that synchronizes a shared document between two users in real time using WebSockets and CRDTs. Specifically, highlight how you would handle a conflict when both users insert a character at the same time (e.g., User A inserts 'X' at position 3, and User B inserts 'Y' at position 3). How would you ensure the result is deterministic and consistent across both clients?\"",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Relevance: Directly targets core role skills (real-time collaboration, conflict resolution, operational transforms via CRDTs). Clarity: Crystal clear, well-scoped example with specific conflict scenario; minor deduction only for unspecified time constraints. Discriminative: Forces deep understanding of CRDTs, concurrency, determinism, and trade-offs, strongly separating experts from surface-level knowers.",
          "composite": 9.666666666666666
        }
      },
      {
        "role_id": "be-432",
        "role_title": "Backend Engineer - Data Validation",
        "question": "\"Design a Great Expectations suite for a data pipeline that ingests raw JSON logs from S3 and loads them into a PostgreSQL table. The logs contain user activity events with fields like `user_id`, `action`, and `timestamp`. The pipeline runs daily and must validate that the data is complete, consistent, and conforms to expected patterns. Describe how you would structure your Great Expectations suite, including the key expectations, how you\u2019d handle schema validation, and how you\u2019d integrate with Airflow. Include a specific example of a statistical check you\u2019d include.\"",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests Great Expectations for data validation (core to role), schema/statistical checks, with pipeline context; minor ding for assuming Airflow/S3/PostgreSQL not explicitly listed (-1). Clarity: Well-scoped design question with specific tech/context, no ambiguity. Discriminative: Requires applied knowledge of GE suites, expectations, integration, and trade-offs in statistical checks, strongly separates skilled candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-433",
        "role_title": "Senior Backend Engineer - Platform APIs",
        "question": "\"Design a scalable and robust service catalog API in Go using gRPC that supports service discovery, versioning, and self-service provisioning. The API must support listing available services, retrieving service details, and triggering a provisioning workflow. Include considerations for service discovery, versioning, and error handling in your design.\"",
        "scores": {
          "relevance": 8,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core service catalog and provisioning skills for the role, but specifies Go/gRPC (not explicitly listed) and misses resource provisioning depth (-2). Clarity: Clear intent and scope for screening, but lacks constraints like scale/performance metrics or time limits (-2). Discriminative: Excellent at probing trade-offs in discovery, versioning, workflows, and error handling, strongly separating strong candidates.",
          "composite": 8.333333333333334
        }
      },
      {
        "role_id": "be-434",
        "role_title": "Principal Engineer - Database Strategy",
        "question": "*You are leading a migration from a monolithic PostgreSQL database to a distributed database architecture using CockroachDB across multiple AWS regions. The current system has a 100GB monolithic PostgreSQL instance with 10TB of transactional data and 5TB of analytical data. The migration must preserve data consistency, minimize downtime, and ensure low-latency access for both transactional and analytical workloads. How would you design the data distribution and partitioning strategy to meet these requirements, and what specific steps would you take to ensure data consistency during the migration?*",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Relevance: Directly tests core role skills in multi-region DB strategy, migration planning, and tech selection (CockroachDB fits distributed/multi-region); minor ding as CockroachDB not explicitly listed (-1). Clarity: Crystal clear, well-scoped with specific data sizes, workloads, and constraints. Discriminative: Excellent at testing deep trade-offs in partitioning, data distribution, consistency models, and migration steps, strongly separating strong DB architects from others.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-435",
        "role_title": "Junior Backend Engineer - Queue Consumers",
        "question": "*You're building a consumer for a RabbitMQ queue that processes user registration events. Each event contains a user ID and email. The consumer must:*",
        "scores": {
          "relevance": 4,
          "clarity": 3,
          "discriminative": 2,
          "reasoning": "Relevance: Superficially mentions RabbitMQ (relevant) but incomplete/incomplete question tests nothing specific; misses core skills like ack patterns, dead letters, idempotency (-3 generic/incomplete, -2 ignores main focus). Clarity: Grammatically broken/incomplete (cuts off mid-sentence), multiple interpretations (-2). Discriminative: No actual requirements or task, trivial/impossible (-2).",
          "composite": 3.0
        }
      },
      {
        "role_id": "be-436",
        "role_title": "Backend Engineer - API Documentation",
        "question": "Explain how you would design a system to auto-generate OpenAPI/Swagger specifications from a Node.js/TypeScript backend, including versioning, example generation, and SDK generation. Walk through the steps, key considerations, and trade-offs you\u2019d make.",
        "scores": {
          "relevance": 8,
          "clarity": 7,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (auto-gen specs, versioning, examples, SDKs) and Node.js/TS stack, but generic phrasing subtracts 2. Clarity: Clear intent matching role, but 'design a system' lacks explicit scoping/constraints (-2). Discriminative: Excellent at probing applied knowledge, trade-offs, and depth for strong separation among candidates.",
          "composite": 8.0
        }
      },
      {
        "role_id": "be-437",
        "role_title": "Senior Backend Engineer - Secrets Management",
        "question": "\"Design a secure secrets rotation mechanism for a Kubernetes cluster using HashiCorp Vault and AWS KMS. The secrets must be encrypted at rest, rotated automatically every 90 days, and accessible only to service pods with minimal required permissions. Include details on how you would configure Vault, AWS KMS, and Kubernetes to enforce encryption-at-rest, handle rotation, and ensure audit logging for all access. Explain how you would verify the mechanism is working correctly.\"",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core needs (secrets rotation, encryption-at-rest, access policies, audit logging) with exact tech (Vault, KMS, K8s). Clarity: Well-scoped design question with specific requirements, minor deduction for lacking time/format constraints. Discriminative: Tests deep applied knowledge, trade-offs in integration/security, verification\u2014strong separation of experts from juniors.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-438",
        "role_title": "Staff Engineer - ML Serving Infrastructure",
        "question": "How would you design a high-throughput, low-latency ML serving system using Triton, and what specific strategies would you implement to optimize batching and model versioning for a real-time inference workload with variable request sizes and arrival patterns?",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Directly targets core role skills (ML serving infra, batching, model versioning, latency opt) with Triton (common for inference serving). Crystal clear scope for screening, tests applied trade-offs in batching/versioning for variable workloads. Minor ding for assuming Triton expertise without role mention, but highly relevant overall.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-439",
        "role_title": "Backend Engineer - Billing Integration",
        "question": "\"Design a system to handle proration for a subscription upgrade. A user upgrades from a $10/month plan to a $20/month plan on the 15th of the month. The original plan ran from the 1st to the 15th, and the new plan starts on the 15th. The user should only be charged for the difference in cost (i.e., $10) for the period from the 15th to the end of the month. How would you design the billing calculation to ensure this proration is accurate and handled correctly in a real-world system?\"",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core proration for subscription upgrades, matching role's billing focus (-0 red flags). Clarity: Precise scenario with clear dates, expected charge, and real-world system scope. Discriminative: Requires deep understanding of proration math, edge cases, and system design trade-offs, strongly separating candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-440",
        "role_title": "Junior Backend Engineer - Metrics Collection",
        "question": "\"Describe how you would instrument a Go service to collect CPU usage as a gauge metric and the number of incoming HTTP requests as a counter metric. Include how you would expose these metrics via Prometheus and visualize them in Grafana.\"",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Directly tests core instrumentation skills (gauge/counter) with Go, Prometheus, and Grafana visualization\u2014perfectly matches entry-level role focus. Crystal clear with specific metrics and full stack. Strong separation via applied knowledge requiring trade-offs in implementation/exposure.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-441",
        "role_title": "Senior Backend Engineer - Autocomplete Systems",
        "question": "Imagine you're building a global autocomplete system that must support 100K QPS with sub-50ms latency. How would you design the caching layer for autocomplete queries using Redis and Elasticsearch, and what specific strategies would you employ to handle sudden spikes in traffic and prevent cache thrashing? Please include how you'd balance personalization (e.g., user-specific suggestions) with performance, and how you'd validate performance under load.",
        "scores": {
          "relevance": 9,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core autocomplete caching, latency/QPS from role, Redis/ES implied as standard; minor ding for not explicitly listing them (-1). Clarity: Well-scoped with specifics, but open-ended design lacks strict constraints (-1). Discriminative: Probes trade-offs (personalization vs perf, spikes/thrashing), deep applied knowledge separates strong candidates.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-442",
        "role_title": "Backend Engineer - Event Streaming",
        "question": "Design a system that routes real-time user activity events (e.g., \"user clicked a button\") to multiple downstream consumers (e.g., analytics, notifications, audit logs) using NATS and Rust. The system must support fan-out, filtering (e.g., only events from users with `role: admin`), and replay capabilities. Describe the architecture, data flows, and how you would implement the fan-out and filtering. Be specific about how you\u2019d use NATS features, and how you\u2019d ensure replay is accurate and consistent.",
        "scores": {
          "relevance": 8,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (event streaming, fan-out, filtering, replay) but specifies unmentioned tech (NATS/Rust; -2). Clarity: Well-scoped with specific requirements and NATS focus, though minor lack of constraints like scale (-1 from 9). Discriminative: Excellent at probing applied knowledge, trade-offs in NATS features, Rust integration, and consistency challenges for strong separation.",
          "composite": 8.333333333333334
        }
      },
      {
        "role_id": "be-443",
        "role_title": "Senior Backend Engineer - Data Lake Architecture",
        "question": "Explain how to configure a Spark job to read from an Iceberg table in S3 with dynamic partition pruning enabled, and describe the steps to validate that partition pruning is actually occurring (not just inferred). Include how you would verify that schema evolution is properly supported and that time travel queries are working correctly.",
        "scores": {
          "relevance": 9,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (Iceberg tables, partition pruning, schema evolution, time travel) with Spark/S3 stack (-1 for no explicit Spark mention in role, but highly aligned). Clarity: Well-formed and unambiguous, but minor deduction for lacking explicit constraints on response format/depth. Discriminative: Excellent at separating candidates via applied knowledge, configuration steps, and validation (trade-offs in verification methods), not mere recall.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-444",
        "role_title": "Backend Engineer - Service Templates",
        "question": "\"Walk me through how you'd create a new Go service template using Cookiecutter and GitHub Actions. Include how you'd structure the project, what defaults you'd include for observability and security, and how you'd set up CI/DR for the service using ArgoCD. Explain your reasoning for each decision.\"",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (service templates, CI/CD, observability, security) with Go/Cookiecutter/ArgoCD; minor ding for GitHub Actions specificity not in desc (-1). Clarity: Well-scoped, unambiguous, single focused walkthrough with clear components. Discriminative: Requires applied knowledge, trade-offs, and reasoning, strongly separating candidates with role-specific expertise.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-445",
        "role_title": "Junior Backend Engineer - Database Migrations",
        "question": "\"Imagine you're working on a service that uses PostgreSQL and SQLAlchemy with Alembic for schema migrations. A recent migration failed due to a constraint violation during a `CREATE TABLE` statement. The migration script includes a `downgrade()` function, but it's not being called when the migration fails. How would you ensure that the database remains in a consistent state if a migration fails during the `up()` phase, and what steps would you take to recover from this scenario?\"",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core role skills (database migrations, PostgreSQL/Alembic implied, rollback/consistency) without red flags (-0). Clarity: Well-scoped scenario with specific context, unambiguous intent and depth. Discriminative: Probes applied knowledge of Alembic failure handling, downgrade invocation, recovery steps, and trade-offs like zero-downtime/rollback strategies, strongly separating candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-446",
        "role_title": "Staff Engineer - Distributed Tracing",
        "question": "\"Design a sampling strategy for distributed traces in a microservices environment with 500+ services, where you need to balance cost, data volume, and observability depth. Explain your approach, including key metrics, thresholds, and how you would validate it in production.\"",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core role responsibility of designing sampling strategies for distributed tracing at scale, matches 500+ services scenario perfectly, no red flags. Clarity: Crystal clear with specific context (microservices, 500+ services, balance factors), well-scoped for screening, minor deduction for lacking explicit time/format constraints. Discriminative: Tests applied design thinking, trade-offs (cost vs. observability), metrics, and validation\u2014strong separation of strong vs. weak candidates, not mere recall.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-447",
        "role_title": "Backend Engineer - Load Shedding",
        "question": "\"Design a load shedding mechanism using Go and Envoy that detects backpressure and triggers a graceful degradation strategy. Explain how you would implement adaptive throttling based on real-time metrics from Prometheus, and how you would ensure the system remains resilient during high-load scenarios. Use specific examples of backpressure signals and how they would be translated into throttling policies.\"",
        "scores": {
          "relevance": 9,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (load shedding, backpressure, adaptive throttling) with priority on Go/Prometheus; minor deduction for unmentioned Envoy (-1). Clarity: Well-scoped with specifics but assumes Envoy/Go familiarity without role context. Discriminative: Requires deep applied knowledge of trade-offs, integration, and resilience, strongly separating candidates.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-448",
        "role_title": "Senior Backend Engineer - Webhook Infrastructure",
        "question": "Design a webhook delivery system in Go that ensures guaranteed delivery using Kafka and PostgreSQL. The system must handle retry logic with exponential backoff, signature verification, and delivery analytics. Implement the core components: webhook receiver, message processor, and delivery tracker. Include error handling, idempotency, and metrics collection. Provide a high-level architecture diagram in text and describe how you would implement the consumer group, idempotency, and retry logic.",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role requirements (guaranteed delivery, retry, signatures, analytics) with exact tech (Go, Kafka, PostgreSQL). Clarity: Precise scope with components listed, minor deduction for lacking strict time/format constraints. Discriminative: Tests deep applied knowledge of distributed systems, trade-offs in retries/idempotency/consumer groups, distinguishes strong candidates.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-449",
        "role_title": "Junior Backend Engineer - Auth Integration",
        "question": "\"Implement a function that handles the OAuth2 authorization code flow with a provider (e.g., Google). The function should:\n1. Receive an authorization code from the client\n2. Exchange it for an access token and refresh token using the provider's token endpoint\n3. Store the tokens securely in a PostgreSQL database (with expiration timestamps)\n4. Return a JWT token for the client to use in subsequent API requests\n5. Include proper error handling for network issues, invalid responses, and expired tokens",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's OAuth focus, token handling, and refresh flows; uses PostgreSQL (common for entry-level backend) without straying. Clarity: Crystal clear steps and scope, minor vagueness on exact DB schema but well-formed for screening. Discriminative: Tests applied OAuth knowledge, security (storage/JWT), DB integration, and error handling\u2014distinguishes those who understand flows from rote memorizers. Minor broadness (-1) but scoped well.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-450",
        "role_title": "Principal Engineer - ML Platform Strategy",
        "question": "\"Describe a recent ML platform decision you made or influenced that had a measurable impact on team productivity or model quality. Walk through the context, your role, the technical approach, and the outcome\u2014specifically how it improved model quality or engineering velocity.\"",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core ML platform strategy, tooling decisions, and impact measurement\u2014perfect match for role. No red flags. Clarity: Crystal clear structure with specific components requested; minor vagueness on 'recent' depth but well-scoped. Discriminative: Forces demonstration of real-world applied experience, trade-offs, and outcomes, strongly separating strategic thinkers from weak candidates. No red flags.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-451",
        "role_title": "Backend Engineer - Subscription Billing",
        "question": "\"Design a system to handle proration for subscription plan changes. Explain how you would calculate the remaining balance after a customer changes plans mid-month, considering usage metering and variable pricing. Include how you would handle edge cases like partial billing cycles, overlapping usage, and plan changes on the last day of the month.\"",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core subscription billing skills (proration, plan changes, usage metering) from role description; no red flags. Clarity: Crystal clear and well-scoped for screening, minor deduction for no explicit time/format constraints. Discriminative: Requires deep applied knowledge of trade-offs and edge cases, strongly separates strong candidates.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-452",
        "role_title": "Senior Backend Engineer - Faceted Search",
        "question": "\"Design a solution to handle faceted search with dynamic filtering in Elasticsearch, ensuring accurate facet counts even when filters are applied. Explain how you'd structure the query, handle edge cases (e.g., overlapping filters, missing data), and optimize performance for large datasets (e.g., 10M+ documents). Address how you'd ensure facet count accuracy and prevent overcounting when multiple filters are applied.\"",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core role skills (faceted search, dynamic filtering, aggregation optimization, scale) using Elasticsearch, no red flags. Clarity: Crystal clear and well-scoped for screening, minor deduction for no explicit time constraint. Discriminative: Tests deep applied knowledge, trade-offs, edge cases, and optimization, strongly separates candidates; appropriately scoped.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-453",
        "role_title": "Backend Engineer - Notification Delivery",
        "question": "\"Design a system to deliver push notifications to mobile devices using FCM and APNS. Your system must handle device registration, batching, and delivery tracking. How would you structure the data model in PostgreSQL to store device registrations and delivery status? Include considerations for scalability, data consistency, and handling device tokens expiration. Provide a brief explanation of how you'd ensure delivery tracking and batching efficiency.\"",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core requirements (device registration, batching, delivery tracking) with specified protocols (FCM/APNS implied in role context). No red flags.\nClarity: Crystal clear, well-scoped to data model + key considerations; minor deduction as it bundles data model design with explanations but remains focused.\nDiscriminative: Tests deep applied knowledge of modeling for scalability/consistency, trade-offs (e.g., token expiration), and efficiency\u2014strong separation between good and weak candidates.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-454",
        "role_title": "Junior Backend Engineer - ETL Scripts",
        "question": "\"Write a Python script using pandas and PostgreSQL that reads a CSV file, transforms the data (e.g., clean missing values, standardize column names), and loads it into a PostgreSQL table. Include error handling for common issues like missing files or database connection failures. Test your script with a sample CSV and a sample database.\"",
        "scores": {
          "relevance": 9,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core ETL skills (read/transform/load) with pandas (data transformation) and PostgreSQL, perfect for entry-level ETL role; minor ding as scheduling not included (-1). Clarity: Clear and well-scoped but lacks specific constraints like CSV structure or table schema. Discriminative: Requires applied ETL pipeline with error handling and testing, distinguishes practical skills from theory.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-455",
        "role_title": "Staff Engineer - Developer Portal",
        "question": "\"Design a plugin architecture for a developer portal that allows plugins to dynamically register services and documentation, and explain how you would ensure that these plugins are safely loaded and validated before being used in production.\"",
        "scores": {
          "relevance": 9,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core plugin architecture skill from role (-1 for no explicit tech stack mention). Clarity: Clear intent and well-formed, but lacks minor constraints like scale or time (-2). Discriminative: Excellent at probing trade-offs in dynamic registration, safety, and validation, distinguishing strong architects.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-456",
        "role_title": "Backend Engineer - Query Optimization",
        "question": "\"Explain how you would use PostgreSQL's `EXPLAIN (ANALYZE)` to identify a query that has a poor execution plan, and walk through a step-by-step process to diagnose and fix the issue using index recommendations based on query patterns and data distribution.\"",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core PostgreSQL query optimization, slow query detection, plan analysis, and index recommendations (-1 for assuming PG as main stack, though fits perfectly). Clarity: Crystal clear, well-scoped step-by-step process with no ambiguity or missing constraints. Discriminative: Requires applied knowledge of EXPLAIN ANALYZE, diagnosis, patterns, and data distribution trade-offs, strongly separates skilled optimizers from basics.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-457",
        "role_title": "Senior Backend Engineer - SLO Automation",
        "question": "\"Design a system to track error budgets and generate burn rate alerts for a set of SLOs. Your solution should use Go and Prometheus to monitor application errors and detect when an SLO is approaching its error budget. Explain how you would structure the data model in Prometheus, how you\u2019d implement the burn rate calculation, and how you\u2019d trigger alerts when the burn rate exceeds a threshold. Include a brief explanation of how you\u2019d handle incident escalation if the burn rate exceeds the threshold.\"",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Directly tests core SLO/error budget tracking, burn rate alerting, and incident escalation from role desc. Specifies Go/Prometheus (relevant stack). Crystal clear with specific components asked. Strong differentiation via design, data modeling, calculations, and trade-offs; minor deduction for no explicit time constraint.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-458",
        "role_title": "Backend Engineer - API Gateway Plugins",
        "question": "You're tasked with creating a Kong plugin that validates JWT tokens using a custom Redis-based token blacklist. The plugin should:\n1. Extract the JWT token from the request header (`Authorization: Bearer <token>`)\n2. Verify the token's signature using a public RSA key stored in Kong's configuration\n3. Check if the token's `jti` (JWT ID) is present in a Redis set of blacklisted tokens\n4. If the token is blacklisted, return a 401 Unauthorized response with a custom error message",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests Kong plugin development, JWT handling, Redis integration, and request/response modification\u2014core to API gateway plugin role (-1 minor for no explicit blacklist Redis details in desc). Clarity: Precise steps, clear requirements, well-scoped for screening. Discriminative: Requires integration of auth, config, Redis, and error handling; tests applied Kong/JWT knowledge with trade-offs (e.g., Redis perf, error cases).",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-459",
        "role_title": "Junior Backend Engineer - Container Security",
        "question": "\"Describe how you would configure Trivy to scan a Docker image for vulnerabilities and ensure that only images with critical and high severity vulnerabilities are allowed to be deployed to production. Include steps for setting up a scan policy, defining severity thresholds, and integrating with a CI/CD pipeline (e.g., GitHub Actions) to enforce this policy before deployment.\"",
        "scores": {
          "relevance": 7,
          "clarity": 9,
          "discriminative": 8,
          "reasoning": "Relevance: Tests core vulnerability scanning skills with Trivy (highly relevant to container scanning role), but deduct -2 for focusing only on one minor tool (Trivy) while ignoring role's main areas like base image management/policy enforcement, and -1 for generic CI/CD without role-specific tech. Clarity: Crystal clear, well-scoped steps with specific expectations, no ambiguity or bundling issues. Discriminative: Requires applied knowledge of configuration, policy setup, thresholds, and integration, testing practical understanding and trade-offs effectively.",
          "composite": 8.0
        }
      },
      {
        "role_id": "be-460",
        "role_title": "Backend Engineer - Experiment Tracking",
        "question": "\"Imagine you're setting up a new experiment tracking system using MLflow and PostgreSQL for storing experiment metadata and metrics. You want to log a model's performance across different hyperparameters, including accuracy, precision, recall, and F1-score. In addition, you want to store model artifacts (e.g., a saved model file) in S3.",
        "scores": {
          "relevance": 9,
          "clarity": 8,
          "discriminative": 8,
          "reasoning": "Relevance: Directly tests core skills (experiment tracking, metric logging, artifact storage) with exact tech stack (MLflow, PostgreSQL, S3) from role; minor deduction for no -1 generic feel. Clarity: Clear scenario and requirements, but lacks explicit constraints on implementation details or scope (-1 for minor vagueness). Discriminative: Requires applied MLflow knowledge, integration trade-offs (DB vs S3), distinguishes candidates with hands-on experience from theorists.",
          "composite": 8.333333333333334
        }
      },
      {
        "role_id": "be-461",
        "role_title": "Senior Backend Engineer - Fraud Detection",
        "question": "\"Design a system to detect fraudulent transactions in real-time using a combination of rule-based triggers and machine learning models. Your response should include:  \n- A high-level architecture overview of the data flow from transaction arrival to fraud score generation  \n- How you would integrate a rule engine (e.g., Drools or a custom one) with the ML model scoring  \n- A description of how you would handle model drift and retraining in a production environment  \n- How you would manage the trade-off between false positives and false negatives in a high-stakes financial system\"",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Directly targets core role skills (real-time fraud pipelines, rule engines, ML integration, drift handling). Crystal clear with specific subpoints scoping response perfectly. Tests deep trade-offs and production concerns, strongly separating strong candidates; no red flags.",
          "composite": 9.666666666666666
        }
      },
      {
        "role_id": "be-462",
        "role_title": "Backend Engineer - Alert Routing",
        "question": "\"Design a smart routing strategy for alerts in a distributed system using Alertmanager. Consider a scenario where alerts from different services (e.g., API gateway, database, cache) are routed based on business impact levels (e.g., high, medium, low). Describe how you would implement alert routing with escalation policies, silence management, and deduplication. Include how you would handle alerts from the same service but with different error types (e.g., 500 vs 404), and how you would ensure alerts are not duplicated during high-volume events (e.g., a database crash affecting multiple endpoints).\"",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core needs (routing, grouping, escalation, silence, deduplication) and directly uses Alertmanager for alert infrastructure. Clarity: Crystal clear scenario and requirements, well-scoped; minor deduction for no explicit time/format constraints. Discriminative: Tests deep applied knowledge, trade-offs in routing/escalation/dedup across scenarios, strongly separates candidates able to design vs recite basics.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-463",
        "role_title": "Junior Backend Engineer - Full-text Search",
        "question": "How would you implement a full-text search query in PostgreSQL to find all products with a description containing \"wireless\" and a price less than $100, using a ranking function to prioritize results based on relevance? Include a brief explanation of how the ranking works.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Directly tests PostgreSQL full-text search (tsvector/ranking) core to role; clear, scoped query with explanation; distinguishes via applied knowledge of implementation and ranking trade-offs. Minor relevance ding as price filter is tangential but relevant.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-464",
        "role_title": "Staff Engineer - Presence Systems",
        "question": "Design a presence system that tracks user connections in real time using Go, WebSocket, and Redis. How would you ensure that presence state is updated reliably and consistently across clients, especially during network interruptions or client disconnects?",
        "scores": {
          "relevance": 9,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core role skills (presence, connections, state sync) with exact tech stack (Go, WebSocket, Redis); minor deduction for not mentioning heartbeats explicitly. Clarity: Well-scoped design question with clear focus on reliability/edge cases, but lacks explicit constraints like scale or time. Discriminative: Probes deep trade-offs in protocols, consistency, and failure handling, strongly separating strong architects from juniors.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-465",
        "role_title": "Backend Engineer - CDC Pipelines",
        "question": "\"Design a CDC pipeline using Debezium and Kafka to replicate data from a PostgreSQL database to Snowflake. Explain how you would handle schema changes, initial data snapshot, and ensure exactly-once delivery. Include a specific example of a schema change scenario and how you'd adapt your pipeline.\"",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Directly targets core role skills (CDC, schema changes, snapshots, exactly-once) with exact tech stack. Crystal clear and well-scoped for screening; minor deduction for no explicit time constraint. Strong trade-off testing via design + specific example distinguishes deep applied knowledge.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-466",
        "role_title": "Senior Backend Engineer - GitOps Tooling",
        "question": "\"Design a reconciliation loop for a Kubernetes cluster using ArgoCD with Helm charts, ensuring that the cluster state is kept in sync with the Git repository. Include considerations for handling drift, partial updates, and rollbacks in your design.\"",
        "scores": {
          "relevance": 9,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core GitOps skills (reconciliation, drift, rollbacks) with exact tech (ArgoCD, Helm, K8s); minor ding for not explicitly mentioning tooling design (-1). Clarity: Clear and well-scoped for screening, but lacks explicit constraints like time or detail level (-2). Discriminative: Excellent at testing applied design, trade-offs (drift/partial/rollbacks), separates strong GitOps experts from juniors.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-467",
        "role_title": "Backend Engineer - Connection Pooling",
        "question": "\"Design a Go service that uses PgBouncer to manage connection pooling for a PostgreSQL database in a Kubernetes environment. The service must support dynamic pool sizing based on workload metrics and handle failover to a standby database when the primary fails. Describe how you would implement the pool sizing logic, the failover mechanism, and how you would ensure the service remains stable during high-load traffic spikes.\"",
        "scores": {
          "relevance": 9,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (pool sizing, failover) with relevant tech (Go, PgBouncer, PostgreSQL, Kubernetes); minor ding as PgBouncer/K8s not explicitly listed (-1). Clarity: Clear and well-scoped, but lacks explicit constraints like time or metrics sources (-1, -1). Discriminative: Excellent at probing trade-offs in dynamic sizing, failover, and stability under load, separating strong candidates.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-468",
        "role_title": "Junior Backend Engineer - CI Pipelines",
        "question": "\u201cWrite a GitHub Actions workflow that builds and tests a Python package using Docker. Include caching of pip dependencies and parallel job execution for faster testing.\u201d",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests GitHub Actions workflows, Docker builds, pip caching, and parallel testing\u2014core to role's CI pipeline focus (-1 minor for no explicit time constraint). Clarity: Precise scope, unambiguous requirements, well-formed for screening. Discriminative: Requires applied knowledge of syntax, caching strategies, and parallelization trade-offs, separating capable entry-level candidates effectively.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-469",
        "role_title": "Senior Backend Engineer - Chaos Engineering",
        "question": "\"How would you design a failure injection test to simulate a node failure in a Kubernetes cluster using Chaos Mesh, ensuring that the application remains operational and recovers automatically within a defined time window? Include specific configuration details for the Chaos Mesh experiment, such as the target pod, failure duration, recovery validation, and any necessary monitoring integration.\"",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core chaos engineering skills (failure injection, recovery validation) with Kubernetes and Chaos Mesh, matching role focus; minor ding as Chaos Mesh not explicitly listed (-1). Clarity: Crystal clear, well-scoped with specific config details requested, no ambiguity. Discriminative: Requires applied knowledge of Chaos Mesh configs, trade-offs in blast radius/recovery, and integration\u2014strong separation of experts from novices.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-470",
        "role_title": "Backend Engineer - GraphQL Federation",
        "question": "\"Design and implement a GraphQL query plan for a federated subgraph that resolves a query involving a nested object with multiple fields. Specifically, create a schema definition for a `User` type that includes a `posts` field (a list of `Post` objects), and a `Post` type with a `comments` field (a list of `Comment` objects). Each subgraph must return only its own data, and the gateway must resolve references to the nested objects. Explain how the query planning works during execution, including how Apollo Federation determines which subgraphs to query, how it resolves field values, and how it handles potential schema conflicts or missing fields.\"",
        "scores": {
          "relevance": 9,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core federated GraphQL skills (query planning, schema, entity resolution) from role; minor ding for not explicitly mentioning stitching (-1). Clarity: Clear and well-scoped example, but lacks explicit constraints on response format/depth (-1) and bundles schema+planning+explanation (-1). Discriminative: Excellent at testing applied deep understanding of federation mechanics, trade-offs in resolution, and execution\u2014strong separation of experts from juniors.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-471",
        "role_title": "Staff Engineer - Zero Trust Architecture",
        "question": "\"Describe how you would implement mutual TLS (mTLS) for service-to-service communication in a Kubernetes cluster using SPIFFE and SPIRE, and how you would ensure that mTLS is enforced at both the Envoy proxy and the Kubernetes API server level. Include at only one practical detail about how you would handle certificate lifetime and renewal.\"",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core mTLS automation in Kubernetes microservices (-0 red flags). Clarity: Unambiguous, well-scoped with specific tech (SPIFFE/SPIRE, Envoy, K8s API) and single detail constraint. Discriminative: Requires applied knowledge of integration, enforcement, and trade-offs like cert renewal, distinguishing strong architects.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-472",
        "role_title": "Backend Engineer - Vector Search",
        "question": "What are the key differences between using Pinecone's built-in vector indexing and implementing a custom indexing pipeline with Redis + PostgreSQL for a high-throughput semantic search application? Evaluate each approach in terms of performance, scalability, cost, and accuracy, and explain which you'd choose for a production environment and why.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core vector search/indexing skills with role-relevant tech (Pinecone, Redis+Postgres for hybrid search); minor ding as Pinecone not explicitly listed but fits perfectly (-1). Clarity: Crystal clear, well-scoped evaluation criteria, no ambiguity. Discriminative: Forces trade-off analysis across key dimensions, distinguishes deep production knowledge from superficial recall.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-473",
        "role_title": "Senior Backend Engineer - Refund Processing",
        "question": "Explain how you would design a system to process partial refunds in a payments platform, ensuring data consistency and auditability across distributed components like Kafka, PostgreSQL, and Redis, while handling ledger reconciliation during reconciliation windows. Consider edge cases like provider callbacks, duplicate refunds, and system failures.",
        "scores": {
          "relevance": 9,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core refund processing, ledger reconciliation, audit trails, and provider callbacks from role; uses specified tech (Kafka, PostgreSQL, Redis) - minor deduction for no explicit 'partial refund support' emphasis. Clarity: Clear intent and scope, but 'design a system' lacks explicit time/format constraints. Discriminative: Excellent at probing trade-offs, edge cases, and deep distributed systems understanding for strong candidate separation.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-474",
        "role_title": "Backend Engineer - Dashboards API",
        "question": "Design a Go endpoint that allows a user to create or update a Grafana dashboard template with variable substitutions. The endpoint should accept a JSON payload containing a dashboard JSON structure and a list of variables (e.g., `{ \"name\": \"env\", \"options\": [\"prod\", \"staging\"] }`). The template should support variable substitution using the format `{{env}}` in the dashboard JSON. The endpoint must validate that:\n1. All variable names in the template are defined in the input.\n2. Variable values are properly substituted into the final dashboard JSON.\n3. The input dashboard JSON is valid (e.g., correct structure, no syntax errors).\n4. The output dashboard JSON has no remaining `{{variable}}` placeholders.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests Go API development, JSON handling, and variable substitution core to dashboard template role (-1 for no explicit role tech stack like DB/storage). Clarity: Well-scoped design question with precise requirements and validation rules. Discriminative: Requires applied Go skills, validation logic, substitution implementation, and edge-case handling to separate strong candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-475",
        "role_title": "Junior Backend Engineer - Search Analytics",
        "question": "Explain how you would design a system to log and analyze search queries for a web application, including how you'd collect query data, store it, and visualize it using Elasticsearch and Kibana. Focus on the key components and trade-offs.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Directly targets core role skills (query logging, analytics) with exact tech stack (Elasticsearch/Kibana). Crystal clear with focused scope on components/trade-offs; strong separation via design reasoning and trade-offs, not recall. Minor ding for entry-level breadth but well-scoped for screening.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-476",
        "role_title": "Backend Engineer - Live Updates",
        "question": "\"Imagine you're building a live dashboard where users can join rooms to view real-time data updates. Each room can have multiple users, and only users in the same room should receive updates. When a user joins a room, they must be able to reconnect later with their state preserved\u2014this includes their current room assignment and any pending updates from previous sessions. Design a solution using Socket.io and Redis to handle room management, selective broadcasting, and reconnection preservation. Explain how you'd structure your code to manage the state of users and rooms, and how you'd ensure that when a user reconnects, they get their previous room and any pending updates.\"",
        "scores": {
          "relevance": 9,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (room mgmt, selective broadcast, reconnection) with exact tech stack (Socket.io, Redis); minor generic feel -1. Clarity: Well-scoped design question with clear requirements, but lacks explicit constraints like scale or time (-1 from red flag). Discriminative: Probes deep applied knowledge, trade-offs in state mgmt/pub-sub/reconnection, excellent separation of strong vs weak candidates.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-477",
        "role_title": "Senior Backend Engineer - Data Catalog",
        "question": "\"Describe how you would design a system to automatically extract and store lineage information from a data pipeline using Apache Atlas, Kafka, and Airflow. Include details on how you would capture the source and target of data movements, how you would handle schema changes, and how you would ensure data lineage is updated in real-time as the pipeline runs.\"",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly targets core role skills (lineage tracking, automated metadata) using relevant tools; no red flags. Clarity: Crystal clear, well-scoped with specific aspects to cover; minor deduction for lacking explicit time/format constraints. Discriminative: Tests applied design knowledge, trade-offs (schema changes, real-time), distinguishes strong big data engineers.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-478",
        "role_title": "Backend Engineer - Cost Attribution",
        "question": "What are the key challenges in building a cost attribution system that accurately assigns cloud costs to teams or projects, and how would you design a solution using Go, Kubernetes, and Prometheus to track and report on cost allocation?",
        "scores": {
          "relevance": 9,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core role skills (cost attribution, team allocation) and matches tech stack (Go, K8s, Prometheus for cloud resource tracking); minor ding for not explicitly mentioning tagging/chargebacks. Clarity: Clear intent and well-scoped for screening, but lacks specific constraints like scale or time. Discriminative: Excellent at probing challenges (accuracy, tagging issues) and design trade-offs (e.g., Prometheus metrics for costs, K8s integration), separating strong systems thinkers from weak ones.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-479",
        "role_title": "Principal Engineer - Data Architecture",
        "question": "\"Design a data architecture for a company that processes 10 million transactions per day and runs daily analytics on 50 million rows of data. The system must support low-latency transactional operations (sub-second response time) and real-time dashboards with a 10-second latency requirement. The solution must be scalable, resilient, and maintain data consistency across all workloads. Suggest the component structure, data flow, and technologies for each component. Explain how you would ensure data consistency between transactional and analytical workloads, and how you would handle data freshness and latency requirements. Be specific about how you would balance OLTP and OLAP workloads. Also, describe how you would monitor and validate data quality across the system.\"",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core data architecture strategy for mixed OLTP/OLAP workloads, scalability, tech selection, and team guidance as per role; no red flags. Clarity: Well-scoped with specific volumes, latencies, and requirements; minor deduction for lacking explicit time/format constraints. Discriminative: Probes deep trade-offs in consistency, freshness, balancing workloads, monitoring\u2014strong separation of strong vs weak candidates; not trivial recall or overly broad.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-480",
        "role_title": "Junior Backend Engineer - Deployment Automation",
        "question": "*You're deploying a Python application to AWS using Docker and Ansible. The app has a config file `app.conf` that must be copied to the server with the correct ownership and permissions. How would you ensure the config file is correctly copied, set with 644 permissions, and owned by the `appuser` user? Provide a relevant Ansible task and explain how it ensures correctness.*",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests Ansible for config management and deployment scripting, core to role (-0 red flags). Clarity: Precise scope, unambiguous task with exact requirements. Discriminative: Requires applied Ansible knowledge (module usage, params), explanation tests understanding of idempotency/permissions, strong separation without being trivial or broad.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-481",
        "role_title": "Backend Engineer - Canary Deployments",
        "question": "\"Design a canary deployment strategy using Istio and ArgoCD to promote a new version of a Go service with automated rollback based on traffic and metrics. Explain how you would ensure reliability during the rollout, including traffic shifting, monitoring, and rollback triggers.\"",
        "scores": {
          "relevance": 9,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core role skills (canary deployments, traffic shifting, metrics, rollback) with Istio/ArgoCD fitting infra focus; minor deduct for assuming specific tools not explicitly listed (-1). Clarity: Well-scoped design question but lacks explicit constraints like time or metrics thresholds (-1, -1 red flag). Discriminative: Excellent at testing applied knowledge, trade-offs in reliability, monitoring, and triggers; strong separation of strong vs weak candidates.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-482",
        "role_title": "Senior Backend Engineer - API Monetization",
        "question": "\"Design a system to track API usage and enforce quotas for a multi-tenant SaaS platform using Go, PostgreSQL, and Redis. Your solution must support:\n- Per-tenant, per-API endpoint usage tracking\n- Real-time quota enforcement (e.g., 1000 requests per day, 100 requests per minute)\n- Rate limiting at both the endpoint and tenant level\n- Cost calculation based on usage (e.g., tiered pricing: free tier 10,000 requests/day, paid tier 100,000 requests/day)",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (usage tracking, quotas, pricing tiers) with exact tech stack (Go, PostgreSQL, Redis). No red flags. Clarity: Crystal clear with specific requirements, but lacks minor constraints like time limit (-1). Discriminative: Tests applied system design, trade-offs (real-time enforcement, multi-level limiting), and deep understanding; strong separation without being too broad.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-483",
        "role_title": "Backend Engineer - WAF Integration",
        "question": "How would you implement a custom ModSecurity rule in Go to detect and block a specific attack pattern (e.g., SQL injection) in an Nginx-based web application, including the handling of rate limiting and logging to Redis?",
        "scores": {
          "relevance": 7,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Tests core WAF rule management, attack pattern detection (SQLi), rate limiting, and logging\u2014directly matches role focus, but Go/Nginx/ModSecurity not explicitly listed (-2 for unlisted tech, -1 generic elements). Clarity: Well-scoped with specific example and components, minor ambiguity on Nginx-Go integration. Discriminative: Requires applied knowledge of rules, integration, rate limiting trade-offs, and Redis\u2014strong separation of strong vs weak candidates.",
          "composite": 8.0
        }
      },
      {
        "role_id": "be-484",
        "role_title": "Staff Engineer - Model Registry",
        "question": "\"Design a model registry system using MLflow and S3 for storing model versions, with automatic versioning and metadata tracking. How would you implement a solution that ensures each model version is uniquely identifiable, tracks metadata (e.g., model parameters, training date, environment), and handles version conflicts when multiple teams are developing and deploying models concurrently?\"",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Directly targets core role skills (model registry, versioning, governance) using specified tech (MLflow, S3). Crystal clear with precise scope, tests applied design knowledge, trade-offs (e.g., conflicts), and multi-team concurrency. Minor ding for no explicit time constraint, but well-scoped overall.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-485",
        "role_title": "Junior Backend Engineer - Payment Webhooks",
        "question": "How would you design a Python FastAPI endpoint to safely process a Stripe payment webhook? Include considerations for signature verification, idempotency, and logging.",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Directly tests core role skills (signature verification, idempotency, event processing) with exact tech stack (Python FastAPI, Stripe). Crystal clear and well-scoped for screening, though minor deduction for no explicit time constraint. Strong trade-off testing (safety considerations) differentiates strong entry-level candidates.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-486",
        "role_title": "Backend Engineer - Span Processing",
        "question": "\"Design a tail sampling strategy for a distributed tracing pipeline using OpenTelemetry and Kafka. The pipeline ingests spans from distributed services, and you need to ensure that only a small fraction (e.g., 0.1%) of the total spans are stored in ClickHouse for long-term analysis. Your design must account for high-volume, high-velocity span data and must minimize data drift and ensure observability of low-frequency events. Consider how to implement this in Go using OpenTelemetry SDK, how to configure sampling decisions at the span level, and how to handle the tail sampling logic in a scalable, fault-tolerant way across a distributed system. Explain your approach to sampling decisions, the trade-offs, and how you would ensure data completeness for rare events.\"",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Relevance: Directly targets core role skill (tail sampling in span pipeline) with exact tech (OpenTelemetry, Kafka, ClickHouse, Go); minor ding as ClickHouse storage not explicitly core but fits optimization. Clarity: Precise scope, constraints (0.1%, high-volume), unambiguous despite length. Discriminative: Excellent at probing deep trade-offs, scalability, fault-tolerance, and rare event handling, strongly separating experts.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-487",
        "role_title": "Senior Backend Engineer - Search Ranking",
        "question": "\"Design a feature pipeline for a learning-to-rank model that ranks search results by relevance. The pipeline should extract features from raw search queries and documents, compute relevance scores, and store them in Elasticsearch for ranking. Describe how you would structure the pipeline, including data sources, feature extraction methods, how you would handle scalability, and how you would ensure feature consistency and quality.\"",
        "scores": {
          "relevance": 9,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core role skills (feature engineering, LTR pipelines) with minor ding for specifying Elasticsearch (-1, not in role desc). Clarity: Well-formed and scoped but lacks explicit constraints like time/depth (-1, 8 range) or A/B testing mention. Discriminative: Excellent at probing applied ML engineering, trade-offs (scalability, quality), distinguishes deep thinkers from superficial ones.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-488",
        "role_title": "Backend Engineer - Gaming Backend",
        "question": "\"Design a state synchronization mechanism for a multiplayer game where players are in different rooms (e.g., room A has 2 players, room B has 3 players). When a player performs an action (e.g., move or shoot), that action must be immediately reflected in the client's view, but only for players in the same room. How would you implement this using Go, WebSocket, and Redis? Include a consideration for handling network latency and disconnections.\"",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core state synchronization for multiplayer game backend with exact tech stack (Go, WebSocket, Redis); minor deduction as role also emphasizes matchmaking/anti-cheat. Clarity: Crystal clear scope (room-based, immediate client reflection, specific tech), well-constrained for screening. Discriminative: Requires applied knowledge of real-time systems, trade-offs in latency/disconnections, strong separation of strong vs weak candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-489",
        "role_title": "Backend Engineer - Data Quality",
        "question": "\"Describe how you would implement a data freshness check for a critical table in a Snowflake warehouse using dbt and Great Expectations. Include how you would define the freshness expectation, set up a scheduled check, and ensure it triggers a warning if the table is not updated within a specified time window.\"",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core data quality monitoring (freshness checks) in Snowflake warehouse with exact tools (dbt, Great Expectations) from role. No red flags. Clarity: Precise, well-scoped steps; minor vagueness on scheduling details but excellent overall. Discriminative: Requires applied knowledge of tool integration, configuration, and alerting logic for strong candidate separation; tests trade-offs implicitly (e.g., time windows).",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-490",
        "role_title": "Senior Backend Engineer - Environment Management",
        "question": "\"Design a system to manage preview environments for a Kubernetes cluster using ArgoCD and Terraform. The system must ensure namespace isolation, enforce resource quotas, and automatically clean up environments that are no longer in use after a TTL. Describe how you would implement this using Go, Kubernetes, ArgoCD, and Terraform. Include a high-level design of your solution, and explain how each component contributes to achieving the isolation, quota enforcement, and cleanup requirements.\"",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core requirements (namespace isolation, quotas, TTL cleanup) and specifies exact tech stack (Kubernetes, ArgoCD, Terraform, Go). No red flags.\nClarity: Crystal clear, well-scoped high-level design with specific requirements and components; minor deduction for lacking explicit time/format constraints.\nDiscriminative: Tests deep applied knowledge, trade-offs in system design, and integration of multiple tools, strongly separating strong candidates.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-491",
        "role_title": "Junior Backend Engineer - Read Replicas",
        "question": "*You're building a read replica router that routes read queries to the appropriate replica based on a query's \"readiness\" score. The readiness score is calculated as:*",
        "scores": {
          "relevance": 2,
          "clarity": 3,
          "discriminative": 4,
          "reasoning": "Relevance: Directly mentions read replica routing (core to role) but no role description provided to confirm tech stack; feels generic/incomplete (-3 generic). Clarity: Incomplete question cuts off mid-sentence, leaving formula undefined and no actual ask (multiple interpretations, broken). Discriminative: Partial setup hints at lag/trade-offs but no task specified, so trivial or impossible to answer properly.",
          "composite": 3.0
        }
      },
      {
        "role_id": "be-492",
        "role_title": "Backend Engineer - CD Pipelines",
        "question": "\"Design a GitOps-based CI/CD pipeline using ArgoCD and GitHub Actions that deploys a Go application to staging and production environments. Include how you would implement multi-environment promotion, approval gates, and rollback automation. Explain your architecture, decision points, and how you\u2019d ensure security and reliability.\"",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core focus on GitOps pipelines, multi-env promotion, approvals, rollbacks; directly tests required skills with relevant tools (ArgoCD aligns with GitOps, GitHub Actions common for CI/CD). No red flags. Clarity: Well-scoped design question with specific components listed; minor deduction as no explicit time/format constraints but excellent for screening. Discriminative: Requires architecture, trade-offs (security/reliability decisions), applied GitOps knowledge; strong separation of strong vs weak candidates. Minor deduction for potential breadth exceeding 10min if overly verbose.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-493",
        "role_title": "Staff Engineer - Reliability Standards",
        "question": "\"Imagine you're leading a production readiness review (PRR) for a new Kubernetes cluster being deployed to production. The team has proposed a 99.99% uptime SLA for a critical microservice. How would you define the reliability standards for this service, including measurable metrics, failure modes, and incident response requirements? Provide a concrete example of how you would evaluate a candidate's proposed PRR checklist, including at least one metric you'd validate and one failure mode you'd investigate.\"",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role responsibilities (reliability standards, PRR, SRE practices) with Kubernetes context fitting production deployment; no red flags. Clarity: Well-scoped scenario with specific asks (metrics, failure modes, response), minor deduction for lacking explicit time/format constraints. Discriminative: Requires applied SRE knowledge, trade-offs in SLAs/metrics, and evaluation skills, strongly separating experienced SREs from juniors; not mere recall.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-494",
        "role_title": "Backend Engineer - API Mocking",
        "question": "Explain how you would design a WireMock setup to mock a user authentication API that returns different responses based on the request headers, including:\n- A 200 OK response with a valid JWT token for authenticated users (header: `X-User-Id: 123`)\n- A 401 Unauthorized response for unauthenticated users (no `X-User-Id` header)\n- A 403 Forbidden response for users with invalid IDs (header: `X-User-Id: 999`)\n- Support dynamic response templating using a JSON template that includes the user ID in the response body",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests WireMock (core for API mocking), request matching via headers, response templating, and conditional responses - matches role's focus perfectly (-0 red flags). Clarity: Precise scenarios, specific headers/responses, well-scoped for screening, unambiguous. Discriminative: Requires applied WireMock knowledge (matchers, transformers, templating), trade-offs in setup, distinguishes practical expertise from theory.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-495",
        "role_title": "Senior Backend Engineer - Audit Logging",
        "question": "How would you design a tamper-proof audit logging system using Go, Kafka, and ClickHouse, ensuring that log entries are immutable, securely stored, and can be queried efficiently for compliance purposes?",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (tamper-proof storage, immutability, querying for compliance) using exact tech stack (Go, Kafka, ClickHouse). Clarity: Crystal clear intent and scope for screening, minor deduction for lacking explicit time/format constraints. Discriminative: Requires deep trade-offs in immutability, distributed systems, and tech integration, strongly separating strong candidates.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-496",
        "role_title": "Junior Backend Engineer - Model Serving",
        "question": "*You are deploying a simple ML model via FastAPI. The model is loaded from a `.pkl` file and served via a `/predict` endpoint. Describe how you would structure the FastAPI app to handle model loading, request batching, and basic monitoring (e.g., request count and latency). Include a brief explanation of how batching improves performance and why you might use Redis for request queuing in a production environment.*",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Directly tests core role skills (model loading from .pkl, FastAPI deployment, batching, monitoring) for entry-level ML API engineer. Crystal clear with specific components and scope; asks for applied structure + explanations of trade-offs (batching benefits, Redis use), distinguishing strong candidates. Minor ding for assuming Redis familiarity, but fits production context well.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-497",
        "role_title": "Backend Engineer - Payout Systems",
        "question": "\"Design a system to process daily payout batches for a marketplace with 10,000+ merchants. Each merchant has a daily payout amount, and payouts are sent to different banks via a secure, auditable channel. The system must handle failures, ensure data consistency, and provide reconciliation logs. Describe your approach, including data flow, error handling, and how you'd ensure auditability and consistency.\"",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core focus on payout systems, batch processing, bank handling, validation, and reconciliation; no red flags. Clarity: Crystal clear with specific scale (10k+ merchants), requirements, and scoped to approach/data flow/error handling; minor deduction for lacking exact time/format constraints. Discriminative: Tests applied system design, trade-offs in failures/consistency/auditability, strongly separates candidates with real-world understanding from superficial ones; not trivial or overly broad.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-498",
        "role_title": "Senior Backend Engineer - Profiling Infrastructure",
        "question": "\"Describe how you would design a continuous profiling system using Pyroscope for Go applications running in Kubernetes, including how you'd collect, store, and analyze flame graphs. Specifically, profile a Go service that runs in a Kubernetes pod, and explain how you'd handle baseline comparisons to detect performance regressions over time.\"",
        "scores": {
          "relevance": 9,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core skills (continuous profiling, flame graphs, baseline comparison) and matches role focus; minor ding for specifying Pyroscope (not in role desc, -1) but it's profiling infra. Clarity: Well-scoped with specifics but lacks minor constraints like scale (-1 from 9). Discriminative: Requires applied design knowledge, trade-offs in K8s/Pyroscope/Go, strong separation of strong vs weak candidates.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-499",
        "role_title": "Principal Engineer - Search Platform",
        "question": "*You have a search platform running on Elasticsearch, which is heavily used across an enterprise. Recently, a new team started using the platform, and their queries are causing a significant increase in latency and resource consumption. After reviewing the logs and metrics, you find that the new team uses a large number of wildcard queries (e.g., `*field*`, `*value*`) and complex boolean combinations. These queries are not being cached and are hitting the cluster frequently. How would you diagnose and resolve this issue to improve performance without compromising the platform\u2019s reliability or stability?*",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core Elasticsearch/search platform optimization skills critical for defining enterprise strategy and operational excellence (-0 red flags). Clarity: Crystal clear scenario with specific issues, well-scoped for screening (no red flags). Discriminative: Requires deep applied knowledge of ES query pitfalls, caching, diagnosis (logs/metrics), and trade-offs (e.g., ngrams vs wildcards, query rewriting), strongly separating experts from juniors.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-500",
        "role_title": "Staff Engineer - Streaming Architecture",
        "question": "\"Design a real-time event streaming pipeline that delivers user activity events to multiple downstream services with exactly-once semantics and low latency. The system must handle up to 100k events per second. Describe your architecture, the tools you\u2019d use, and how you\u2019d ensure exactly-once delivery. Be specific about how you\u2019d handle fan-out and message ordering. Also, exactly-once delivery must be achieved at the system level, not just via a single component.\"",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's focus on streaming infrastructure, event schemas (implied), delivery guarantees (exactly-once), and fan-out patterns; no red flags. Clarity: Crystal clear and well-scoped with throughput and specifics, minor deduction for no explicit time constraint. Discriminative: Tests deep applied knowledge, trade-offs in tools/architecture, fan-out, and system-level guarantees, strongly separates candidates; not trivial or recall-based.",
          "composite": 9.333333333333334
        }
      }
    ]
  },
  "grok-4-1": {
    "model": "grok-4-1",
    "num_samples": 100,
    "avg_relevance": 9.16,
    "avg_clarity": 9.06,
    "avg_discriminative": 9.02,
    "avg_composite": 9.080000000000002,
    "results": [
      {
        "role_id": "be-406",
        "role_title": "Senior Backend Engineer - LLM Serving",
        "question": "**Question:**  \nIn vLLM, explain how continuous batching works for LLM inference and describe one optimization you've implemented (or would implement) for KV cache management in a production serving setup using Python and vLLM. (2-3 sentences)",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Directly tests core role skills (batching optimization, KV cache management) using vLLM, which aligns perfectly with LLM serving infrastructure; clear scoping (2-3 sentences, explain + one optimization) avoids ambiguity; strong separation via applied knowledge requiring trade-offs/experience, not just recall, while personal implementation tests depth.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-403",
        "role_title": "Backend Engineer - Inference Optimization",
        "question": "**Question:**  \nYou're optimizing a latency-sensitive transformer model for inference using TensorRT on a single GPU. The model experiences high tail latency due to variable input sizes. Describe a step-by-step approach (including code snippets if relevant) to implement dynamic batching and INT8 quantization in TensorRT to reduce p99 latency by at least 50%, while ensuring GPU memory usage stays under 80%. What trade-offs would you monitor?",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Relevance: Directly targets core role skills (quantization, batching, GPU memory) with TensorRT for transformer inference; minor ding as TensorRT not explicitly listed but fits optimization focus (-1). Clarity: Precise scope, constraints, and expectations; no bundling or vagueness. Discriminative: Requires deep applied knowledge, trade-offs, and practical steps, strongly separating experts from pretenders.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-404",
        "role_title": "Senior Backend Engineer - Feature Platform",
        "question": "**Question:**  \nImagine you're building a feature platform using Feast and need to compute a user's average transaction amount over the last 7 days (with a 1-day lookback window) for real-time ML serving via Redis. Walk me through how you'd define the feature view in Feast, including the entities, aggregation, and online serving setup in Python. What key considerations would you have for latency and data freshness?",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests Feast (core to feature platforms), feature computation, online serving (Redis), and discovery\u2014perfect match for role (-0 red flags). Clarity: Precise scope, unambiguous, well-formed with specific params, no bundling or vagueness. Discriminative: Requires applied Feast knowledge (feature view definition, entities, agg, serving), plus trade-offs (latency/freshness), distinguishing strong ML platform engineers from weak ones.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-408",
        "role_title": "Junior Backend Developer - Health Endpoints",
        "question": "**Question:**  \nIn Go, implement a simple HTTP handler for a `/healthz` liveness probe endpoint that returns a 200 OK status if the application is healthy (e.g., a database connection is available) or a 500 status if not. Assume a global `dbHealthy` bool variable tracks the DB status (set it to `true` initially). Use the `net/http` package and keep it under 20 lines.\n\n*(Follow-up: How would you expose this endpoint's response time as a Prometheus metric?)*",
        "scores": {
          "relevance": 9,
          "clarity": 10,
          "discriminative": 9,
          "reasoning": "Directly tests core role skills (health checks/liveness probes in Go) with exact endpoint and assumptions. Crystal clear with line limit and stdlib constraint. Applied coding + follow-up on metrics tests trade-offs (strong separation). Minor ding on relevance for assuming global var (not idiomatic), but spot-on for entry-level role focus.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-401",
        "role_title": "Backend Engineer - Training Infrastructure",
        "question": "**Question:**  \nIn a Kubernetes cluster with multiple GPU nodes, you're using Ray to launch a distributed PyTorch training job for a large language model. The job needs 4 GPUs across 2 nodes but fails with a scheduling error. Walk me through 3 practical steps you'd take to debug and resolve GPU allocation issues in this setup (using Python/Ray/K8s commands or configs). \n\n*(Expected: Quick mentions like `ray status`, `kubectl describe pod`, checking RayCluster CRD GPU requests/limits, node taints/labels, or `nvidia-smi` via debug pod.)*",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core skills in distributed ML infra (K8s, Ray, PyTorch, GPU allocation/scheduling) matching role; no red flags (-1 for minor tech like nvidia-smi but central to GPU issues). Clarity: Precise scenario, specific ask for 3 steps with command examples, well-scoped; expected answer clarifies without ambiguity. Discriminative: Requires applied debugging knowledge and trade-offs in real Ray/K8s GPU setup, distinguishes practitioners from theorists; not recall or trivial.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-402",
        "role_title": "Backend Engineer - Model Deployment",
        "question": "**Question:**  \nYou're deploying a new version of a machine learning model to production using KServe on Kubernetes. Describe a step-by-step Python-based approach (using KServe SDK or kubectl) to perform a canary deployment with 10% traffic initially, monitor for errors, and automate a full rollout or rollback if latency exceeds 200ms or error rate >5%. Include key code snippets or commands.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core model deployment, Kubernetes, canary strategies, monitoring, and automation (-1 for no explicit KServe mention in role, but perfect fit otherwise). Clarity: Crystal clear, well-scoped steps with specific metrics, no bundling or ambiguity. Discriminative: Requires applied knowledge of KServe/K8s, Python automation, monitoring integration, and trade-offs in rollout/rollback, strongly separates skilled candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-410",
        "role_title": "Backend Engineer - Bulk Operations",
        "question": "**Question:**  \nYou're building a Go API endpoint that accepts a bulk request to update up to 1,000 user records in PostgreSQL (e.g., via a JSON array of user IDs and updates). The operation must be idempotent, support partial success (e.g., some updates succeed, others fail due to constraints), and use Kafka for async processing of successes/failures.\n\nDescribe your high-level approach in Go, including:  \n1. How you'd ensure idempotency.  \n2. How you'd handle batching/partial failures in PostgreSQL.  \n3. The Kafka integration flow.  \n\nSketch a simple code outline or pseudocode for the handler if it helps explain. (Answer in 2-5 minutes.)",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core role needs (bulk ops, idempotency, partial success) with exact tech stack (Go, PostgreSQL, Kafka, JSON arrays); minor -1 for assuming Kafka scale not explicit. Clarity: Precise scope (up to 1k, specific subpoints, 2-5 min limit, pseudocode optional). Discriminative: Requires applied trade-offs (idempotency patterns, batch SQL strategies, Kafka flows), separates strong candidates via reasoning depth vs rote answers.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-409",
        "role_title": "Junior Backend Engineer - File Upload",
        "question": "**Question:**\n\nIn a FastAPI endpoint for file uploads, how would you implement basic validation to ensure only image files (under 5MB) are accepted? Provide a short code snippet showing the endpoint, including file reading, size/content-type checks, and a clear error response if invalid.\n\n*(Expected: Use `UploadFile`, check `content_type` starts with 'image/', read bytes to verify size < 5*1024*1024, return 400 error if invalid. Tests practical FastAPI file handling at junior level.)*",
        "scores": {
          "relevance": 9,
          "clarity": 10,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core file upload validation and FastAPI (matches role's multipart uploads, validation focus); minor deduction as role doesn't explicitly list FastAPI (-1). Clarity: Crystal clear with specific requirements, expected checks, and scoped for short snippet. Discriminative: Requires practical applied knowledge of UploadFile, content_type/size checks, error handling\u2014distinguishes juniors who can code it from those who can't; not mere recall.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-411",
        "role_title": "Backend Engineer - Admin APIs",
        "question": "**Question:**  \nYou're building a FastAPI endpoint for an admin to delete a user account, which requires privileged access and must log the action for audit purposes. The endpoint uses PostgreSQL for user data and Redis for session caching.  \n\nOutline the key steps and code structure (in Python/FastAPI) to:  \n1. Authenticate and authorize the admin (e.g., via JWT or API key).  \n2. Perform the deletion atomically.  \n3. Log the audit trail to PostgreSQL.  \n4. Invalidate any related Redis cache.  \n\nProvide a high-level code snippet focusing on dependencies, security checks, and error handling. (No need for full implementation.)",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (admin APIs, privileged access, audit logging) with exact tech stack (FastAPI, PostgreSQL, Redis); minor ding for assuming JWT/API key. Clarity: Crystal clear, well-scoped with numbered steps, high-level snippet request, no ambiguity. Discriminative: Requires applied knowledge of auth, transactions, auditing, caching invalidation, and error handling trade-offs, strongly separates strong candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-414",
        "role_title": "Senior Backend Engineer - Long-Polling",
        "question": "**Question:**  \nIn a Go-based long-polling API for async operations (using Redis for state storage and PostgreSQL for persistence), describe how you'd implement connection management and timeout handling for a `/poll/{operationId}` endpoint. Include key Go code snippets for the handler, a timeout mechanism (e.g., context with deadline), and a fallback strategy if the connection hangs. What Redis operations would you use to check operation status?",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core focus on long-polling APIs, connection management, timeout handling, fallback strategies, with exact tech stack (Go, Redis, PostgreSQL). No red flags. Clarity: Crystal clear, well-scoped for screening with specific endpoint and examples; minor deduction for lacking explicit time/format constraints. Discriminative: Tests applied deep knowledge, trade-offs (timeouts, fallbacks), code snippets, and Redis ops, strongly separating strong candidates; not trivial recall or too broad.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-405",
        "role_title": "Senior Backend Engineer - Model Monitoring",
        "question": "**Question:**  \nIn a production ML system, you've detected data drift in one of your models using a statistical test like Kolmogorov-Smirnov. Walk me through how you'd implement an automated alerting pipeline for this using Python, Prometheus, and Grafana. What key metrics would you expose, and how would you configure the alert to avoid false positives? (Feel free to sketch pseudocode or a high-level architecture.)",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly targets core role responsibilities (drift detection, alerting automation) with exact tech stack implied for production ML monitoring. Clarity: Crystal clear, well-scoped with specific tools and asks for architecture/pseudocode; minor deduction for no strict time constraint. Discriminative: Requires applied knowledge of drift handling, metrics, alerting config, and false positive mitigation\u2014tests trade-offs and integration deeply, strong separation.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-416",
        "role_title": "Staff Backend Engineer - External APIs",
        "question": "**Question:**  \nAs a Staff Backend Engineer leading our external API strategy, imagine we're designing a monetized API for partners using Go, OpenAPI specs, and Kong as the gateway on Kubernetes. Walk me through your approach to implementing rate limiting that supports multiple partner tiers (e.g., free, pro, enterprise) while ensuring fair usage and scalability. What key OpenAPI annotations, Kong plugins, and Go middleware patterns would you use, and why?",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core role responsibilities (API strategy, partner enablement, monetization) with exact tech stack (Go, OpenAPI, Kong, Kubernetes); no red flags. Clarity: Crystal clear scenario, well-scoped to rate limiting with specific tech; minor deduction for lacking explicit time constraint. Discriminative: Tests applied knowledge, trade-offs (fairness, scalability, tiers), and integration of multiple tools, strongly separating strong candidates who can reason through patterns from weak ones.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-415",
        "role_title": "Senior Backend Engineer - Multipart APIs",
        "question": "**Question:**  \nIn FastAPI, describe how you would implement a resumable multipart upload endpoint for large files to S3, including how to handle chunked requests, track upload progress/state (e.g., via a database or Redis), and resume interrupted uploads. Provide a high-level code structure with key dependencies like `boto3` and mention any relevant S3 multipart upload APIs you'd use (e.g., `create_multipart_upload`). What challenges might arise with concurrent chunk uploads, and how would you address them?",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core focus on multipart APIs, resumable uploads, chunk management, and S3 integration with exact tech stack (FastAPI, boto3). No red flags. Clarity: Crystal clear, well-scoped for screening with specific components requested; minor deduction for no explicit time constraint. Discriminative: Tests deep applied knowledge, trade-offs (concurrency challenges), and architecture decisions, strongly separating strong candidates.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-413",
        "role_title": "Senior Backend Engineer - API Gateway Logic",
        "question": "**Question:**  \nIn Envoy, you're building a custom Lua filter for an API gateway to transform incoming requests by adding a custom `x-user-id` header based on a JWT token in the `Authorization` header. The JWT is signed with RS256 and contains a `sub` claim.\n\nProvide a concise Lua script snippet (10-20 lines) for the `onRequest` phase that:  \n1. Extracts and validates the JWT (basic signature check not required, just decode).  \n2. Pulls the `sub` claim.  \n3. Adds it as `x-user-id` header if valid, or returns a 401 response if invalid/missing.  \n\nWhat headers would you inspect or set, and how do you handle errors? (Answer in code + 1-2 sentences explanation.)",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests Envoy Lua filters, JWT handling, and request transformation\u2014core to API gateway extensions (-1 minor for not explicitly listing Envoy/Lua/JWT but perfectly matches role). Clarity: Crystal clear scope, line limit, specific requirements, and format. Discriminative: Requires applied Lua/Envoy/JWT knowledge with error handling and trade-offs, distinguishing strong candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-407",
        "role_title": "Staff Backend Engineer - ML Infrastructure",
        "question": "**Question:**  \nYou're leading the migration of our distributed ML training jobs from Kubernetes-native setups to Ray on Kubernetes for better scalability. Walk me through your 3-step plan to ensure a smooth rollout, including how you'd handle cost optimization and one key failure mode you'd monitor for.\n\n*(Expected answer outline: 1) Pilot with shadow traffic/canary deploys; 2) Tune Ray cluster autoscaling + spot instances for cost; 3) Monitor for node failures via Ray dashboard/ Prometheus. Probes leadership in ML infra platforms.)*",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core ML infra leadership skills (Ray/K8s migration, scaling, cost opt), matches role perfectly (-0 red flags). Clarity: Unambiguous 3-step structure with specific inclusions, well-scoped for screening. Discriminative: Requires applied trade-offs (pilot strategy, cost tuning, failure monitoring), separates leaders with Ray/ML infra depth from generalists.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-417",
        "role_title": "Junior Backend Developer - Event Sourcing",
        "question": "**Question:**\n\nIn an event-sourced system using Kafka and Java, you have an `Order` aggregate. An `OrderCreated` event is published to a Kafka topic. \n\nWrite a short Java code snippet (5-10 lines) for a simple **event handler** that loads the current aggregate state from a repository, applies the `OrderCreated` event to build the initial state (e.g., set order ID and status to \"PENDING\"), and saves the updated state back to PostgreSQL.\n\n*(No need for full Kafka consumer setup or DB connection details\u2014just focus on the handler logic.)*\n\n**Expected Answer Outline (for recruiter reference):**  \n```java\npublic class OrderEventHandler {\n    private OrderRepository repo;\n\n    public void handle(OrderCreated event) {\n        Order order = repo.load(event.getOrderId()).orElse(new Order()); // Load or init\n        order.apply(event); // e.g., order.setId(event.getOrderId()); order.setStatus(\"PENDING\");\n        repo.save(order);\n    }\n}\n```\n*(Tests: event application to aggregate, basic repo interaction, event sourcing flow\u2014junior level, no complex projections/distribution needed.)*",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly targets event sourcing core (event handlers, aggregate state, repo interaction) for entry-level role; uses listed tech (Java, Kafka, PostgreSQL) without straying. Clarity: Crystal clear scope (5-10 lines, handler logic only), unambiguous instructions. Discriminative: Requires applied understanding of event sourcing flow and basic Java patterns, distinguishes juniors who grasp aggregates from those stuck on CRUD; not trivial recall. Minor subtract avoided as single focused task.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-412",
        "role_title": "Backend Engineer - Analytics APIs",
        "question": "**Question:**  \nYou're building a Go-based analytics API endpoint that aggregates daily active users (DAU) from a ClickHouse table over the past 30 days, grouped by date. The query is expensive and runs frequently for dashboard refreshes.  \n\n1. Sketch a simple Go handler function outline (using Gin or net/http) that executes this aggregation query.  \n2. Describe **two caching strategies** you'd implement with Redis to optimize it (e.g., cache keys, TTLs, invalidation), and when you'd use each.  \n\n*(Expect 2-5 min: pseudocode + brief explanation)*",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests Go API development, ClickHouse aggregation queries, time-series DAU, and caching\u2014core to role (-1 minor for not explicitly listing Gin/Redis, but highly aligned). Clarity: Precise scope, parts clearly separated, time-constrained, unambiguous. Discriminative: Requires applied Go pseudocode + trade-off reasoning on caching strategies (keys/TTL/invalidation), separates strong candidates via depth and practicality.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-420",
        "role_title": "Backend Engineer - Graceful Shutdown",
        "question": "**Question:**  \nIn a Go gRPC server running on Kubernetes, explain how you would implement graceful shutdown to handle SIGTERM signals, drain existing connections, and complete in-flight requests without dropping them. Outline the key code changes (e.g., context management, HTTP server shutdown) and any relevant Kubernetes config (e.g., terminationGracePeriodSeconds).",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core focus on graceful shutdown, connection draining, in-flight requests, and signals in exact tech stack (Go, gRPC, Kubernetes). No red flags. Clarity: Crystal clear, well-scoped with specific elements (SIGTERM, code changes, K8s config), minor deduction for lacking explicit time constraint. Discriminative: Tests deep applied knowledge, trade-offs in context/signal handling, and cross-domain integration, strongly separates strong candidates.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-418",
        "role_title": "Junior Backend Engineer - Service Clients",
        "question": "**Question:**  \nIn Go, when implementing a gRPC service client, how would you add a simple retry mechanism for transient errors (e.g., network timeouts)? Provide a short code snippet showing the key parts using the standard gRPC retry options or a basic loop, and explain why you'd choose transient errors only.\n\n*(Expected answer: Use gRPC's built-in `grpc.WithBackoff` and `grpc.RetryPolicy` with `grpc.RetryPolicy{RetryableStatusCodes: []codes.Code{codes.ResourceExhausted, codes.Unavailable}}`, or a simple loop with `time.Sleep` backoff. Focus on not retrying idempotency-sensitive errors like `AlreadyExists`. Tests practical error handling knowledge for juniors.)*",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 8,
          "reasoning": "Relevance: Directly tests core error handling/retry skills for service clients in Go/gRPC per role (-1 minor for entry-level specificity). Clarity: Precise scope, expected depth clear, unambiguous. Discriminative: Requires applied knowledge of gRPC APIs, transient error reasoning, and trade-offs (idempotency), separates juniors with practical skills from trivia recall.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-419",
        "role_title": "Backend Engineer - Retry Logic",
        "question": "**Question:**\n\nYou're implementing retry logic in Go for a Kafka producer that sends critical events to a topic, backed by Redis for tracking retry budgets (max 5 retries per event ID) and PostgreSQL for final failure logging. Describe how you'd structure the retry function, including exponential backoff with jitter. Provide a concise Go code snippet (~20 lines) showing the key logic (e.g., budget check, backoff calc, sleep). What Go libraries would you use, and why?\n\n*(Expected: full jitter impl like `time.Duration(rand.Intn(max-jitter))`, redis.Get/Set for budget, time.Sleep; stdlib `math/rand` or `retry-go`; mid-level shows error classification, budget decrement.)*",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's retry infrastructure focus (backoff, jitter, budgets) with exact tech stack (Go, Kafka, Redis, PostgreSQL). No red flags. Clarity: Crystal clear, well-scoped with line limit, expected depth, and example hints; minor jargon but contextualized. Discriminative: Requires applied knowledge of backoff/jitter math, budget logic, error handling, and Go/Redis patterns; distinguishes mid/junior via code quality and trade-offs without being too broad.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-424",
        "role_title": "Backend Engineer - Message Serialization",
        "question": "**Question:**  \nYou're using Avro with Kafka and Schema Registry to serialize messages for a distributed system. A new version of your Avro schema adds an optional string field `userRole` (default: null) after an existing required field `userId`. Explain whether this change is backward and/or forward compatible, and why. If not fully compatible, what minimal adjustment would make it so?",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core schema evolution, backward/forward compatibility, and validation using Avro (implied in role's serialization infra). No red flags. Clarity: Crystal clear, well-scoped scenario with specific schema change; minor deduction as Avro/Kafka not explicitly in role desc but perfectly aligned. Discriminative: Requires deep understanding of compatibility rules, trade-offs (e.g., reader vs writer schemas), and practical fixes, strongly separating experts from juniors.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-422",
        "role_title": "Junior Backend Engineer - API Testing",
        "question": "**Question:**\n\nYou're writing pytest tests for a FastAPI endpoint that retrieves user data from a PostgreSQL database. The endpoint is defined as:\n\n```python\n@app.get(\"/users/{user_id}\")\ndef get_user(user_id: int, db: Session = Depends(get_db)):\n    return db.query(User).filter(User.id == user_id).first()\n```\n\nWrite a simple pytest test function to verify that the endpoint returns a 404 status code when the user ID doesn't exist in the database. Assume a test client and a test database fixture (`test_db`) are available.\n\n(Expect a concise response showing `TestClient`, a mock non-existent user, `assert response.status_code == 404`, and basic setup.)",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 8,
          "reasoning": "Relevance: Directly tests pytest + FastAPI + PostgreSQL (core stack for API tests role), with fixture usage; minor ding as contract testing not explicit (-1). Clarity: Crystal clear with code example, explicit assumptions, expected response format. Discriminative: Tests applied testing skills (setup, client usage, assertions) distinguishing entry-level competence; not trivial recall.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-423",
        "role_title": "Staff Engineer - Incident Response Platform",
        "question": "**Question:**  \nYou're building an incident response platform in Go that integrates with PagerDuty and Slack. Describe how you would design and implement a reliable retry mechanism for sending Slack notifications triggered by PagerDuty escalations, including handling duplicates, rate limits, and failures. What Go libraries or patterns would you use, and how would you ensure it doesn't exacerbate an ongoing incident?",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core incident response tooling skills with Go, PagerDuty, Slack integration, and reliability\u2014matches role focus perfectly (-0 red flags). Clarity: Well-scoped design question with specific constraints (duplicates, rate limits, failures, non-exacerbation); unambiguous for screening. Discriminative: Probes applied knowledge of retries, Go patterns (e.g., idempotency, backoff), trade-offs in failure handling\u2014strong separation of strong vs weak candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-421",
        "role_title": "Senior Backend Engineer - Security Automation",
        "question": "**Question:**  \nYou're automating secrets rotation for a Kubernetes cluster using HashiCorp Vault and Python. Walk me through how you'd implement a Python script to rotate a database password stored in Vault, update the corresponding Kubernetes Secret, and roll out the change to a Deployment without downtime. What key Vault APIs or auth methods would you use, and how would you handle errors like lease expiration?",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Relevance: Directly tests core secrets rotation in K8s/Vault/Python (-1 minor for not mentioning scanning/compliance). Clarity: Precise, well-scoped steps with specific tech. Discriminative: Requires deep applied knowledge of APIs, auth, K8s rolling updates, error handling, and trade-offs like zero-downtime.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-426",
        "role_title": "Junior Backend Engineer - DevOps Scripting",
        "question": "**Question:**\n\nYou're setting up a simple CI/CD pipeline in GitHub Actions to build and test a Python application. The workflow should:\n\n1. Trigger on push to `main`.\n2. Run on `ubuntu-latest`.\n3. Install Python 3.11 and dependencies from `requirements.txt`.\n4. Run `pytest` tests.\n\nWrite the YAML for `.github/workflows/ci.yml`. (No Docker needed.)\n\n*Expected time: 3-5 minutes. Look for correct structure, `uses:`, `run:`, and basic steps.*",
        "scores": {
          "relevance": 9,
          "clarity": 10,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests GitHub Actions YAML for CI/CD pipelines, core to DevOps automation role (-1 for not mentioning deployments despite role scope). Clarity: Crystal clear with precise requirements, triggers, runner, steps, and time estimate. Discriminative: Requires applied YAML knowledge of structure, 'uses', 'run', and Python setup, distinguishing those who can write working workflows from those who can't; 3-5 min scope perfect.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-427",
        "role_title": "Backend Engineer - Feature Store",
        "question": "**Question:**  \nIn Feast, describe the key steps to define a simple feature view for real-time serving of a user activity count feature (e.g., computed from Kafka events and stored in Redis). Include the main configuration elements you'd specify in the feature view YAML, and explain how you'd retrieve it online for inference in a Python service.\n\n*(Expected answer outline: FeatureView with entities (e.g., user_id), stream source (Kafka), online store (Redis), TTL, and features with value_type. Retrieval via feature_store.get_online_features() with entity rows.)*",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests Feast for feature store infrastructure, core to ML pipelines with versioning, real-time serving, Kafka, and Redis\u2014no red flags. Clarity: Precise, well-scoped with YAML elements and retrieval; minor vagueness on exact YAML syntax prevents 10. Discriminative: Requires applied Feast knowledge, configuration details, and API usage with trade-offs (e.g., TTL, stores), distinguishing strong candidates from weak ones.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-425",
        "role_title": "Senior Backend Engineer - Graph Analytics",
        "question": "**Question:**  \nYou're optimizing a Neo4j graph traversal query for a recommendation engine on a social network with 1 billion edges (users \u2192 follows \u2192 interests). The current Cypher query for \"find top-10 interest recommendations for a user\" is taking 30+ seconds due to high cardinality.  \n\n```\nMATCH (u:User {id: 123})-[:FOLLOWS*1..3]->(other:User)-[:LIKES]->(i:Interest)\nWITH i, count(DISTINCT other) as score\nRETURN i.name, score\nORDER BY score DESC\nLIMIT 10\n```\n\nWhat are 2-3 specific optimizations you'd apply (e.g., indexes, query rewrites, or config changes), and why would they improve performance at this scale?",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests Neo4j/Cypher optimization for graph traversal at billion-edge scale, core to role (-1 minor for assuming Neo4j not explicitly listed). Clarity: Crystal clear with specific query, scenario, and scoped ask for 2-3 optimizations. Discriminative: Requires applied knowledge of graph query tuning, indexes, cardinality issues, and scale trade-offs, separating strong candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-430",
        "role_title": "Junior Backend Engineer - Search Indexing",
        "question": "**Question:**  \nYou're building a Python script to bulk index 10,000 user profiles from PostgreSQL into Elasticsearch. The profiles have fields like `name`, `email`, and `bio`.  \n\n1. Write a simple Elasticsearch mapping for these fields (JSON).  \n2. Provide a Python code snippet using the `elasticsearch` library to connect, prepare a bulk action for 3 sample docs, and execute the indexing.  \n\n(Expect ~100-200 words of code/response; focus on correctness over optimization.)",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 8,
          "reasoning": "Relevance: Directly tests core role skills (bulk indexing, mapping design) with exact tech stack (Python, PostgreSQL, Elasticsearch); minor ding for not covering relevance tuning (-1). Clarity: Crystal clear with specific fields, sample size, library, and scoped expectations. Discriminative: Requires applied knowledge of mappings and bulk API, distinguishes those who can code correctly from theorists; not elite trade-offs but strong for entry-level.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-429",
        "role_title": "Backend Engineer - Log Aggregation",
        "question": "**Question:**  \nYou're building a log aggregation pipeline in Go using Vector for collection and ClickHouse for storage, processing millions of events per second on Kubernetes. Describe a practical approach to implement dynamic retention policies (e.g., delete logs older than 7 days for high-volume sources but 30 days for critical ones). Include key Go code snippets for the retention logic and any ClickHouse/Vector configurations you'd use. (2-5 min)",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core role skills (log pipelines, retention policies at scale) with exact tech stack (Go, Vector, ClickHouse, K8s); minor generic penalty avoided (-0). Clarity: Well-scoped (2-5 min, practical approach, specific elements), unambiguous with clear expectations. Discriminative: Requires applied knowledge of integration, trade-offs (dynamic policies at scale), and code/config, strongly separating experienced candidates; not mere recall.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-428",
        "role_title": "Senior Backend Engineer - Payment Orchestration",
        "question": "**Question:**  \nIn our payment orchestration system built with Go, Kafka, and PostgreSQL, we route transactions to multiple providers based on success rates, costs, and failover rules. Describe how you would implement a failover mechanism in Go that switches providers mid-transaction if the primary provider times out or fails (e.g., after 2 seconds), while ensuring the transaction state is reliably tracked across retries. Include key code snippets for the routing logic and any relevant Kafka/PostgreSQL interactions.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core payment orchestration skills (failover, routing, success tracking) with exact tech stack (Go, Kafka, PostgreSQL); minor deduction as role doesn't explicitly specify 'mid-transaction' switch. Clarity: Well-scoped with specific timeout example, clear expectations for code snippets and DB/Kafka interactions. Discriminative: Requires applied knowledge of Go concurrency, distributed state, retries, and trade-offs (idempotency, at-least-once), strongly separates strong candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-435",
        "role_title": "Junior Backend Engineer - Queue Consumers",
        "question": "**Question:**  \nYou're building a Python RabbitMQ consumer that processes order updates from a queue and saves them to PostgreSQL. The consumer might crash mid-processing, causing duplicates if restarted.  \n\nExplain in 2-3 sentences how you'd make it **idempotent** (safe to retry). What code pattern or DB technique would you use, and why?  \n\n*(Follow-up if time: Sketch a simple code snippet.)*",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests idempotency for message queue consumers (core role skill) with Python/RabbitMQ/PostgreSQL stack; minor deduction as role is entry-level but question fits learning focus (-1). Clarity: Crystal clear, scoped to 2-3 sentences with specific ask for pattern/technique/why; follow-up is optional. Discriminative: Requires applied understanding of trade-offs (e.g., DB unique constraints vs. app logic), distinguishes those grasping retry safety from rote memorizers.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-433",
        "role_title": "Senior Backend Engineer - Platform APIs",
        "question": "**Question:**  \nYou're designing a gRPC-based internal platform API in Go for a service catalog that allows developers to query and provision resources (e.g., databases or caches). The API needs to support pagination for large result sets and integrate with PostgreSQL for storage and Redis for caching.\n\nWalk us through how you'd structure the protobuf schema for the `ListServices` RPC, including pagination and caching strategy. What key gRPC and Go features would you use, and why? (e.g., streaming vs. unary, context handling).",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (API design, service catalog, provisioning) with exact tech stack (gRPC, Go, PostgreSQL, Redis); minor deduction as role doesn't explicitly list gRPC but fits internal platform APIs (-1). Clarity: Crystal clear, well-scoped to one RPC with specific constraints, no ambiguity. Discriminative: Probes deep applied knowledge of protobuf design, gRPC features (streaming vs unary), Go specifics, caching trade-offs, and context handling for strong candidate separation.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-434",
        "role_title": "Principal Engineer - Database Strategy",
        "question": "**Question:**  \nYou're leading the database strategy for a new multi-region application deployment using PostgreSQL as the primary database, with CockroachDB as a potential option for future scalability, Redis for caching, and Kafka for event streaming, all orchestrated on Kubernetes.  \n\nWalk me through your high-level approach to selecting between PostgreSQL (with replication) and CockroachDB for strong consistency across 3 regions, including 2-3 key trade-offs you'd evaluate and how you'd ensure operational resilience (e.g., failover, monitoring).  \n\n*(Expected response: 2-5 min verbal explanation demonstrating strategic trade-offs like latency vs. availability, cost, ops complexity, and practical Kubernetes integration.)*",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core database strategy, multi-region deployment, and tech selection (PostgreSQL, CockroachDB) from role; minor ding for unmentioned Redis/Kafka/K8s (-1). Clarity: Crystal clear, well-scoped with explicit trade-offs, resilience focus, and time expectation. Discriminative: Forces trade-off analysis (latency/availability/cost/ops) and applied Kubernetes knowledge, separating strategic thinkers from rote learners.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-432",
        "role_title": "Backend Engineer - Data Validation",
        "question": "**Question:**\n\nIn a Python-based data pipeline using Great Expectations and Airflow, you've set up an expectation suite that includes schema validation and a statistical check for row count drift (e.g., expecting row counts to stay within 10% of a baseline). During an Airflow DAG run, the task fails due to a Great Expectations validation error on the row count check.\n\nWalk me through **3 practical steps** you'd take to debug and resolve this issue, including specific Great Expectations or Airflow commands/tools you'd use, and explain why each step helps ensure pipeline quality.\n\n*(Expected response time: 2-4 minutes; follow-up: Ask for code snippet if they mention CLI commands.)*",
        "scores": {
          "relevance": 9,
          "clarity": 10,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (schema validation, statistical checks) with exact tech stack (Python, Great Expectations, Airflow); minor deduction as role mentions data lineage not covered (-1). Clarity: Crystal clear, well-scoped with 3 steps, specific tools, expected time, and follow-up. Discriminative: Requires applied debugging knowledge, trade-offs in pipeline quality, and specific commands; strong separation beyond recall.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-431",
        "role_title": "Staff Engineer - Real-time Collaboration",
        "question": "**Question:**  \nYou're designing a real-time collaborative text editor using WebSockets and CRDTs. A user applies a large text insertion (e.g., 10KB) while another user deletes a nearby character simultaneously. Walk me through how you'd ensure both operations are correctly merged without conflicts or data loss, including any trade-offs in your CRDT choice (e.g., Logoot vs. Yjs).",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Relevance: Directly tests core role skills (real-time collab, conflict resolution, operational transforms via CRDTs); minor ding as role mentions OT too but question focuses perfectly on CRDTs (-1). Clarity: Crystal clear scenario, well-scoped, specific examples, no ambiguity. Discriminative: Forces deep understanding of CRDT mechanics, concurrency, and trade-offs (Logoot vs Yjs), excellent separation of strong vs weak candidates.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-438",
        "role_title": "Staff Engineer - ML Serving Infrastructure",
        "question": "**Question:**  \nYou're designing a Triton Inference Server deployment on Kubernetes to serve a high-throughput computer vision model with sub-100ms p99 latency targets. The model uses dynamic batching. Walk me through your approach to configuring batching parameters (e.g., max_batch_size, preferred_batch_size) and any Kubernetes autoscaling integrations you'd use to handle traffic spikes, including one key Prometheus metric you'd monitor for optimization.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Directly targets core role skills (ML serving, batching, latency optimization) with Triton/K8s specifics highly relevant to infrastructure design; crystal clear with precise parameters, integrations, and metric scope; tests applied trade-offs in config and monitoring, distinguishing deep expertise from superficial knowledge. Minor ding for assuming Triton without role mention, but fits ML serving context perfectly.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-442",
        "role_title": "Backend Engineer - Event Streaming",
        "question": "**Question:**  \nIn our event streaming system built with Rust and NATS, we need to support fan-out of real-time updates to thousands of connected clients (e.g., via WebSocket). Describe a practical approach you'd take to efficiently fan-out a single high-volume event topic across NATS subjects, including how you'd handle filtering for client-specific subscriptions (e.g., by user ID or event type). What Rust crates or NATS features would you leverage, and how would you ensure scalability without overwhelming Redis or PostgreSQL?",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core role skills (event streaming, fan-out, filtering) with exact tech stack (Rust, NATS); minor ding for mentioning Redis/PostgreSQL not in role desc (-1). Clarity: Well-scoped, unambiguous, specific constraints provided. Discriminative: Requires applied knowledge of trade-offs, scalability, and specific tools, strongly separates strong candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-437",
        "role_title": "Senior Backend Engineer - Secrets Management",
        "question": "**Question:**  \nYou're implementing automated secret rotation for a service account used by a Go application running in Kubernetes, storing its credentials in HashiCorp Vault. The secret needs to be rotated every 24 hours, with the app fetching the latest version via Vault's API, and old versions revoked after successful update. Walk me through your approach in Go, including key code snippets for the rotation logic, handling Vault authentication, and ensuring zero-downtime during the update. What Vault features (e.g., policies, dynamic secrets) would you leverage, and how would you test this in a Kubernetes environment?",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's secrets management, rotation, Vault, with Go/K8s specifics (no red flags). Clarity: Crystal clear, well-scoped for screening, minor deduction for lacking explicit time/format constraints. Discriminative: Tests deep applied knowledge, trade-offs (zero-downtime), Vault features, and testing\u2014strong separation of strong vs weak candidates.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-440",
        "role_title": "Junior Backend Engineer - Metrics Collection",
        "question": "**Question:**  \nIn Go, using the Prometheus client library, write a short code snippet (5-10 lines) to create and increment a counter metric named `http_requests_total` that tracks the number of incoming HTTP requests, labeled by HTTP method (e.g., \"GET\", \"POST\"). Explain briefly (1-2 sentences) how you'd register and expose this metric for Prometheus scraping.\n\n*(Expected answer: Import `prometheus/client_golang/prometheus` and `prometheus/client_golang/prometheus/promhttp`; define var `httpRequests = prometheus.NewCounterVec(prometheus.CounterOpts{Name: \"http_requests_total\"}, []string{\"method\"});` register it with `prometheus.MustRegister(httpRequests);` then `httpRequests.WithLabelValues(\"GET\").Inc();` in handler; expose via `/metrics` handler with `promhttp.Handler().` Tests basic metric patterns, Go syntax, and observability setup.)*",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's focus on counter metrics/patterns in Go with Prometheus, core to entry-level observability; no red flags. Clarity: Crystal clear with line limit, specific labels, and scoped explanation; minor deduction for not specifying handler context explicitly. Discriminative: Requires applied knowledge of Prometheus client (not just recall), Go syntax, registration, and exposure, distinguishing candidates familiar with observability setup from novices.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-444",
        "role_title": "Backend Engineer - Service Templates",
        "question": "**Question:**  \nYou're setting up a Cookiecutter template for a new Go microservice that will deploy to Kubernetes via ArgoCD. Walk me through 3-5 key best practices you'd include in the template for security defaults and observability, and briefly explain how you'd implement one of them using Go or GitHub Actions.\n\n*(Expected response: Covers basics like non-root containers, network policies, structured logging with zap/zerolog, Prometheus metrics, GitHub Actions for scanning dependencies/secrets. Implementation example: Embedding OTEL exporter in Go main() or Actions lint/test workflow.)*",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (templates, security defaults, observability) with exact tech (Cookiecutter, Go, K8s/ArgoCD); minor ding for not mentioning CI/CD explicitly (-1). Clarity: Precise scope (3-5 practices, one implementation), no ambiguity or bundling. Discriminative: Requires applied knowledge of best practices + implementation, tests trade-offs/explanations to separate strong candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-443",
        "role_title": "Senior Backend Engineer - Data Lake Architecture",
        "question": "**Question:**  \nIn a data lake using Apache Iceberg on S3 with Spark for writes and Trino for queries, describe a practical approach to handle schema evolution (e.g., adding a new column) while ensuring safe partition pruning and time travel queries remain functional. What Iceberg-specific features would you leverage, and why? (Briefly outline 2-3 steps with trade-offs.)\n\n*(Expected answer time: 2-4 minutes. Tests senior-level practical experience with Iceberg schema evolution, integration across Spark/Trino/S3, and awareness of features like schema evolution modes, metadata tables, and branching for time travel.)*",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core skills (Iceberg table formats, schema evolution, partition pruning, time travel) with exact tech stack (S3, Spark, Trino). Clarity: Crystal clear, well-scoped with steps/trade-offs, expected time, and no bundling/ambiguity. Discriminative: Requires applied senior-level knowledge of Iceberg features/modes, trade-offs, and integrations, strongly separating experienced candidates; minor deduction as not ultra-deep edge-case probing.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-439",
        "role_title": "Backend Engineer - Billing Integration",
        "question": "**Question:**  \nIn a Python-based billing system using Stripe and PostgreSQL, describe how you would implement a customer upgrade from a $20/month plan to a $50/month plan mid-cycle (e.g., on day 10 of a 30-day month), ensuring proper proration of the unused portion of the old plan while immediately applying the new plan. Include key Stripe API calls and any relevant database considerations. (Answer in 3-5 sentences or bullet points.)\n\n*(Expected answer outline: Use `stripe.Subscription.modify()` with `proration_behavior='create_prorations'` and `items` to update the price ID; credit the unused $10 days of the old plan ($20 * 20/30 \u2248 $13.33) and charge the prorated new plan ($50 * 20/30 \u2248 $33.33); store subscription metadata and sync status/events in PostgreSQL via a Celery task triggered by Stripe webhooks for idempotency and audit trail.)*",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core focus on billing integrations, proration, Stripe, with Python/PostgreSQL directly from description (no red flags). Clarity: Crystal clear scenario, well-scoped with format/depth guidance; minor deduction for lacking exact proration formula. Discriminative: Tests deep applied knowledge of Stripe API, proration math, DB sync, and idempotency\u2014distinguishes strong candidates via trade-offs and integration thinking.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-441",
        "role_title": "Senior Backend Engineer - Autocomplete Systems",
        "question": "**Question:**  \nYou're building an autocomplete system using Elasticsearch that needs to support prefix matching with user personalization (e.g., boosting recent searches for the user) while targeting sub-50ms latency at 100K QPS. Traffic is sharded across 10 ES nodes.\n\nDescribe your approach to indexing and querying for prefix matches with personalization. What key ES features would you use (e.g., specific query types, mappings), and how would you optimize for latency at this scale? (Focus on 2-3 most critical optimizations.)",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core autocomplete skills, prefix matching, personalization, and latency/QPS from role desc using ES (relevant tech); minor ding as ES not explicitly listed (-1). Clarity: Well-scoped with specific constraints, focus on 2-3 opts, no ambiguity. Discriminative: Requires applied ES knowledge, trade-offs in indexing/querying/optimizations at scale, separates strong candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-445",
        "role_title": "Junior Backend Engineer - Database Migrations",
        "question": "**Question:**\n\nYou're using Alembic to manage database migrations for a PostgreSQL database with SQLAlchemy. A recent migration added a non-nullable `email` column to the `users` table without a default value, but some existing rows now fail validation because they lack email data.\n\nExplain in 2-3 steps how you would create a follow-up Alembic migration to fix this (backfill missing emails with a placeholder like `'missing@example.com'`, make the column nullable temporarily, then add a NOT NULL constraint). Include the key Alembic commands and any relevant SQLAlchemy/Alembic revision code snippets.\n\n*(Expected answer time: 2-4 minutes)*",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly targets Alembic migrations, PostgreSQL, SQLAlchemy, and zero-downtime/rollback patterns for entry-level role; no red flags. Clarity: Crystal clear with specific steps, expected output, and time scope; minor deduction for not specifying exact code format. Discriminative: Requires applied knowledge of Alembic revision creation, upgrade/downgrade logic, and safe schema changes, distinguishing those with hands-on experience from theorists.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-447",
        "role_title": "Backend Engineer - Load Shedding",
        "question": "**Question:**\n\nYou're implementing load shedding in a Go service running on Kubernetes with Envoy as the ingress proxy. Under high load, you need to gracefully degrade by dropping low-priority requests while preserving critical ones. Describe a practical approach using Go code snippets, including:\n\n1. How you'd integrate with Envoy's external authorization or backpressure signals (e.g., via headers or metrics).\n2. A simple adaptive throttling mechanism (e.g., using Redis for a priority queue or rate limiter).\n3. How you'd signal backpressure to upstream callers without crashing.\n\nKeep it concise\u2014focus on the key logic and trade-offs. (Answer in 2-5 minutes.)",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core load shedding, priority queues, throttling, and backpressure from role; mentions Go/K8s/Envoy (relevant stack), minor ding for Redis not in desc (-1). Clarity: Crystal clear, well-scoped with numbered parts, time limit, and focus on logic/trade-offs. Discriminative: Requires applied knowledge, integration, code snippets, and trade-offs; distinguishes deep understanding from superficial knowledge.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-450",
        "role_title": "Principal Engineer - ML Platform Strategy",
        "question": "**Question:**  \nYou're leading the ML platform strategy for our organization and need to recommend a tooling stack for productionizing ML workflows across 50+ teams. Our current setup includes Kubernetes for orchestration, and we're evaluating Kubeflow, MLflow, and Ray.  \n\nWalk me through your high-level architecture design: How would you integrate 1-2 of these tools with Kubernetes to support end-to-end ML lifecycles (experimentation, training, serving, monitoring)? What key governance considerations (e.g., access control, cost management) would you prioritize, and why?",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core ML platform strategy, tooling selection (Kubeflow/MLflow/Ray), governance, and Kubernetes integration from role (-1 for not specifying all role elements like team enablement). Clarity: Crystal clear, well-scoped with specific tools, lifecycle stages, and governance examples; no bundling or vagueness. Discriminative: Requires applied architecture design, integration trade-offs, and prioritized reasoning, strongly separating strategic thinkers from rote responders.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-448",
        "role_title": "Senior Backend Engineer - Webhook Infrastructure",
        "question": "**Question:**  \nIn Go, design a simple retry mechanism for delivering a webhook payload to an endpoint that might fail transiently (e.g., 5xx errors). Outline the key components (code structure, not full implementation), including exponential backoff, deduplication, and a max retry limit. How would you persist retry state across restarts using Redis or PostgreSQL? (Answer in 300 words or less.)\n\n*(Expected answer: Struct with payload ID, exponential backoff like `time.Sleep(base * 2^attempt)`, Redis/PostgreSQL for state like `SETEX payload_id {attempts,deadline}`, dead letter queue on max retries; tests delivery guarantees at senior level.)*",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (retry logic, guaranteed delivery) in Go, with persistence; minor deduction as role doesn't explicitly specify Go or Redis/PostgreSQL (-1). Clarity: Crystal clear, well-scoped with word limit, specific components listed, no ambiguity. Discriminative: Requires applied knowledge of backoff, deduplication, persistence trade-offs, distinguishes seniors via design choices and guarantees; not mere recall.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-452",
        "role_title": "Senior Backend Engineer - Faceted Search",
        "question": "**Question:**\n\nIn Elasticsearch, you're building a faceted search for an e-commerce product catalog with millions of items. Users want to filter by category, price range, and brand, while seeing accurate facet counts (e.g., \"10 laptops in stock under $1000\"). \n\nExplain how you'd structure the search query using `bool` filters and `composite` or `terms` aggregations to ensure facet counts exclude the applied filters and handle high cardinality efficiently. Include a brief code snippet in Python (using elasticsearch-py) and mention one key optimization for scale.\n\n*(Expected answer: Use `post_filter` for client-side filters or filtered aggs; `composite` agg for pagination/high-cardinality; optimizations like sampler agg or keyword subfields.)*",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's faceted search, dynamic filtering, aggregation queries, facet accuracy at scale with Elasticsearch; no red flags. Clarity: Crystal clear, well-scoped with specific requirements and example; minor deduction for not specifying exact response format/length. Discriminative: Tests deep applied knowledge of ES query structure, aggs trade-offs (composite vs terms), optimizations; expected answer shows strong separation; not pure recall.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-449",
        "role_title": "Junior Backend Engineer - Auth Integration",
        "question": "**Question:**  \nIn a Node.js/Express app using TypeScript and PostgreSQL, you're implementing an OAuth2 authorization code flow for Google login. After receiving the authorization code from Google and exchanging it for an access token and refresh token via their token endpoint, walk us through the key steps you'd take to securely store and manage these tokens for the user. Mention any relevant database schema considerations and security best practices (e.g., for refresh tokens). \n\n*(Expected answer outline: Validate code, exchange for tokens securely (HTTPS), hash refresh token before storing in DB with user ID/expiry, store access token in HTTP-only secure cookie or short-lived session, implement refresh logic with validation/rotation, use libraries like `passport-oauth2` or `oauth4webapi`.)*",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's OAuth focus, token handling, refresh flows, and exact tech stack (Node.js/Express/TS/PostgreSQL); no red flags. Clarity: Crystal clear, well-scoped steps with specific context, minor deduction only for implicit time constraint. Discriminative: Requires applied knowledge of secure token practices, trade-offs (hashing vs cookies, rotation), and DB design, strongly separates candidates familiar with OAuth security from those with superficial knowledge; not mere recall.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-436",
        "role_title": "Backend Engineer - API Documentation",
        "question": "**Question:**  \nYou're building an OpenAPI spec for a Node.js/Express API endpoint that retrieves user data from PostgreSQL. The endpoint is `GET /api/v1/users/{userId}` and returns a JSON object like `{ id: number, name: string, email: string }`.  \n\nWrite a minimal OpenAPI 3.0 YAML snippet for this endpoint, including:  \n- Path parameters  \n- A 200 response schema  \n- One example response  \n- Basic security (e.g., API key)  \n\n(Provide just the `paths` and `components` sections; no need for full spec.)  \n\n**Expected Time:** 3-5 minutes  \n**What it tests:** Practical OpenAPI authoring, TypeScript-aligned schema design, API versioning awareness.",
        "scores": {
          "relevance": 9,
          "clarity": 10,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core OpenAPI spec generation and API versioning (v1 in path), central to role's auto-generated specs and documentation tooling; minor ding as Node.js/PostgreSQL are illustrative not required (-1). Clarity: Crystal clear with precise scope (minimal YAML, specific sections, constraints), expected time, no ambiguity. Discriminative: Requires practical applied knowledge of OpenAPI 3.0 YAML syntax, schemas, examples, and security; distinguishes those who can author specs from theorists (strong separation, not recall).",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-451",
        "role_title": "Backend Engineer - Subscription Billing",
        "question": "**Question:**  \nIn a subscription billing system using PostgreSQL and Java/Spring Boot, how would you implement usage-based metering for a feature like API calls, including storing the usage data and checking limits during billing cycles? Briefly describe the schema design and a key method or query you'd use to compute billable usage (e.g., for proration across mid-cycle plan changes).",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Directly tests core role skills (usage metering, proration, billing cycles) with exact tech stack (PostgreSQL, Java/Spring Boot). Crystal clear, well-scoped with specific schema and method/query asks. Requires applied knowledge of design trade-offs, distinguishes strong candidates via practical implementation details. Minor ding for not specifying exact constraints, but excellent overall.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-446",
        "role_title": "Staff Engineer - Distributed Tracing",
        "question": "**Question:**  \nYou're designing a distributed tracing system for a high-scale service using OpenTelemetry and Jaeger as the backend. Traffic is at 10,000 requests/second, and storing 100% of traces is infeasible due to storage costs in ClickHouse.  \n\nWalk me through how you'd architect a sampling strategy: What type(s) of sampling would you use (e.g., head/tail, rate-limiting, probabilistic), how would you configure it in OpenTelemetry, and how would you tune it dynamically based on errors or SLOs? What trade-offs are you considering for trace completeness vs. cost?",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Relevance: Directly targets core role responsibilities (distributed tracing, sampling strategies, SLO correlation) with exact tech stack (OpenTelemetry, Jaeger, ClickHouse); minor ding for assuming specific backend not explicitly listed (-1). Clarity: Crystal clear, well-scoped scenario with concrete numbers, specific asks, no ambiguity. Discriminative: Forces deep applied knowledge of sampling types, config, dynamic tuning, and trade-offs, perfectly separating strong architects from weak ones.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-454",
        "role_title": "Junior Backend Engineer - ETL Scripts",
        "question": "**Question:**\n\nYou're writing a Python ETL script using pandas to load sales data from a CSV file into a PostgreSQL table. The CSV has 100k rows with a 'sale_date' column in string format ('2023-12-01'). \n\nWrite a short code snippet (5-10 lines) that:\n- Reads the CSV into a pandas DataFrame\n- Converts 'sale_date' to a proper datetime type\n- Handles any invalid dates by setting them to NaT (not a time)\n- Inserts the cleaned data into a PostgreSQL table named 'sales' (assume connection string is provided)\n\nInclude necessary imports and explain one potential error you might encounter during the insert.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core ETL skills (pandas data loading/transformation) and PostgreSQL for entry-level role; minor ding for assuming connection string (-1). Clarity: Precise scope (5-10 lines), clear steps, well-constrained. Discriminative: Requires applied pandas/datetime handling, error management, SQL integration, and trade-off explanation (e.g., connection errors), separating competent from weak candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-456",
        "role_title": "Backend Engineer - Query Optimization",
        "question": "**Question:**  \nYou're analyzing a slow PostgreSQL query using `EXPLAIN ANALYZE` that shows a sequential scan on a large table (`users`) with 10M rows, filtering on `email` (varchar(255)). The query is `SELECT * FROM users WHERE email = 'user@example.com';`.  \n\n1. What index would you recommend adding, and why?  \n2. How would you verify if it improves performance?  \n\n*(Expected answer: B-tree index on `email` since it's selective and equality-based; re-run `EXPLAIN ANALYZE` to check for index scan, compare planning/execution time.)*",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Directly tests core role skills (slow query detection, index recommendations, plan analysis) with PostgreSQL example matching query optimization focus; no red flags. Crystal clear with specific scenario, two focused sub-questions (minor bundle deduct not applied as well-scoped). Strong separation via trade-offs (why B-tree/selective), verification method testing applied knowledge beyond recall.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-455",
        "role_title": "Staff Engineer - Developer Portal",
        "question": "**Question:**  \nIn Backstage, imagine you're designing a plugin architecture for a developer portal where teams need to onboard new services to the service catalog with minimal friction. Walk us through how you would structure a custom Backstage plugin (using TypeScript and React) to automate service entity creation from a GitHub PR webhook, including key code snippets for the backend handler and frontend scaffolder UI. What Backstage APIs or patterns would you leverage to ensure it integrates seamlessly with the existing catalog and supports extensibility for future teams?",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Relevance: Directly tests Backstage plugin architecture, service catalog onboarding, and extensibility\u2014core to the role (-1 for not explicitly mentioning team flows). Clarity: Well-scoped with specific tech (TypeScript/React), clear deliverables (structure, snippets, APIs), no bundling or vagueness. Discriminative: Requires deep applied knowledge of Backstage APIs/patterns, trade-offs in integration/extensibility, and code-level implementation\u2014strong separation of strong vs weak candidates.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-453",
        "role_title": "Backend Engineer - Notification Delivery",
        "question": "**Question:**  \nYou're implementing a Go service for delivering push notifications using FCM and APNS. Devices register their tokens in Redis (with TTL for expiration) and PostgreSQL (for persistence). Describe a simple batching strategy to efficiently send notifications to 10,000 registered devices in real-time, including how you'd handle token expiration checks and delivery tracking. What Go data structures and Redis operations would you use?\n\n*(Expected answer outline: Use Redis SCAN or Lua script for batched token fetching with TTL checks; Go channels/slices for concurrent goroutine batches; PostgreSQL for idempotent delivery status updates via UPSERT. Limit batch size e.g., 100-500 per goroutine to respect FCM/APNS rate limits.)*",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core requirements (push notifications, device registration, batching, delivery tracking) with exact tech stack (Go, Redis, PostgreSQL, FCM/APNS). Clarity: Crystal clear, well-scoped for screening, specific tech details provided. Discriminative: Tests applied knowledge of batching, concurrency (Go channels/goroutines), Redis ops, rate limits, and trade-offs like batch size; expected outline shows deep understanding needed. Minor clarity ding for no explicit time constraint, but excellent overall.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-458",
        "role_title": "Backend Engineer - API Gateway Plugins",
        "question": "**Question:**  \nIn Kong, you're developing a custom Lua plugin to implement JWT authentication. The plugin needs to verify the JWT token from the `Authorization` header, decode it using the `lua-resty-jwt` library, validate the `exp` claim against the current time, and either allow the request or return a 401 Unauthorized response.  \n\nWrite a concise code snippet for the `access` phase of the plugin that accomplishes this (assume `kong.request.get_header(\"Authorization\")` provides the header value starting with \"Bearer \"). Include error handling for missing/invalid tokens.  \n\n*(Expected: ~10-15 lines of Lua code demonstrating practical Kong plugin structure, header parsing, JWT library usage, and phase logic.)*",
        "scores": {
          "relevance": 10,
          "clarity": 10,
          "discriminative": 9,
          "reasoning": "Directly tests core Kong Lua plugin development for API gateway authentication (matches role's custom auth focus). Crystal clear with precise scope, assumptions, expected output, and error handling requirements. Strong separation via applied coding with library integration, validation logic, and Kong phase structure; minor deduction as top candidates might discuss edge cases beyond snippet.",
          "composite": 9.666666666666666
        }
      },
      {
        "role_id": "be-457",
        "role_title": "Senior Backend Engineer - SLO Automation",
        "question": "**Question:**  \nIn Go, implement a simple function to calculate the current SLO burn rate for a service, given a time window (e.g., 28 days), the total error budget (e.g., 1000 \"credits\"), the good request count, and the total request count over that window. The burn rate should be the rate at which error budget is consumed per request (e.g., 0.01 credits per bad request). Explain your assumptions and edge cases briefly.\n\n*(Expected answer: Something like `func BurnRate(window int64, budget, good, total uint64) float64 { errorBudgetConsumed := float64(total - good) / float64(budget); return errorBudgetConsumed / float64(window) }` \u2013 tests Go proficiency, SLO math, and senior-level reasoning on reliability concepts like error budgets.)*",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests Go (core for role), SLO burn rate/error budget math central to SLO infrastructure/burn rate alerting; minor deduct for not specifying exact role tech stack but fits perfectly (-1). Clarity: Crystal clear inputs/outputs, well-scoped function impl with explanation request. Discriminative: Requires Go syntax, correct SLO math trade-offs (per-request vs per-time), edge cases (zero requests, full budget); distinguishes seniors who catch bad expected formula from juniors.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-459",
        "role_title": "Junior Backend Engineer - Container Security",
        "question": "**Question:**  \nYou're setting up Trivy to scan a Docker image for vulnerabilities in a CI/CD pipeline. The scan reveals a high-severity vulnerability in a dependency. Walk me through 3 practical steps you'd take next to address it, including one command you'd run with Trivy to help investigate. (Keep your answer to 3-5 sentences.)",
        "scores": {
          "relevance": 9,
          "clarity": 10,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core container scanning skills with Trivy (key for vulnerability detection), perfectly matching entry-level role focus (-0 red flags). Clarity: Crystal clear, well-scoped with exact step count, sentence limit, and specific command request. Discriminative: Requires practical application and trade-offs (e.g., investigate vs. fix), distinguishing those with hands-on CI/CD scanning experience from theorists.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-463",
        "role_title": "Junior Backend Engineer - Full-text Search",
        "question": "**Question:**  \nYou're implementing a full-text search feature in PostgreSQL for a blog application using Python and FastAPI. You have a `posts` table with a `content` column (text). Write a SQL query to create a GIN-indexed `tsvector` column for full-text search, then show a basic `SELECT` query that searches for posts matching the query \"machine learning\" and ranks results by relevance (using `ts_rank`).  \n\n*Bonus (optional): How would you pass this search query from a FastAPI endpoint to execute it?*",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests PostgreSQL full-text search core skills (tsvector, GIN index, ts_rank) from role description, with relevant FastAPI context (-1 for minor Python mention not core to entry-level Postgres focus). Clarity: Crystal clear, well-scoped tasks with specific elements, bonus properly separated. Discriminative: Requires applied SQL knowledge and trade-offs (indexing, ranking), distinguishes candidates who understand FTS implementation from those with superficial knowledge.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-465",
        "role_title": "Backend Engineer - CDC Pipelines",
        "question": "**Question:**  \nIn a Debezium CDC pipeline streaming PostgreSQL changes to Kafka (with Snowflake as the downstream sink), describe how you would handle an unexpected schema change, such as a new required column being added to a table mid-stream. Walk through your step-by-step approach to ensure exactly-once delivery and minimal downtime (2-3 sentences max).",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Directly tests core CDC pipeline skills (schema changes, exactly-once delivery) matching role focus on PostgreSQL/Debezium/Kafka/Snowflake; crystal clear with specific scenario, constraints (2-3 sentences), and step-by-step scope; probes applied trade-offs for minimal downtime, strongly separating experienced engineers from juniors. Minor ding for assuming exact tech stack familiarity without role confirmation (-1 relevance).",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-464",
        "role_title": "Staff Engineer - Presence Systems",
        "question": "**Question:**  \nYou're designing a real-time presence system in Go using WebSockets to track user online status across 1M+ concurrent connections. Describe how you'd implement a heartbeat protocol for connection health checks, including client-side and server-side logic, handling missed heartbeats, and ensuring low-latency state updates (e.g., via Redis). What trade-offs would you consider for scalability in Kubernetes?",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (presence, heartbeats, connection management, state sync) with exact tech (Go, WebSockets, Redis) and scale (1M+), minor ding for Kubernetes not explicit in role (-1). Clarity: Well-scoped, unambiguous, specifies components without bundling multiples. Discriminative: Probes applied knowledge, trade-offs, and deep scalability understanding, strongly separates candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-462",
        "role_title": "Backend Engineer - Alert Routing",
        "question": "**Question:**  \nIn Alertmanager, you've received a batch of 10 alerts from a service outage (e.g., high CPU, high latency, error rate spikes across 5 pods). Describe your Go-based approach to implement smart grouping and deduplication before routing to PagerDuty. Include key grouping labels and a brief code snippet for the matching logic (no full implementation needed). How would you handle escalation if no one acknowledges within 5 minutes?",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (alert grouping, deduplication, escalation) with Alertmanager and Go, matching role focus; minor ding as PagerDuty is implied not explicit (-1). Clarity: Well-scoped, specific scenario, clear asks (approach, labels, snippet, escalation) without ambiguity. Discriminative: Requires applied knowledge of Alertmanager concepts, Go implementation trade-offs, and system design, distinguishing strong candidates via reasoning and code insight.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-460",
        "role_title": "Backend Engineer - Experiment Tracking",
        "question": "**Question:**  \nIn MLflow, you've just logged a run with parameters, metrics, and a model artifact to an S3-backed artifact store. Describe the exact Python code (or key MLflow API calls) you'd use to query and compare the average validation accuracy across the top 3 runs from that experiment, sorted by a custom metric like 'f1_score'. Assume you're using a PostgreSQL tracking backend.\n\n*(Expected answer: Use `mlflow.search_runs()` with `order_by=[\"metrics.f1_score DESC\"]`, `max_results=3`, filter by experiment ID if needed, then aggregate `metrics.val_accuracy` via pandas or list comprehension. Tests practical MLflow querying, metric handling, and Python fluency.)*",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests MLflow (core for experiment tracking), metric handling, artifact storage (S3), and comparison - matches role perfectly; minor ding as not all role aspects explicit (-1). Clarity: Precise, well-scoped API-focused question with clear assumptions; no bundling or ambiguity. Discriminative: Requires applied MLflow knowledge (search_runs params, filtering, aggregation), trade-offs in querying; distinguishes practical users from theorists.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-461",
        "role_title": "Senior Backend Engineer - Fraud Detection",
        "question": "**Question:**  \nYou're building a real-time fraud detection pipeline using Kafka for event streaming, Redis for caching, and scikit-learn models for scoring. Describe how you'd architect the system to score a payment transaction in under 100ms, including how you'd handle feature lookups from PostgreSQL, rule engine evaluation (e.g., velocity checks), and ML inference. What trade-offs would you make for latency vs. accuracy, and how would you use Redis here?\n\n*(Expected answer outline: Kafka consumer \u2192 Redis for hot features/low-latency lookups \u2192 simple rule engine in Python \u2192 lightweight scikit-learn inference (e.g., cached/ONNX) \u2192 score + queue to Postgres/manual review. Trade-offs: approximate features in Redis, async ML retraining, sampling for accuracy monitoring.)*",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's fraud detection pipelines, real-time scoring, rule engines, ML integration; uses exact tech stack implied (streaming, caching, ML, DB). No red flags.\nClarity: Crystal clear, well-scoped to <100ms latency with specific components; minor -1 for not specifying response format/depth.\nDiscriminative: Excellent test of applied architecture, trade-offs (latency/accuracy), and integration; distinguishes deep understanding from superficial knowledge. Not too broad for screening.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-466",
        "role_title": "Senior Backend Engineer - GitOps Tooling",
        "question": "**Question:**  \nIn ArgoCD, you've deployed an application via GitOps, but a manual `kubectl edit` by a developer has caused configuration drift. Walk me through how you'd detect this drift, confirm it, and design a simple Go-based reconciliation loop to automatically revert it to the desired Git state. What key ArgoCD APIs or CRDs would you use, and how would you handle potential race conditions?",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Directly tests core GitOps skills (drift detection, reconciliation loops) matching role's focus on declarative infra tooling. Crystal clear with specific scenario and asks for design, APIs, and trade-offs like race conditions, strongly distinguishing deep ArgoCD/Go expertise. Minor ding for assuming ArgoCD familiarity not explicitly listed, but it's core GitOps tooling.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-467",
        "role_title": "Backend Engineer - Connection Pooling",
        "question": "**Question:**  \nIn a Go application using PgBouncer for connection pooling to PostgreSQL in a Kubernetes environment, you've observed high connection latency during traffic spikes. Walk me through your step-by-step approach to diagnose and tune the PgBouncer pool sizing (e.g., default_pool_size, reserve_pool_size) while ensuring it handles failover without dropping connections. What key metrics would you monitor, and how would you validate the changes?",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Directly tests core role skills in connection pooling (PgBouncer pool sizing), scaling, failover, and transaction handling in Kubernetes; highly relevant to PostgreSQL/Go context. Crystal clear with specific parameters, step-by-step expectation, metrics, and validation; well-scoped. Strong differentiation via diagnostic reasoning, tuning trade-offs, and monitoring, distinguishing experienced engineers.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-468",
        "role_title": "Junior Backend Engineer - CI Pipelines",
        "question": "**Question:**  \nYou're setting up a GitHub Actions workflow in YAML to build and test a Python application using Docker. The workflow runs `pip install -r requirements.txt` which takes 2 minutes every time.  \n\nWrite a simple 4-step job that:  \n1. Builds a Docker image from a `Dockerfile` (assume it installs Python deps).  \n2. Caches the pip dependencies to speed it up.  \n3. Runs unit tests inside the container.  \n4. Uses matrix strategy to parallelize tests across Python 3.9 and 3.11.  \n\nProvide the full `jobs.<job_id>.steps` YAML snippet (no need for full workflow file).  \n\n*Follow-up (if time): What does the cache key look like, and why use it?*",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core CI pipeline skills (GitHub Actions YAML, caching, matrix parallelization, Docker) from role description (-0 red flags). Clarity: Precise scope (4 steps, YAML snippet only), unambiguous instructions, well-constrained. Discriminative: Requires applied YAML knowledge with trade-offs (caching keys, matrix), follow-up probes deeper understanding; strong separation for entry-level.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-469",
        "role_title": "Senior Backend Engineer - Chaos Engineering",
        "question": "**Question:**\n\nIn Chaos Mesh on Kubernetes, you're designing a failure injection experiment to test pod resilience in a critical service. Describe a scenario where you'd use a **NetworkChaos** to simulate 50% packet loss between pods in a specific namespace, including the key fields you'd set in the Chaos Mesh CRD YAML (e.g., mode, selector, and disruption rules). Why would you choose **One** mode over **All** here, and how would you limit the blast radius to avoid impacting production traffic? \n\n*(Expected: Concise YAML snippet or key fields, rationale on mode for controlled testing, namespace/selector for blast radius; tests practical Chaos Mesh + K8s knowledge in 2-5 mins.)*",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests Chaos Mesh and Kubernetes skills core to chaos engineering role (-1 for assuming Chaos Mesh as 'the' tool, not explicitly listed). Clarity: Precise, well-scoped with examples, avoids ambiguity. Discriminative: Requires applied YAML knowledge, trade-offs (One vs All), and blast radius control, separating practical experts from theorists in <5 mins.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-471",
        "role_title": "Staff Engineer - Zero Trust Architecture",
        "question": "**Question:**  \nIn a Kubernetes-based microservices environment using Envoy as a sidecar proxy, walk me through how you would automate mTLS between services using SPIFFE/SPIRE for identity, including key components you'd configure and a high-level step-by-step approach. What challenges might arise with certificate rotation, and how would you address them?",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core mTLS automation, SPIFFE identity (matches SPIRE), and microservices security in Kubernetes/Envoy context; minor -1 for not explicitly mentioning federation/policy but central to zero trust. Clarity: Precise, well-scoped with components/steps/challenges; no bundling or vagueness. Discriminative: Requires applied knowledge of integration, trade-offs in rotation challenges; distinguishes deep expertise from superficial recall.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-470",
        "role_title": "Backend Engineer - GraphQL Federation",
        "question": "**Question:**  \nIn Apollo Federation, explain how entity resolution works across subgraphs during query execution. Walk through a simple example with two subgraphs (e.g., `User` service exposing a `User` entity and `Order` service referencing it), including the key directives involved and what the gateway does step-by-step. What happens if entity resolution fails?\n\n*(Expected answer: Candidate describes `@key` directive on entity in owning subgraph, `@requires` or `@provides` if needed, reference resolution via `_entities` field on gateway, canonical entity definition, batching via `@ApolloFederationInclude`, and error handling like partial resolution or normalization failures. Tests practical understanding of federation flow.)*",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core Apollo Federation skills (schema stitching, entity resolution, query planning) from role description; no red flags. Clarity: Crystal clear with specific example scope, minor deduction for lacking explicit time/format constraint. Discriminative: Requires deep applied knowledge of directives, flow, trade-offs (batching, errors), strong separation of experts from surface-level knowers; not mere recall.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-472",
        "role_title": "Backend Engineer - Vector Search",
        "question": "**Question:**  \nYou're building a semantic search API using FastAPI, Pinecone for vector storage, and a model like Sentence Transformers for embeddings. Walk me through the key steps to implement a hybrid search endpoint that combines vector similarity (from Pinecone) with a keyword filter (using metadata), including how you'd generate and upsert embeddings for new documents. What Pinecone features would you use, and how would you handle rate limiting with Redis? \n\n*(Expected: 2-4 min verbal explanation covering embedding gen/upsert, Pinecone hybrid query, metadata filtering, Redis for caching/rate limits)*",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's vector search, embeddings, indexing, hybrid search; uses exact tech stack (FastAPI, Pinecone, embeddings). Clarity: Crystal clear steps and scope, minor deduction for no explicit time constraint (expected time noted but not in question). Discriminative: Tests integrated applied knowledge, trade-offs in hybrid search/upsert/rate limiting; strong separation without being too broad.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-477",
        "role_title": "Senior Backend Engineer - Data Catalog",
        "question": "**Question:**  \nYou've implemented a data pipeline using Apache Atlas for metadata management and Kafka for streaming events. Describe a practical approach to automatically extract and register lineage metadata (e.g., from Airflow DAGs processing data in PostgreSQL) into Atlas, including how you'd handle updates if a downstream job fails. What Kafka topic schema and Atlas entity types would you use?",
        "scores": {
          "relevance": 7,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Tests core lineage/metadata skills but assumes unlisted tools (Atlas, Kafka, Airflow; -2 red flag); Clarity: Crystal clear, well-scoped with specific scenario; Discriminative: Excellent on trade-offs, integration depth, schema design, and error handling for strong separation.",
          "composite": 8.333333333333334
        }
      },
      {
        "role_id": "be-479",
        "role_title": "Principal Engineer - Data Architecture",
        "question": "**Question:**  \nYou're leading the data architecture for a platform handling both high-throughput transactional workloads (OLTP) and real-time analytics (OLAP). We use PostgreSQL for transactions, ClickHouse for analytics, Kafka for streaming, Redis for caching, and Kubernetes for orchestration.  \n\nWalk me through your approach to architecting an efficient data pipeline that ingests events from Kafka, supports low-latency OLTP lookups in PostgreSQL, and feeds aggregated analytics into ClickHouse\u2014while ensuring scalability and fault tolerance. What key trade-offs would you consider, and how would you mentor a junior engineer on implementing it?  \n\n*(Expected: 2-5 min response; probes practical architecture strategy, tech stack integration, trade-offs like consistency vs. latency, and mentorship at principal level)*",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's data architecture strategy for analytics/transactional workloads, directly tests listed tech stack (PostgreSQL, ClickHouse, Kafka, Redis, K8s), no red flags. Clarity: Crystal clear, well-scoped with expected time and probes specified, minor deduction for no explicit constraints on format. Discriminative: Tests deep applied knowledge, trade-offs (e.g., consistency vs. latency), integration, scalability, and principal-level mentorship, strongly separates candidates.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-474",
        "role_title": "Backend Engineer - Dashboards API",
        "question": "**Question:**  \nIn our observability platform, we use Grafana dashboards with templating variables (e.g., `${instance}` or `${namespace}`) that are resolved at query time from Prometheus data sources. When building a Go API endpoint to render and share a dashboard snapshot (e.g., for public viewing), how would you handle variable substitution server-side? Describe the key steps, potential pitfalls with PostgreSQL/Redis storage, and a rough Go pseudocode snippet for the substitution logic. (2-4 min)",
        "scores": {
          "relevance": 9,
          "clarity": 10,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core role skills (Go APIs, dashboard templating, variable substitution, sharing) with exact tech (Grafana, Prometheus); minor ding for not explicitly mentioning PostgreSQL/Redis in role but fits observability platform (-1). Clarity: Crystal clear, well-scoped (2-4 min), specific context, precise ask. Discriminative: Tests applied knowledge, trade-offs (pitfalls with DB storage), and practical pseudocode; strong separation of deep vs superficial candidates.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-476",
        "role_title": "Backend Engineer - Live Updates",
        "question": "**Question:**\n\nIn our live updates system using Socket.io and Redis, we need to broadcast dashboard data updates only to users viewing a specific dashboard (e.g., dashboard ID \"abc123\"). Describe how you'd implement room management for this: \n\n1. How do you join/leave rooms when a user connects/disconnects?\n2. How do you trigger a selective broadcast (e.g., on data change)?\n3. How do you handle a user reconnecting mid-session to rejoin the correct room?\n\nProvide concise TypeScript/Node.js code snippets for the key parts (e.g., join logic, emit logic).",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests Socket.io rooms, Redis broadcasting, and reconnection\u2014core to role's live updates, dashboard-specific broadcasting, and reconnection logic (-0 red flags). Clarity: Well-scoped with 3 focused sub-questions, clear TypeScript/Node.js expectations, no bundling issues. Discriminative: Requires applied knowledge of real-time systems with trade-offs (reconnection handling), code snippets test practical skills, strong separation beyond recall.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-483",
        "role_title": "Backend Engineer - WAF Integration",
        "question": "**Question:**  \nIn a Go-based WAF integration using ModSecurity with Nginx, you need to implement custom rate limiting to block IPs exceeding 100 requests per minute. Describe a practical approach using Redis for the rate limiter, including key code snippets for the increment/check logic and eviction strategy. How would you integrate this with ModSecurity rules? (Answer in 2-5 minutes)",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core WAF rate limiting with Go, Redis, ModSecurity/Nginx from role; minor ding for assuming Go/ModSecurity not explicit (-1). Clarity: Precise scope, time limit, clear components requested. Discriminative: Requires applied knowledge of rate limiting (sliding window/token bucket trade-offs), Redis ops, and ModSecurity integration, distinguishing strong candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-475",
        "role_title": "Junior Backend Engineer - Search Analytics",
        "question": "**Question:**  \nYou're analyzing slow search queries in Kibana using Elasticsearch logs for our search analytics platform. A sample log shows a query taking 5+ seconds with this snippet:  \n\n```\n{\n  \"query\": {\n    \"bool\": {\n      \"must\": [\n        { \"match\": { \"title\": \"python tutorial\" } },\n        { \"range\": { \"timestamp\": { \"gte\": \"now-7d\" } } }\n      ]\n    }\n  },\n  \"aggs\": { \"by_user\": { \"terms\": { \"field\": \"user_id\", \"size\": 1000 } } }\n}\n```\n\nWhat\u2019s one likely performance issue here, and how would you fix it using Python (e.g., via elasticsearch-py) or Kibana? Explain briefly.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests Elasticsearch query analysis and optimization, core to search analytics logging (9, minor -1 for Python not in role desc but fits entry-level). Clarity: Crystal clear with concrete log snippet, specific scope, unambiguous (9). Discriminative: Requires applied understanding of ES performance (e.g., high-cardinality terms agg), trade-offs, and fixes; distinguishes candidates with practical knowledge (9).",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-473",
        "role_title": "Senior Backend Engineer - Refund Processing",
        "question": "**Question:**  \nIn a refund processing system using Java and Spring Boot, describe how you would implement partial refunds (e.g., refunding $15 out of a $20 original transaction) while ensuring ledger reconciliation remains balanced. Include how you'd handle a payment provider callback confirming the partial refund, and what data you'd log for audit trails. (Sketch a simple code structure or key methods if possible.)\n\n*(Expected answer outline: Use double-entry ledger (credit original charge, debit refund liability, credit customer on confirmation); idempotent callback handler with transaction ID/versioning; atomic DB updates via Spring @Transactional; audit logs with event sourcing or append-only tables capturing before/after states. Tests Kafka/Redis integration implicitly via scalability mentions.)*",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core refund processing, partial refunds, ledger reconciliation, provider callbacks, and audit trails from role; Java/Spring Boot matches implied stack (-0 red flags). Clarity: Crystal clear, well-scoped with specific scenario, no ambiguity or bundling. Discriminative: Requires applied knowledge of double-entry accounting, idempotency, transactions, and auditing; elicits trade-offs and structure to separate strong candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-481",
        "role_title": "Backend Engineer - Canary Deployments",
        "question": "**Question:**  \nYou're implementing a canary deployment for a Go microservice using Istio and ArgoCD. Walk me through how you'd configure Istio VirtualService and DestinationRule to split 10% of traffic to the canary version (v2), monitor key Prometheus metrics for comparison against the baseline (v1), and set up an automated promotion gate in ArgoCD if the canary succeeds. What specific metrics would you prioritize, and how would you trigger a rollback if they degrade?",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Directly tests core canary deployment skills (traffic splitting, metrics, rollback, promotion gates) with exact role tech (Istio, ArgoCD, Prometheus, Go microservice). Crystal clear, well-scoped walkthrough with specific constraints; elicits trade-offs in metrics, monitoring, and automation, strongly separating experienced candidates. Minor clarity ding for no explicit time constraint, but excellent overall.",
          "composite": 9.666666666666666
        }
      },
      {
        "role_id": "be-482",
        "role_title": "Senior Backend Engineer - API Monetization",
        "question": "**Question:**\n\nYou're building an API monetization platform in Go using Kong as the gateway, PostgreSQL for persistent storage, and Redis for caching. Describe how you would implement usage quota enforcement for API requests, including:\n\n1. Tracking requests per API key and time window (e.g., per hour/day).\n2. Handling quota checks at high scale (e.g., 10k reqs/sec).\n3. Integrating with Stripe for overage billing when quotas are exceeded.\n\nProvide a high-level design with key data structures, Go pseudocode snippets for critical parts (e.g., middleware or service), and trade-offs for consistency vs. performance.",
        "scores": {
          "relevance": 7,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Tests core quota management and billing skills for the role but docked -2 for mentioning Kong/Stripe/Go stack details not explicitly listed in role description. Clarity: Crystal clear, well-scoped with specific subpoints and expectations for design/pseudocode/trade-offs. Discriminative: Excellent at separating candidates via applied high-scale design, trade-offs, and implementation details; not mere recall.",
          "composite": 8.333333333333334
        }
      },
      {
        "role_id": "be-478",
        "role_title": "Backend Engineer - Cost Attribution",
        "question": "**Question:**  \nIn our cost attribution system built with Go and Kubernetes, we need to allocate cloud resource costs (e.g., from AWS or GCP) to teams based on resource tags like `team:engineering` or `team:sales`. Walk me through how you would design and implement a Go function to parse a list of Kubernetes pod metadata (including labels/tags), aggregate costs by team, and output a simple chargeback report. What key considerations would you have for accuracy, performance, and error handling? (Feel free to sketch pseudocode.)\n\n*(Expected: 2-5 min verbal/whiteboard response testing practical Go logic, grouping/aggregation, awareness of K8s labels, and mid-level tradeoffs like concurrency or validation.)*",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (cost attribution, resource tagging, K8s, Go) with no red flags (-0). Clarity: Crystal clear, well-scoped for screening with explicit time/depth expectations. Discriminative: Tests applied Go logic, K8s knowledge, aggregation, and trade-offs (accuracy/perf/error handling), strongly separating mid-level candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-480",
        "role_title": "Junior Backend Engineer - Deployment Automation",
        "question": "**Question:**  \nYou're tasked with deploying a simple Python Flask app to an AWS EC2 instance using Ansible. The app is in a Git repo, and you need to pull the latest code, install dependencies via `pip`, and start the app with `gunicorn`.  \n\nWrite a basic Ansible playbook (or key tasks from it) to automate this deployment on a target host. Assume Docker and AWS setup are already handled, and focus on the Ansible tasks for code deployment and service startup. What error-handling or idempotency features would you add to make it junior-friendly and reliable?  \n\n*(Expected: 5-10 lines of YAML tasks showing `git`, `pip`, `systemd` or similar, plus 1-2 sentences on best practices like `creates:` or `notify:`.)*",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests Ansible for deployment scripts, core to role's config management and environment setup; minor ding as EC2/Docker not explicitly listed but fits AWS deployment context (-1). Clarity: Well-scoped with clear assumptions, expected output, and constraints; no bundling or ambiguity. Discriminative: Requires writing practical YAML tasks plus explaining idempotency/trade-offs, separates those who can apply Ansible from theorists.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-486",
        "role_title": "Backend Engineer - Span Processing",
        "question": "**Question:**  \nIn our span processing pipeline using OpenTelemetry and Go, we're implementing tail sampling to retain only the most relevant traces (e.g., based on error rate or latency). Describe a practical approach you'd take to implement a tail sampler in Go that decides on-the-fly whether to keep a completed trace, including key data structures and any concurrency considerations. What trade-offs would you make for memory usage vs. sampling accuracy?",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core role skill (tail sampling in span processing) with exact tech stack (OpenTelemetry, Go); no red flags. Clarity: Crystal clear, well-scoped for screening with specific focus on data structures/concurrency; no ambiguity or bundling. Discriminative: Tests deep applied knowledge, trade-offs (memory vs accuracy), and concurrency in distributed context; strong separation of strong vs weak candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-484",
        "role_title": "Staff Engineer - Model Registry",
        "question": "**Question:**  \nYou're designing a model registry using MLflow to manage the ML lifecycle across a team of 20 data scientists. Models are trained daily, stored in S3, and deployed via Kubernetes. Walk me through your versioning strategy for models (including how you'd handle staging vs. production versions), and how you'd implement a simple governance workflow to require approval before promoting a model from staging to production. What MLflow features or integrations (e.g., with PostgreSQL or Kubernetes) would you leverage, and why?",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Directly targets core role skills (model registry, versioning, governance, deployment). Specifies MLflow (key for model registry), S3/K8s (storage/deployment), with clear scope. Highly discriminative via trade-offs in versioning/approvals and MLflow integrations; minor ding for assuming MLflow familiarity not explicitly in role desc (-1 relevance). Crystal clear, well-scoped, tests applied deep knowledge.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-489",
        "role_title": "Backend Engineer - Data Quality",
        "question": "**Question:**\n\nIn a dbt project running on Snowflake with Great Expectations for data quality checks, you've noticed intermittent schema drift where a source table occasionally adds a new column, causing downstream dbt models to fail. Describe a practical approach using dbt, Great Expectations, and Python (e.g., in an Airflow task) to detect this proactively and alert your team, including 2-3 key code snippets or configurations you'd implement.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core schema drift detection in dbt/Snowflake/Great Expectations stack for warehouse pipelines (-0 red flags). Clarity: Well-scoped with specific tools, scenario, and request for 2-3 snippets; no ambiguity or bundling. Discriminative: Requires applied knowledge of integration, proactive detection, alerting, and trade-offs in real pipeline failure scenario, strongly separates strong candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-487",
        "role_title": "Senior Backend Engineer - Search Ranking",
        "question": "**Question:**  \nIn a learning-to-rank (LTR) pipeline for search relevance, you've identified that your model's precision@10 is dropping in production after deploying a new version. Walk me through your step-by-step debugging process, including 2-3 key metrics you'd check first and how you'd use your tech stack (e.g., Elasticsearch, Python, ML tools) to investigate and iterate.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core LTR pipeline skills (debugging production issues) matching role's focus on pipelines, feature engineering, model training, and A/B testing; mentions relevant stack without extras (-0 red flags). Clarity: Crystal clear, well-scoped with specific asks (step-by-step, 2-3 metrics, tech stack use), no bundling or ambiguity. Discriminative: Tests applied debugging, trade-offs, and deep production LTR understanding, strongly separates candidates; not recall or trivial.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-485",
        "role_title": "Junior Backend Engineer - Payment Webhooks",
        "question": "**Question:**\n\nYou're building a FastAPI endpoint to receive payment webhooks from a provider like Stripe. The webhook payload contains a `payment_id` and event details. Explain in 3-5 sentences how you'd implement idempotency to avoid duplicate processing if the same webhook is retried. Include relevant Python/FastAPI code snippet (e.g., using Redis or PostgreSQL).",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests idempotency for payment webhooks, core to role; FastAPI/Python matches implied stack, no red flags (-0). Clarity: Precise scope (3-5 sentences, code snippet, specific DB options), unambiguous for screening. Discriminative: Requires applied knowledge of idempotency patterns, DB integration, and trade-offs, strongly separates candidates able to implement vs. theorize.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-488",
        "role_title": "Backend Engineer - Gaming Backend",
        "question": "**Question:**  \nIn a multiplayer game backend using Go and WebSockets, you need to synchronize player positions across 100 connected clients with minimal latency. Redis is available for pub/sub, and PostgreSQL for persistent state.  \n\nDescribe a simple, practical approach to handle state synchronization (e.g., player position updates). Include:  \n- How you'd structure the update flow in Go.  \n- Role of WebSocket vs. Redis.  \n- How to avoid overwhelming clients or the server (1-2 sentences on optimizations).  \n\n*(Expect 3-5 sentence response; follow-up on trade-offs if time allows.)*",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 8,
          "reasoning": "Relevance: Directly targets core role skills (real-time state sync for multiplayer game backend) and exact tech stack (Go, WebSockets, Redis pub/sub, PostgreSQL), no red flags. Clarity: Crystal clear with precise scope, components specified, and expected response length; minor jargon is contextualized. Discriminative: Tests applied architecture knowledge and optimizations with good separation via trade-offs potential, though slightly broad (-1 for possible edge over 10min).",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-493",
        "role_title": "Staff Engineer - Reliability Standards",
        "question": "**Question:**  \nAs a Staff Engineer defining reliability standards for our org, walk me through how you would design a production readiness review (PRR) process for a new Kubernetes-based service. What key criteria would you include, and how would you ensure it catches reliability risks early? (Feel free to sketch a simple checklist or gates on paper if helpful.)",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly matches core role duties (reliability standards, PRR design) with Kubernetes from context; no red flags. Clarity: Well-scoped for screening, clear intent, minor deduction for no explicit time constraint. Discriminative: Tests applied SRE knowledge, trade-offs, and process design, strongly separates experienced Staff Engineers from juniors.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-490",
        "role_title": "Senior Backend Engineer - Environment Management",
        "question": "**Question:**  \nIn our environment management system for preview deployments, we're using Kubernetes namespaces for isolation, with resource quotas and TTL-based cleanup via a Go service. Describe a practical approach you'd take in Go to implement the TTL cleanup logic\u2014specifically, how would you efficiently identify and delete namespaces that have exceeded their TTL (e.g., 24 hours), while minimizing API server load and handling potential race conditions? Include key code snippets or pseudocode for the core loop or query logic.\n\n*(Expected response: Candidate outlines a cron-like scheduler or periodic reconciliation loop using Kubernetes client-go/informers for efficient watching/listing of annotated namespaces, filtering by creation timestamp + TTL annotation, then graceful deletion with finalizers or preconditions. Senior candidates might mention leader election, rate limiting, backoff retries, or integration with custom resources.)*",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core focus on namespace isolation, quotas, and TTL cleanup in Kubernetes/Go (-0 red flags). Clarity: Crystal clear with specific scope (TTL logic, efficiency, race conditions), well-formed for screening; minor deduction for lacking exact time constraint. Discriminative: Tests applied deep knowledge, trade-offs (API load, races), distinguishes seniors via advanced concepts like leader election; not trivial recall.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-494",
        "role_title": "Backend Engineer - API Mocking",
        "question": "**Question:**\n\nYou're setting up WireMock in a Docker container for mocking a PostgreSQL-based user service API during contract testing with TypeScript/Node.js. The API endpoint is `GET /users/:id`, which should return a templated JSON response like `{ \"id\": \"{{request.path.[1]}}\", \"name\": \"John Doe\", \"email\": \"{{randomEmail}}\" }` only if the request includes a valid `Authorization: Bearer <token>` header.\n\nProvide a concise code snippet (or docker-compose.yaml + stubs config) showing:\n1. How to run WireMock in Docker.\n2. How to configure the stub for header-based request matching and response templating.\n\n(Expect ~3-5 lines of key config; explain any non-obvious parts briefly.)",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests Docker, WireMock, TypeScript/Node.js context, templating, and header matching\u2014core to API mocking infrastructure (-1 for not explicitly covering contract verification). Clarity: Precise scope, constraints (3-5 lines), and expectations make it unambiguous. Discriminative: Requires applied knowledge of WireMock config, Docker integration, and templating syntax to distinguish proficient candidates from those with superficial familiarity.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-495",
        "role_title": "Senior Backend Engineer - Audit Logging",
        "question": "**Question:**  \nIn Go, design a simple tamper-proof audit logging function that appends a log entry (e.g., `{userID: string, action: string, timestamp: time.Time}`) to a Kafka topic, ensuring immutability and integrity against tampering. Include hashing for tamper detection and briefly explain how you'd extend it for ClickHouse storage with retention policies. Provide code snippet and rationale (2-3 sentences).",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core audit logging skills (tamper-proofing, hashing) with Go (likely relevant), Kafka for streaming, and ClickHouse for storage/retention per role (-1 for assuming Go/Kafka not explicitly listed). Clarity: Well-scoped with specific log structure, requirements, code snippet request, and brief extension; no ambiguity. Discriminative: Requires applied design (hashing, Kafka producer), code, trade-offs for immutability, and extension thinking, separating strong candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-492",
        "role_title": "Backend Engineer - CD Pipelines",
        "question": "**Question:**  \nIn an ArgoCD GitOps setup using Helm charts for a multi-environment deployment (e.g., dev, staging, prod), describe a practical approach to implement automated promotion from staging to prod, including a manual approval gate and a rollback mechanism if the deployment fails health checks. What Go-based tool or custom controller might you use to automate the rollback, and why?  \n\n*(Expected answer outline: Use ArgoCD App of Apps or ApplicationSets with environment-specific Helm values; implement approval via ArgoCD's resource hooks or integrate with GitHub Actions for manual gates; for rollback, leverage Argo Rollouts with AnalysisTemplates for health checks or a custom Go controller using the ArgoCD API/client-go to detect failures and trigger `argocd app rollback`. Tests GitOps/CD pipeline design at mid-level.)*",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core GitOps/CD pipeline skills (ArgoCD, Helm, multi-env, approvals, rollbacks) from role; minor ding for Go tool not explicitly listed (-1). Clarity: Precise, well-scoped with context, no ambiguity. Discriminative: Requires applied design knowledge, trade-offs (hooks vs Actions, Rollouts vs custom), distinguishes mid-level expertise from basics.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-499",
        "role_title": "Principal Engineer - Search Platform",
        "question": "**Question:**  \nAs a Principal Engineer leading our search platform, imagine our enterprise search service\u2014built on Elasticsearch, Kafka for event streaming, and Redis for caching\u2014is experiencing increased tail latencies during peak traffic, impacting user experience. Walk me through your step-by-step approach to diagnose and resolve this issue, highlighting key trade-offs in scaling decisions you'd consider for long-term platform strategy.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core search platform leadership skills with exact tech stack (Elasticsearch, Kafka, Redis) from role; minor genericism (-1). Clarity: Crystal clear scenario, step-by-step scoping, no ambiguities. Discriminative: Probes applied diagnostics, resolution, and strategic trade-offs, strongly separating experienced principals from juniors.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-491",
        "role_title": "Junior Backend Engineer - Read Replicas",
        "question": "**Question:**\n\nYou're building a Python service using SQLAlchemy and PostgreSQL with a primary database and one read replica. You notice that some customer-facing queries (e.g., fetching a user's recent orders) are occasionally returning stale data due to replication lag on the read replica.\n\nIn 2-3 sentences, explain how you would modify your code to route **only these customer queries** to the primary database while sending all other read-heavy queries (e.g., analytics) to the read replica. Include a brief code snippet showing the routing logic (using SQLAlchemy sessions).",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Directly tests core role skills (read replica routing, replication lag handling) with exact tech stack (Python, SQLAlchemy, PostgreSQL); crystal clear scope (2-3 sentences, specific query types, code snippet); evaluates applied knowledge of conditional routing and trade-offs, distinguishing candidates who understand session management from those reciting basics. Minor deduction for no exact role match on 'customer queries,' but highly targeted.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-496",
        "role_title": "Junior Backend Engineer - Model Serving",
        "question": "**Question:**  \nYou're building a FastAPI endpoint to serve a pre-trained scikit-learn model for predicting house prices from JSON input (e.g., `{\"area\": 1000, \"rooms\": 3}`). The model is saved as a `joblib` file named `house_model.joblib`.\n\nWrite a minimal FastAPI code snippet (including imports) that:  \n1. Loads the model once at startup.  \n2. Accepts a POST request at `/predict` with JSON input.  \n3. Returns the prediction as JSON (e.g., `{\"prediction\": 250000.0}`).\n\nInclude a simple Pydantic model for input validation. No need for error handling or extras\u2014just the core working code.",
        "scores": {
          "relevance": 9,
          "clarity": 10,
          "discriminative": 8,
          "reasoning": "Relevance: Directly tests FastAPI (API deployment), scikit-learn/joblib (ML model loading), Pydantic (validation)\u2014core to role; minor ding as role emphasizes batching/monitoring not tested (-1). Clarity: Crystal clear, well-scoped with explicit requirements, constraints, and example formats. Discriminative: Requires applied integration of multiple tools with correct startup loading and validation; distinguishes those who can write working code from theorists.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-497",
        "role_title": "Backend Engineer - Payout Systems",
        "question": "**Question:**  \nIn our payout system, we process daily batches of marketplace settlements (e.g., 10k+ transfers) using Go and PostgreSQL, with Kafka for event streaming and Redis for caching. Walk me through how you'd design the batch processing flow to ensure exactly-once delivery of payouts, including handling duplicates, failures, and reconciliation with bank responses. What key Go libraries or patterns (e.g., idempotency) would you use, and how would you leverage Kafka/Redis/PostgreSQL?",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Relevance: Perfectly matches role's core focus on payout batch processing, settlements, validation, reconciliation, and exact tech stack (Go, PostgreSQL, Kafka, Redis). No red flags. Clarity: Crystal clear, well-scoped to batch flow with specific elements; minor deduction as no explicit time constraint for screening. Discriminative: Tests deep applied knowledge, trade-offs (duplicates, failures, exactly-once), and role-specific integration, strongly separating strong candidates.",
          "composite": 9.666666666666666
        }
      },
      {
        "role_id": "be-498",
        "role_title": "Senior Backend Engineer - Profiling Infrastructure",
        "question": "**Question:**  \nIn Pyroscope, you've collected a flame graph for a Go service showing high CPU usage in a specific goroutine. Walk me through your step-by-step process to investigate, optimize, and validate the performance issue using Pyroscope's features (e.g., baseline comparison, diffing) and integrate the fix into a Kubernetes-deployed service with Grafana dashboards for ongoing monitoring. What key Pyroscope CLI or UI commands would you use, and how would you store/query the profiling data efficiently in ClickHouse?",
        "scores": {
          "relevance": 3,
          "clarity": 6,
          "discriminative": 7,
          "reasoning": "Relevance: Superficially mentions performance analysis but assumes Pyroscope/ClickHouse expertise not in role description (build infra, flame graphs generically); red flags: tests unlisted techs (-2), generic perf process (-3). Clarity: Understandable but vague on expected depth/scope for screening, bundles multiple steps without constraints (-2). Discriminative: Tests applied knowledge well but too scenario-specific and broad (>10min, -2).",
          "composite": 5.333333333333333
        }
      },
      {
        "role_id": "be-500",
        "role_title": "Staff Engineer - Streaming Architecture",
        "question": "**Question:**  \nYou're designing a real-time user activity feed using Kafka, Redis, and WebSockets in Rust on Kubernetes. Describe how you'd implement exactly-once fan-out delivery from a single Kafka topic to 10,000+ concurrent WebSocket connections, including your approach to event schemas, deduplication, and scaling across K8s pods. What trade-offs would you make for latency vs. durability?",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Relevance: Directly targets core role skills (streaming infra, event schemas, fan-out, delivery guarantees) with Kafka (implied in streaming) and Kubernetes; minor ding for unmentioned Redis/WebSockets/Rust (-1). Clarity: Well-scoped, specific constraints, unambiguous intent for screening depth. Discriminative: Probes deep applied knowledge, trade-offs (latency/durability), architecture decisions, strongly separates experts.",
          "composite": 9.333333333333334
        }
      }
    ]
  },
  "claude-4-5-haiku": {
    "model": "claude-4-5-haiku",
    "num_samples": 100,
    "avg_relevance": 9.15,
    "avg_clarity": 8.86,
    "avg_discriminative": 9.02,
    "avg_composite": 9.010000000000003,
    "results": [
      {
        "role_id": "be-406",
        "role_title": "Senior Backend Engineer - LLM Serving",
        "question": "# Technical Screening Question\n\n**Describe a scenario where you'd need to tune the batch size and max_tokens parameters in vLLM, and walk me through your approach to finding the right balance. What metrics would you monitor, and how would you handle a situation where increasing batch size actually decreases throughput?**\n\n---\n\n## Why This Question Works:\n\n**Practical & Role-Specific:**\n- Directly relevant to the core responsibility (batching optimization)\n- Tests real-world decision-making, not memorization\n\n**Senior-Level Appropriate:**\n- Requires systems thinking (throughput vs. latency tradeoffs)\n- Expects knowledge of multiple failure modes\n- Assesses debugging methodology\n\n**Time-Bound:**\n- Can be answered substantively in 2-5 minutes\n- Allows follow-ups to probe deeper if needed\n\n**Reveals Key Competencies:**\n- Understanding of LLM serving bottlenecks (memory, compute, I/O)\n- Familiarity with vLLM's batching mechanics\n- Monitoring/observability mindset\n- Problem-solving approach under constraints\n\n---\n\n## What You're",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core focus on batching optimization and LLM serving with vLLM (no red flags). Clarity: Crystal clear and well-scoped for screening, minor deduction for lacking explicit time constraint. Discriminative: Tests deep applied knowledge, trade-offs, debugging, and metrics\u2014strong separation of senior talent from juniors.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-409",
        "role_title": "Junior Backend Engineer - File Upload",
        "question": "# Screening Question\n\n**You're building a file upload endpoint in FastAPI that accepts image files and stores them in S3. The endpoint needs to:**\n- Accept files up to 10MB\n- Validate that only PNG and JPG files are allowed\n- Return the S3 URL where the file was stored\n\n**Write the code for this endpoint, including:**\n1. The route definition with appropriate parameters\n2. File validation logic\n3. Basic S3 upload logic (you can use pseudocode/boto3 calls)\n\n---\n\n## Evaluation Criteria\n\n\u2713 **Strong answer includes:**\n- Proper use of `UploadFile` from FastAPI\n- File size and extension validation\n- Correct S3 upload implementation (boto3 `put_object` or similar)\n- Appropriate error handling/HTTP status codes\n- Returns the S3 URL\n\n\u26a0\ufe0f **Common gaps to listen for:**\n- Forgetting to validate file size before upload\n- Not checking file extensions or doing it insecurely\n- Missing error handling for S3 operations\n- Not understanding async/await patterns in FastAPI\n\n**Follow-up if",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Directly tests core file upload API skills (multipart handling, validation, S3 integration) matching role; crystal clear with precise scope, constraints, and expectations; strong separation via applied FastAPI/S3 knowledge, validation pitfalls, and error handling\u2014reveals practical ability beyond trivia.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-402",
        "role_title": "Backend Engineer - Model Deployment",
        "question": "# Technical Screening Question\n\n**You need to deploy two versions of a machine learning model to production simultaneously\u2014a stable v1 serving 90% of traffic and a new v2 serving 10% for canary testing. Both models should be accessible through a single endpoint.**\n\n**How would you implement this using KServe, and what are the key components you'd need to configure?**\n\n---\n\n## Why This Question Works:\n\n\u2705 **Practical relevance** \u2014 Directly addresses the core responsibility (deployment strategies)\n\n\u2705 **Mid-level appropriate** \u2014 Requires KServe knowledge but doesn't demand expert-level optimization details\n\n\u2705 **Reveals competency across the stack** \u2014 Tests understanding of:\n- KServe InferenceService configuration\n- Traffic splitting concepts\n- Kubernetes service routing\n- Model versioning\n\n\u2705 **Timeboxed** \u2014 Candidates can outline the approach in 2-3 minutes, with room for follow-ups on implementation details\n\n## Possible Follow-ups:\n- \"How would you monitor metrics to decide when to shift more traffic to v2?\"\n- \"What happens if v2 has high lat",
        "scores": {
          "relevance": 7,
          "clarity": 9,
          "discriminative": 8,
          "reasoning": "Relevance: Tests core deployment strategies but KServe not mentioned in role description (-2); not generic (-0). Clarity: Crystal clear, well-scoped with specific scenario and components; no red flags. Discriminative: Tests applied knowledge of configuration and trade-offs like traffic splitting, good separation for mid-level roles; timeboxed appropriately.",
          "composite": 8.0
        }
      },
      {
        "role_id": "be-403",
        "role_title": "Backend Engineer - Inference Optimization",
        "question": "# Screening Question\n\n**You're optimizing an inference pipeline for a large language model that's hitting latency SLAs in production. The model runs on a single GPU, and you're seeing:**\n- P95 latency: 850ms (SLA: 500ms)\n- GPU memory utilization: 85%\n- Average batch size: 4\n\n**Walk me through your approach to identify the bottleneck and propose 2-3 optimization strategies. What would you measure first, and why?**\n\n---\n\n## Why This Question Works\n\n**Practical scope:** Tests real-world problem-solving without requiring deep implementation details\n\n**Assesses:**\n- Systems thinking (recognizing multiple potential bottlenecks: compute, memory, batching)\n- Prioritization (what to measure first with limited time)\n- Knowledge of core optimization techniques (quantization, batching, memory optimization)\n- Understanding of the inference stack (where TensorRT/Triton would fit)\n\n**Level-appropriate:** A mid-level engineer should know *which* tools exist and *when* to use them, without needing to implement from scratch\n\n**Follow-ups available:**",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core role skills (quantization, batching, GPU memory) in realistic inference optimization scenario (-0 for no red flags). Clarity: Well-scoped with specific metrics/constraints, unambiguous intent for screening. Discriminative: Requires prioritization, systems thinking, and trade-off knowledge to distinguish strong mid-level candidates from weak ones.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-401",
        "role_title": "Backend Engineer - Training Infrastructure",
        "question": "# Screening Question\n\n**You're designing a Ray cluster that trains models on a dataset too large for a single GPU. Walk me through how you would structure the data loading and training code to efficiently distribute work across multiple GPUs. What are the key things you'd consider to avoid bottlenecks?**\n\n---\n\n## Why This Works\n\n\u2705 **Practical & Real** - Directly mirrors the job's core responsibility  \n\u2705 **Right Difficulty** - Mid-level should know distributed basics, but nuances separate strong candidates  \n\u2705 **Open-Ended** - Reveals depth: data parallelism strategies, Ray actor patterns, bottleneck awareness, Kubernetes resource requests  \n\u2705 **Time-Boxed** - Answerable in 3-4 minutes without being trivial  \n\u2705 **Follow-ups Available** - Can probe deeper on Ray task scheduling, fault tolerance, or GPU memory management\n\n**What you're listening for:**\n- Understanding of data sharding/batching across workers\n- Awareness of I/O bottlenecks (data loading shouldn't block training)\n- Mention of Ray's distributed data or actor model",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core distributed ML training skills with Ray, matching job's focus on GPU allocation and orchestration (-0 red flags). Clarity: Crystal clear, well-scoped for screening, specific context avoids ambiguity. Discriminative: Probes applied knowledge of data parallelism, bottlenecks, and Ray patterns, revealing trade-offs and depth to separate strong candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-404",
        "role_title": "Senior Backend Engineer - Feature Platform",
        "question": "# Technical Screening Question\n\n**Scenario-based question:**\n\n\"You're designing a feature store where ML teams need both batch features (computed daily) and real-time features (computed on-request). Walk me through how you'd handle a situation where a real-time feature depends on aggregations from a batch feature\u2014for example, a user's purchase count from the last 30 days.\n\nWhat would be your approach to:\n1. Computing and storing this in Feast\n2. Ensuring consistency between batch and real-time paths\n3. Handling the latency tradeoff\"\n\n---\n\n## Why this works:\n\n\u2705 **Practical** \u2013 Tests real architectural decisions in feature platforms  \n\u2705 **Seniority-appropriate** \u2013 Expects systems thinking, not just API knowledge  \n\u2705 **Tech-specific** \u2013 Naturally brings up Feast concepts (batch sources, online stores, feature views)  \n\u2705 **Open-ended** \u2013 Senior candidates can discuss tradeoffs (Redis consistency, staleness, compute costs)  \n\u2705 **Timeboxed** \u2013 Can be answered thoroughly in 3-4 minutes with follow-ups",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core feature store skills (batch/real-time computation, dependencies) for ML platform role using Feast (implied in context); minor ding for not quoting role's exact tech but perfectly aligned (-1 red flag avoided). Clarity: Well-scoped scenario with 3 focused subpoints, unambiguous, no bundling issues. Discriminative: Probes trade-offs (consistency, latency), systems thinking, and Feast application, strongly separates seniors from juniors.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-410",
        "role_title": "Backend Engineer - Bulk Operations",
        "question": "# Screening Question\n\n**Design a bulk update API endpoint that processes up to 10,000 records. Walk me through how you'd handle:**\n\n1. **Partial failures** \u2014 some records succeed, others fail validation or hit conflicts\n2. **Idempotency** \u2014 the same request submitted twice shouldn't create duplicate side effects\n3. **Client feedback** \u2014 how would you return results to the caller?\n\n**You can use pseudocode or describe your approach. What would be your main design decisions?**\n\n---\n\n## Why This Question Works\n\n- **Practical:** Tests real-world bulk operation challenges they'd face on day one\n- **Reveals depth:** Answers expose knowledge of idempotency keys, transaction boundaries, error handling patterns, and API design\n- **Mid-level appropriate:** Doesn't require exotic knowledge; a solid mid-level engineer should have encountered these problems\n- **Conversational:** Opens dialogue\u2014you can dig into their Redis/Kafka choices, PostgreSQL transaction strategy, or Go concurrency patterns based on their answer\n- **Time-boxed:** Candidates can sketch a solid answer in 2-5 minutes",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core needs (bulk ops, partial success, idempotency). Clarity: Crystal clear with specific scope (10k records), well-scoped for screening; minor deduction for no explicit time/format constraint. Discriminative: Tests applied design trade-offs (idempotency keys, transactions, error aggregation), reveals depth without being trivial or impossible.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-405",
        "role_title": "Senior Backend Engineer - Model Monitoring",
        "question": "# Screening Question\n\n**You're implementing a monitoring system for a production recommendation model. Over the past week, your metrics show the model's precision dropped from 0.92 to 0.78, but the feature distributions look virtually identical to training data. What are the most likely causes you'd investigate first, and how would you approach diagnosing this systematically?**\n\n---\n\n## Why This Question Works\n\n**Tests practical expertise:**\n- Distinguishes between data drift vs. model/prediction drift (common senior-level insight)\n- Requires knowledge of what monitoring actually catches and what it misses\n- Shows systematic debugging approach for production issues\n\n**Appropriate for seniority:**\n- Not a gotcha question\u2014open-ended to reveal depth of experience\n- Senior engineer should discuss multiple hypotheses, not just one answer\n- Tests judgment about what to prioritize (efficiency matters in production)\n\n**Realistic scenario:**\n- Common real-world situation that trips up junior engineers\n- Directly relevant to the role's core responsibility\n\n**Reveals:**\n- Model monitoring maturity (do they know feature drift \u2260 performance drift?)\n- Production troubleshooting experience\n- Whether they",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core focus on model monitoring, drift detection, and performance tracking; no red flags. Clarity: Crystal clear scenario with precise metrics, well-scoped for screening; minor deduction as no explicit time/format constraint. Discriminative: Tests deep applied knowledge of drift types, prioritization, and systematic diagnosis, strongly separates seniors from juniors via trade-offs and hypotheses.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-407",
        "role_title": "Staff Backend Engineer - ML Infrastructure",
        "question": "# Screening Question\n\n**You're tasked with designing a training platform where data scientists can submit jobs that need to scale from a single GPU to 100+ GPUs across a cluster. Walk me through how you'd approach this problem\u2014what are the key architectural decisions you'd make, and what trade-offs would you consider?**\n\n---\n\n## Why This Works:\n\n**Tests practical, staff-level expertise:**\n- Requires understanding of distributed systems (Ray/Spark fundamentals)\n- Assesses platform thinking beyond just coding\n- Reveals experience with real scaling challenges\n- Tests cost/complexity trade-offs (staff engineers optimize for this)\n\n**Appropriate for seniority:**\n- Not a gotcha question; experienced engineers have concrete opinions\n- Open-ended enough to see their decision-making process\n- Allows them to showcase leadership perspective (defining standards, trade-offs)\n\n**Clear signal in the answer:**\n- Do they mention fault tolerance, checkpointing, resource scheduling?\n- Do they think about DX (data scientist experience)?\n- Do they consider observability, cost monitoring?\n- Do they acknowledge Ray/Spark trade-offs knowingly?\n\n**Time-",
        "scores": {
          "relevance": 7,
          "clarity": 6,
          "discriminative": 8,
          "reasoning": "Relevance: Tests core ML infra scaling and platform design directly matching role, but generic distributed training (no specific tech stack like role implies) and lacks cost optimization emphasis (-1 generic, -1 misses specifics). Clarity: Clear intent for architectural approach but 'design a system' without time/format/constraints/scope (e.g., team size, budget) (-2 red flag), vague depth. Discriminative: Strong on trade-offs, applied knowledge, and deep signals like fault tolerance/DX/observability, good separation for staff-level.",
          "composite": 7.0
        }
      },
      {
        "role_id": "be-408",
        "role_title": "Junior Backend Developer - Health Endpoints",
        "question": "# Screening Question\n\n**Describe the difference between a liveness probe and a readiness probe in Kubernetes. Why would you implement them differently, and what might each endpoint return in Go?**\n\n---\n\n## Why This Question Works:\n\n\u2713 **Practical & Role-Relevant:** Directly addresses the core responsibility (health endpoints)\n\n\u2713 **Right Difficulty Level:** Junior developers should know this\u2014it's foundational Kubernetes knowledge, not advanced\n\n\u2713 **Multi-Part Assessment:** Tests understanding of:\n- Kubernetes concepts (liveness vs readiness)\n- Implementation reasoning (why they differ)\n- Go coding ability (what the endpoints look like)\n\n\u2713 **2-5 Minute Response:** Candidates can sketch a quick answer without deep system design\n\n\u2713 **Open-Ended:** Reveals depth of understanding\u2014a junior might say \"they're different states,\" while someone stronger explains cascading failures or dependency checks\n\n---\n\n## What You're Looking For:\n\n- **Good answer:** Explains that liveness = \"is the app running?\" and readiness = \"can it handle traffic?\"\n- **Better answer:** Mentions that readiness checks",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core focus on health check endpoints (liveness/readiness) in Kubernetes/Go. Clarity: Crystal clear, well-scoped multi-part question for screening, minor vagueness on exact Go return format but not deducting. Discriminative: Excellent separation via conceptual understanding, reasoning (trade-offs like cascading failures), and implementation; reveals depth without being recall or trivial. No red flags.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-411",
        "role_title": "Backend Engineer - Admin APIs",
        "question": "# Technical Screening Question\n\n## Design an audit logging system for an admin API\n\n**Scenario:**\nYou're building an admin API where privileged operations (user suspension, permission changes, data exports) must be logged for compliance. The system needs to:\n- Record *what* action was performed, *who* performed it, and *when*\n- Handle high volume (thousands of operations/day)\n- Allow querying audit logs by user, action type, and date range\n- Ensure logs cannot be tampered with\n\n**Your task:**\n1. Sketch a database schema for storing audit logs\n2. Explain how you'd integrate logging into your FastAPI endpoints (show a brief code example or pseudocode)\n3. Discuss one trade-off you'd make between completeness and performance\n\n---\n\n## What This Tests\n\n\u2713 **Practical API design** - How they structure data for a real constraint  \n\u2713 **Python/FastAPI knowledge** - Middleware, decorators, or dependency injection patterns  \n\u2713 **Database thinking** - Schema design, indexing, query patterns for compliance use cases  \n\u2713 **Mid-level judgment** - Trade-",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core role needs (audit logging for admin APIs), tests FastAPI (implied in role context), no red flags (-0). Clarity: Well-scoped scenario with specific requirements, single focused design task with clear subtasks, no bundling issues. Discriminative: Requires applied schema design, integration knowledge, and trade-off analysis, strongly separates mid-level thinkers via practical judgment.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-413",
        "role_title": "Senior Backend Engineer - API Gateway Logic",
        "question": "# Screening Question\n\n**You're implementing a custom Envoy filter in Go that needs to:**\n1. Extract a user ID from request headers\n2. Apply different rate limits based on user tier (stored in a shared cache)\n3. Add response headers with latency metrics\n4. Reject requests that exceed their tier's limit with a 429 status\n\n**Walk me through your approach:** How would you structure this filter? What are the key extension points in Envoy you'd use, and how would you handle the cache lookup without blocking the request path?\n\n---\n\n## Why This Question Works\n\n\u2713 **Practical & Real** \u2014 Actual API gateway use case (rate limiting, header manipulation, caching)  \n\u2713 **Senior-Level** \u2014 Requires understanding of Envoy architecture, concurrency patterns, and performance trade-offs  \n\u2713 **Tech Stack** \u2014 Tests knowledge of Envoy filters and Go integration  \n\u2713 **Concise** \u2014 Answerable in 3-4 minutes with room for follow-ups  \n\u2713 **Reveals Depth** \u2014 Response shows whether they understand filter lifecycle, async operations, and gateway",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests Envoy filters, Go, request transformation, routing logic, and caching - core to API gateway extensions role (-0 red flags). Clarity: Precise tasks with specific non-blocking constraint, unambiguous for screening scope. Discriminative: Probes architecture, concurrency trade-offs, and Envoy extension points, strongly separates experienced candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-412",
        "role_title": "Backend Engineer - Analytics APIs",
        "question": "# Technical Screening Question\n\n**You're building an analytics API endpoint that returns daily revenue aggregations for the past year. The query currently takes 8 seconds, which is too slow for dashboard load times. Walk me through your approach to optimize this\u2014what would you investigate first, and what specific strategies would you consider?**\n\n---\n\n## Why This Question Works:\n\n**Practical Relevance:** Directly mirrors the core responsibility of aggregating and caching time-series data\n\n**Seniority Appropriate:** \n- Mid-level engineers should recognize multiple optimization layers (query-level, caching, architecture)\n- Not expecting mastery of all, but strategic thinking across the stack\n\n**Reveals Key Competencies:**\n- Understanding of ClickHouse query optimization (sampling, pre-aggregation, partitioning)\n- Redis caching strategy decisions (TTL, key design, invalidation)\n- Go API design (pagination, response formatting)\n- Diagnostic thinking (where to start troubleshooting)\n\n**Time Efficient:** Candidate can outline approach in 2-3 minutes, with room for follow-up depth based on their answers\n\n**Natural Follow-ups:**",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (aggregation queries, time-series, caching) without generic flaws (-0 red flags). Clarity: Crystal clear scenario, well-scoped for screening, no ambiguity or missing constraints. Discriminative: Probes diagnostic thinking, trade-offs (query opt, caching strategies), distinguishes strategic depth from superficial answers; 2-3 min response fits perfectly.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-415",
        "role_title": "Senior Backend Engineer - Multipart APIs",
        "question": "# Screening Question\n\n**You're designing a resumable upload API for large files (up to 10GB). A client uploads a file in 5MB chunks, but the connection drops after chunk 3. When the client reconnects, how would you:**\n\n1. **Identify which chunks were successfully persisted?**\n2. **Handle the case where chunk 4 was partially written to S3?**\n3. **Decide whether to use S3's multipart upload API or manage chunks yourself?**\n\n**What trade-offs would you consider between complexity, cost, and reliability?**\n\n---\n\n## Why This Works\n\n\u2705 **Practical & Real** \u2014 Directly mirrors the job responsibility  \n\u2705 **Senior-level** \u2014 Requires architectural thinking, not just syntax  \n\u2705 **Concise** \u2014 Can be answered in 3-4 minutes with good follow-ups  \n\u2705 **Multi-faceted** \u2014 Tests state management, AWS knowledge, and trade-off analysis  \n\u2705 **Room to dig deeper** \u2014 You can probe their reasoning on idempotency, metadata storage, cleanup logic, etc.",
        "scores": {
          "relevance": 7,
          "clarity": 7,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core resumable upload/chunk skills but assumes S3/AWS (not explicitly listed, -2) and bundles multiple sub-questions (-1 from red flags). Clarity: Clear scenario but multiple bundled questions (-2) make it less focused for screening. Discriminative: Excellent on trade-offs, applied AWS knowledge, and edge cases (partial writes, resumption), strongly separates seniors.",
          "composite": 7.666666666666667
        }
      },
      {
        "role_id": "be-416",
        "role_title": "Staff Backend Engineer - External APIs",
        "question": "# Technical Screening Question\n\n**You're designing an external API for partners that need real-time access to customer transaction data. Some partners are high-volume, others are occasional users. Walk me through how you'd approach the rate limiting and monetization strategy for this API\u2014what would you consider, and what trade-offs would you make?**\n\n---\n\n## Why This Question Works:\n\n**Tests practical, staff-level thinking:**\n- Requires understanding of API gateway capabilities (Kong is their tech stack)\n- Demonstrates monetization strategy knowledge (key requirement)\n- Shows partner enablement thinking (tiered access, fairness)\n- Reveals architectural decision-making at scale\n\n**Allows for 2-5 minute response** covering:\n- Rate limiting approaches (fixed vs. token bucket, per-tier quotas)\n- Monetization models (usage-based, tier-based, hybrid)\n- Technical implementation (where enforcement happens\u2014Kong, application layer)\n- Trade-offs (complexity vs. fairness, cost recovery vs. partner friction)\n\n**Differentiates staff-level candidates** by how they discuss:\n- Partner friction and business implications\n- System design trade-offs",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core API strategy, monetization, and partner enablement; aligns perfectly with role despite not naming Kong (-1 minor). Clarity: Crystal clear, well-scoped for screening with explicit focus on rate limiting/monetization and trade-offs. Discriminative: Excellent at separating staff-level candidates via trade-offs, business implications, and architectural depth; not recall-based.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-418",
        "role_title": "Junior Backend Engineer - Service Clients",
        "question": "# Screening Question\n\n**You're building a gRPC client that calls a downstream service. The service occasionally times out or returns temporary errors. Walk me through how you'd implement a basic retry mechanism in Go. What would you consider to prevent making things worse?**\n\n---\n\n## Why This Question Works\n\n\u2705 **Practical & Real** \u2013 Junior engineers actually build this  \n\u2705 **Right Scope** \u2013 Tests Go basics + distributed systems thinking in 2-5 min  \n\u2705 **Reveals Depth** \u2013 Answer shows if they think beyond \"just retry\":\n- Exponential backoff vs fixed delays\n- Max retries / circuit breaker concepts\n- Idempotency awareness\n- Timeout configuration\n\n\u2705 **Appropriate Level** \u2013 Doesn't require advanced patterns, but rewards good thinking\n\n---\n\n## What You're Listening For\n\n**Strong signals:**\n- Mentions exponential backoff or jitter\n- Considers max retry limits\n- Thinks about idempotency (\"don't retry if it's not safe\")\n- References context deadlines in Go\n\n**Red flags:**\n- \"Just retry forever\"\n- No mention",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests Go implementation of retries for service clients, core to role; minor ding as gRPC unspecified but aligns perfectly (-1 no red flags). Clarity: Crystal clear, well-scoped for screening, single focused question with practical context. Discriminative: Excellent at separating via trade-offs (backoff, idempotency, limits), reveals depth without being trivial or overly broad.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-420",
        "role_title": "Backend Engineer - Graceful Shutdown",
        "question": "# Screening Question\n\n**You're deploying a Go backend service to Kubernetes that handles both gRPC and HTTP requests. During a rolling update, you need to ensure in-flight requests complete gracefully before the pod terminates. Walk me through how you'd implement graceful shutdown\u2014specifically, how you'd handle the signal, drain new connections, and wait for existing requests to finish. What are the key timing considerations with Kubernetes' termination lifecycle?**\n\n---\n\n## Why This Works\n\n\u2705 **Practical & Real**: Directly addresses the role's core responsibility  \n\u2705 **Right Scope**: Answerable in 3-4 minutes without being trivial  \n\u2705 **Tests Understanding**: Requires knowledge of:\n- Go signal handling (`context.Context`, `syscall.SIGTERM`)\n- Kubernetes termination flow (SIGTERM \u2192 grace period \u2192 SIGKILL)\n- Connection/request draining mechanics\n- Trade-offs (grace period timeout vs. request latency)\n\n\u2705 **Mid-Level Appropriate**: Expects practical implementation knowledge, not architectural perfection\n\n---\n\n## Strong Answer Signals\n\n- Mentions `signal.",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core focus on graceful shutdown, signals, and in-flight requests in Go/K8s context (+no red flags). Clarity: Crystal clear, well-scoped for screening, specific components listed (minor deduction for lacking explicit time/format constraints). Discriminative: Tests applied knowledge, trade-offs, and integration of Go signals/K8s lifecycle, strongly separates candidates with practical depth.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-419",
        "role_title": "Backend Engineer - Retry Logic",
        "question": "# Screening Question\n\n**You're designing a retry mechanism for failed Kafka message processing in your Go service. Messages that fail processing should be retried with exponential backoff, but you also need to prevent retry storms that could overwhelm your system.**\n\n**Walk me through how you would implement exponential backoff with jitter, and explain how you'd implement a retry budget to prevent cascading failures. What would you consider when setting these parameters?**\n\n---\n\n## Why This Question Works\n\n**Tests practical distributed systems knowledge:**\n- Exponential backoff implementation (core skill)\n- Jitter concept (prevents thundering herd)\n- Retry budgets (prevents cascading failures)\n- Parameter tuning reasoning (judgment at mid-level)\n\n**Appropriate for mid-level:**\n- Not asking to code it end-to-end (that's senior)\n- Not trivia (they need to think through trade-offs)\n- Evaluates real architectural decisions they'd make\n\n**Reveals important signals:**\n- Do they understand *why* jitter matters?\n- Can they think about resource constraints?\n- Do they consider monitoring/observability?\n- Can they balance reliability",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core focus on retry infrastructure, backoff, jitter, and budgets in Go/Kafka context (+no red flags). Clarity: Crystal clear and well-scoped for screening, minor deduction for unspecified constraints on parameters. Discriminative: Tests deep trade-offs, judgment, and applied knowledge (why jitter, tuning), strong separation without being too broad.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-414",
        "role_title": "Senior Backend Engineer - Long-Polling",
        "question": "# Technical Screening Question\n\n**Design a long-polling endpoint in Go that handles async job status updates. Walk me through:**\n\n1. **How you'd structure the HTTP handler** to accept a poll request and avoid blocking the entire server\n2. **Your timeout strategy** \u2014 how long should you wait before responding, and how do you prevent client/server resource exhaustion?\n3. **How you'd integrate Redis** to check for job completion without hammering PostgreSQL\n4. **One concrete edge case** you'd need to handle and your solution for it\n\n---\n\n## Why This Works\n\n\u2705 **Practical & Scenario-Based** \u2014 Tests real architectural decisions, not memorization\n\n\u2705 **Multi-Layered** \u2014 Assesses Go concurrency, caching patterns, system design thinking, and edge case awareness\n\n\u2705 **Senior-Level** \u2014 Expects thoughtful tradeoffs (timeout tuning, resource management) rather than just \"here's the code\"\n\n\u2705 **Appropriate Scope** \u2014 Answerable with a whiteboard/verbal explanation in 3-4 minutes, room for depth if they know their stuff\n\n\u2705 **Role-",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (long-polling APIs, connection mgmt, timeouts, fallbacks) with Go, Redis, PostgreSQL integration; minor ding as Go not explicitly in desc (-1). Clarity: Well-structured, numbered parts with precise scope, no bundling issues. Discriminative: Probes trade-offs, architecture, integration, and edge cases for strong candidate separation; appropriately scoped for screening.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-417",
        "role_title": "Junior Backend Developer - Event Sourcing",
        "question": "# Screening Question\n\n**\"Walk me through how you would design an event handler for a `UserRegistered` event in an event sourcing system. Specifically:**\n- **What information would the event contain?**\n- **What would the handler do with that event?**\n- **How would you ensure the same event isn't processed twice?\"**\n\n---\n\n## Why This Question Works\n\n**Tests practical understanding:**\n- Event structure and design (domain knowledge)\n- Handler responsibility (core pattern)\n- Idempotency (distributed systems challenge)\n\n**Appropriate for junior level:**\n- No trick answers or obscure knowledge required\n- Can be answered with a simple, concrete example\n- Shows if they've thought about real implementation details\n\n**Conversational flow:**\n- Allows follow-ups (Kafka partitions? Database constraints? Projection updates?)\n- Easy to gauge depth without being overwhelming\n- Natural discussion about event sourcing fundamentals\n\n---\n\n## What You're Listening For\n\n\u2705 **Good signals:** Event payload clarity, handler side effects, mention of idempotency keys or constraints  \n\u26a0\ufe0f **Watch for:** Confusion between events an",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 8,
          "reasoning": "Relevance: Directly tests core event sourcing skills (event handlers, structure) for the role, no red flags (-0). Clarity: Crystal clear, well-scoped bullet points, unambiguous for screening (-0). Discriminative: Tests applied knowledge and trade-offs like idempotency, distinguishes practical understanding from rote knowledge, junior-appropriate (-0).",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-421",
        "role_title": "Senior Backend Engineer - Security Automation",
        "question": "# Screening Question\n\n**Describe how you would design a secrets rotation mechanism for database credentials across a Kubernetes cluster using HashiCorp Vault. Walk me through your approach, including:**\n\n1. **How would you trigger rotation?** (timing, events, manual vs. automated)\n2. **How would you handle in-flight connections** during rotation to minimize disruption?\n3. **What would you implement to prevent a failed rotation from leaving the system in an inconsistent state?**\n\n---\n\n## Why This Question Works\n\n\u2713 **Practical & Real-World** \u2014 directly relevant to the role's core responsibility of secrets management\n\n\u2713 **Senior-Level** \u2014 expects architectural thinking and understanding of failure modes, not just \"how to use Vault\"\n\n\u2713 **Multi-Dimensional** \u2014 tests knowledge of orchestration, state management, and operational concerns simultaneously\n\n\u2713 **Time-Appropriate** \u2014 can be answered substantively in 2-5 minutes with follow-ups available\n\n\u2713 **Tech-Stack Aligned** \u2014 naturally incorporates Vault, Kubernetes, and automation concepts\n\n---\n\n## What You're Evaluating\n\n- Understanding of secrets",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's secrets rotation focus, Vault/K8s alignment, no red flags. Clarity: Crystal clear with specific subpoints, minor deduction for lacking explicit time/format constraints. Discriminative: Tests deep architectural trade-offs, failure handling, and ops concerns, strongly separates seniors from juniors.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-422",
        "role_title": "Junior Backend Engineer - API Testing",
        "question": "# Technical Screening Question\n\n**Write a simple pytest test for a FastAPI endpoint that creates a user. The endpoint accepts a POST request with `name` and `email` fields, returns the created user with an `id`, and should return a 400 status code if `email` is missing.**\n\n**Requirements:**\n- Write the test code (you can assume the FastAPI app is imported as `app`)\n- Include any necessary test setup/fixtures you'd use\n- Explain what your test is validating\n\n---\n\n## Why This Question Works\n\n\u2705 **Practical & Role-Relevant:** Tests core skills (pytest, FastAPI, API testing)  \n\u2705 **Time-Appropriate:** Solvable in 2-5 minutes  \n\u2705 **Junior-Level:** Doesn't require advanced mocking or complex patterns  \n\u2705 **Reveals Understanding:** Shows familiarity with:\n- Test structure and assertions\n- HTTP status codes\n- Request/response handling\n- Basic test organization\n\n**Follow-ups** (if needed):\n- \"How would you test the database was actually updated?\"\n- \"What if you needed to test 10",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 8,
          "reasoning": "Relevance: Directly tests pytest, FastAPI, and API testing - core to entry-level API test engineer role; no red flags. Clarity: Crystal clear with specific endpoint, requirements, and scoped for screening; assumes app import avoids ambiguity. Discriminative: Tests applied knowledge of test structure, fixtures, assertions, and HTTP handling to separate juniors with practical skills from weak ones; not pure recall but lacks trade-offs for 10.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-423",
        "role_title": "Staff Engineer - Incident Response Platform",
        "question": "# Screening Question\n\n**Tell me about a time you designed or improved an on-call system or incident response process. Walk me through:**\n\n1. **What problem were you solving?** (alert fatigue, slow response times, unclear escalation, etc.)\n2. **What metrics or SLOs did you use to measure success?**\n3. **What was the hardest part to get right, and how did you handle the tradeoffs?**\n\n---\n\n## Why This Works\n\n\u2713 **Open-ended** \u2013 Reveals their actual incident response experience and maturity  \n\u2713 **Multi-layered** \u2013 Tests problem-solving, SLO thinking, and pragmatism  \n\u2713 **Staff-level signal** \u2013 Staff engineers own systems holistically; this reveals whether they think about metrics, tradeoffs, and operational reality  \n\u2713 **Conversational** \u2013 Natural follow-ups emerge (escalation logic, automation, tooling choices, team buy-in)  \n\u2713 **Tech-agnostic** \u2013 Their answer matters more than specific tools they've used  \n\n**What you're listening for:**\n- Did they measure the impact",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Directly targets core role responsibilities (incident response, on-call optimization, SLO tracking) with behavioral format testing holistic ownership. Crystal clear structure with specific prompts avoids ambiguity. Strong differentiation via tradeoffs, metrics, and real-world problem-solving; staff-level depth without being generic or recall-based. Minor ding for behavioral over pure technical design but aligns perfectly with role's operational focus.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-425",
        "role_title": "Senior Backend Engineer - Graph Analytics",
        "question": "# Technical Screening Question\n\n**You're building a recommendation engine that needs to find \"friends of friends\" connections across a social network with 500 million users and 10 billion edges in Neo4j. A naive Cypher query doing multi-hop traversals is timing out.**\n\n**Walk me through how you'd approach optimizing this:**\n1. What specific Neo4j performance issues would you investigate first?\n2. Would you consider moving this workload to Spark, and if so, when/why?\n3. How would you use caching (Redis) strategically here without creating stale data problems?\n\n---\n\n## Why This Question Works\n\n\u2713 **Practical & Real** - Directly mirrors the job's core challenge (billion-edge scale, recommendation systems)\n\n\u2713 **Multi-layered** - Tests understanding of Neo4j query optimization, Spark's role in the stack, and caching strategy trade-offs\n\n\u2713 **Senior-level** - Requires architectural thinking beyond syntax; expects them to weigh trade-offs, not just recite solutions\n\n\u2713 **Time-appropriate** - Can be answered in 3-4 minutes with a",
        "scores": {
          "relevance": 7,
          "clarity": 7,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core graph traversal/rec skills at scale and Neo4j (7-8 range), but penalize -2 for Spark/Redis not in role desc (-2). Clarity: Clear intent and structured, but multiple bundled questions (-2) and vague scoping. Discriminative: Excellent on trade-offs/caching strategies at senior level, strong separation.",
          "composite": 7.666666666666667
        }
      },
      {
        "role_id": "be-424",
        "role_title": "Backend Engineer - Message Serialization",
        "question": "# Technical Screening Question\n\n**You're designing a message serialization system where producer services publish events to Kafka. Your schema needs to evolve: you're adding a new required field `userId` to an existing event that previously only had `eventId` and `timestamp`.**\n\n**Walk me through:**\n1. **How would you handle this schema change to ensure consumers aren't broken?**\n2. **What's the difference between backward and forward compatibility here, and which one matters for your scenario?**\n3. **If you're using Avro with Schema Registry, what would you actually do to deploy this change safely?**\n\n---\n\n## Why This Question Works\n\n\u2713 **Tests practical understanding** - Not asking to define terms, but apply them to a real problem  \n\u2713 **Mid-level appropriate** - Assumes familiarity with Kafka/Avro, not mastery of edge cases  \n\u2713 **Reveals key competencies:**\n- Schema evolution strategy (backward vs forward compatibility)\n- Knowledge of schema registry patterns\n- Consideration of producer/consumer deployment order\n- Understanding of breaking vs non-breaking changes\n\n\u2713 **Conversational** -",
        "scores": {
          "relevance": 7,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Tests core schema evolution/compatibility skills for the role and mentions Kafka/Avro (inferred from role's serialization focus), but deduct -3 for being 3 bundled questions (-2) and somewhat generic serialization pattern (-1). Clarity: Clear scenario and intent, well-scoped, but multiple questions bundled (-2). Discriminative: Excellent at testing applied trade-offs, strategies, and tools to separate strong candidates.",
          "composite": 8.0
        }
      },
      {
        "role_id": "be-426",
        "role_title": "Junior Backend Engineer - DevOps Scripting",
        "question": "# Screening Question\n\n**You need to create a simple deployment script that:**\n1. Pulls the latest code from a GitHub repository\n2. Builds a Docker image\n3. Runs a basic health check on the container\n4. Reports success or failure\n\n**Write pseudocode or actual code (Python or Bash) showing how you'd structure this script. What error handling would you include, and why?**\n\n---\n\n## Why This Question Works\n\n\u2705 **Practical** \u2013 Tests real DevOps tasks they'll do daily  \n\u2705 **Appropriate Level** \u2013 Doesn't require advanced knowledge, but reveals fundamentals  \n\u2705 **Open-Ended** \u2013 Shows their thinking: error handling, logging, script structure  \n\u2705 **Time-Bound** \u2013 Can sketch this in 2-5 minutes  \n\u2705 **Reveals Gaps** \u2013 Junior candidates often forget error handling or health checks; strong juniors think about both\n\n## What You're Looking For\n\n- **Basic structure:** Sequential steps, clear logic\n- **Error handling:** Do they check if git pull/docker build succeeded?\n- **Health check thinking:** How do they",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core skills (Git, Docker, scripting for deployments) with no red flags. Clarity: Crystal clear, well-scoped steps, minor deduction for not specifying language preference explicitly. Discriminative: Strong separation via error handling and reasoning, reveals junior gaps in fundamentals without being trivial or overly broad.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-427",
        "role_title": "Backend Engineer - Feature Store",
        "question": "# Technical Screening Question\n\n**Scenario-based question:**\n\n\"You're designing a feature store where you need to serve both batch features for model training and real-time features for inference. A data scientist requests a feature that takes 30 seconds to compute from your data warehouse, but it needs to be available in <100ms for online serving.\n\nWalk me through how you'd approach this problem using Feast. What are the trade-offs you'd consider, and what would you recommend?\"\n\n---\n\n## Why this works:\n\n\u2713 **Practical & real** - Core feature store problem  \n\u2713 **Mid-level appropriate** - Tests architectural thinking, not deep Feast internals  \n\u2713 **Open-ended** - Reveals how they think about latency, caching, and design decisions  \n\u2713 **Timeboxed** - Can explore thoroughly in 3-4 minutes  \n\n**What you're evaluating:**\n- Understanding of batch vs. real-time serving gap\n- Knowledge of Feast's materialization + online store strategy\n- Awareness of caching/pre-computation trade-offs\n- Problem-solving approach (pre-compute? cache",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Directly tests core feature store skills (batch/real-time serving, computation, versioning implied) matching role; Feast aligns perfectly as role focuses on feature store infra. Crystal clear scenario with specific constraints, well-scoped for screening. Strong separation via trade-offs/caching reasoning, Feast strategies distinguish mid-level expertise without trivia. Minor ding: assumes Feast familiarity but it's spot-on for role.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-428",
        "role_title": "Senior Backend Engineer - Payment Orchestration",
        "question": "# Technical Screening Question\n\n**Design a payment provider failover system for a payment orchestration platform. You have 3 payment providers (A, B, C) with different success rates (95%, 92%, 88%), costs, and latencies. A transaction fails with provider A. How would you:**\n\n1. **Decide which provider to failover to?** What factors beyond success rate would you consider?\n2. **Prevent cascading failures?** (e.g., avoid repeatedly routing to a temporarily degraded provider)\n3. **Track this decision** so you can optimize routing over time?\n\n*Assume you have access to: transaction metadata, provider health metrics, PostgreSQL, Redis, and Kafka.*\n\n---\n\n## Why This Question Works\n\n- **Practical & Real**: Directly mirrors the core responsibility (routing + failover)\n- **Senior-Level Depth**: Expects understanding of tradeoffs (cost vs. reliability), circuit breaker patterns, and observability\u2014not just \"use the best provider\"\n- **Tech Stack Integration**: Naturally leads to discussing Redis (circuit breaker state), Kafka (event logging), PostgreSQL (analytics), and Go (implementation",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core focus on failover, routing, optimization, and tracking; integrates exact tech stack. Clarity: Well-scoped with specifics (3 providers, metrics, tech access), minor deduction for bundled sub-questions. Discriminative: Probes deep trade-offs, patterns (circuit breakers), and observability, strongly separates seniors from juniors.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-429",
        "role_title": "Backend Engineer - Log Aggregation",
        "question": "# Screening Question\n\n**You're designing a log aggregation pipeline that needs to handle 2M events/second. Logs arrive via Kafka, get parsed and enriched by Vector, then stored in ClickHouse.**\n\n**Walk me through how you'd handle a scenario where:**\n- **Parsing fails on 5% of incoming logs** (malformed JSON, unexpected fields)\n- **You need to retain raw logs for debugging but also ensure failed logs don't block the pipeline**\n\n**What would you implement, and why?**\n\n---\n\n## Why This Question Works\n\n**Tests practical depth:**\n- Understanding of real failure modes in log pipelines\n- Knowledge of Vector's error handling capabilities\n- ClickHouse schema design decisions (separate tables? columns?)\n- Go considerations if writing custom handling\n\n**Seniority-appropriate:**\n- Mid-level should know basic patterns but may not have all details polished\n- Room to discuss trade-offs (buffering, dead-letter queues, alerting)\n- Acceptable to say \"I'd research Vector's exact features, but here's my approach...\"\n\n**Observable reasoning:**\n- Listen for: asynchronous handling,",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core log pipeline skills (parsing, enrichment, failure handling, retention) with exact tech stack (Kafka, Vector, ClickHouse); no red flags (-0). Clarity: Crystal clear scenario, well-scoped with specific rates and goals; no bundling or vagueness (-0). Discriminative: Probes deep trade-offs (DLQs, buffering, async, alerting, schema design), distinguishes applied knowledge from rote recall; senior-appropriate depth without being too broad (-0).",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-430",
        "role_title": "Junior Backend Engineer - Search Indexing",
        "question": "# Technical Screening Question\n\n**\"Walk me through how you would index a list of 10,000 product records from a PostgreSQL database into Elasticsearch. What would you consider important, and what potential issues might you run into?\"**\n\n## Why This Question Works:\n\n**Tests practical knowledge:**\n- Understanding of data movement between systems\n- Awareness of bulk indexing (not indexing one-by-one)\n- Basic mapping/schema thinking\n\n**Appropriate for junior level:**\n- No requirement for advanced optimization\n- Open-ended enough to show thinking process\n- Candidates can discuss real concerns they'd encounter\n\n**Reveals important gaps:**\n- Do they know about batch/bulk APIs?\n- Are they aware of memory/performance constraints?\n- Do they think about data transformation?\n- Can they identify common failure modes (timeout, connection issues, invalid data)?\n\n**Follow-up prompts** (if needed):\n- \"How would you handle documents that fail to index?\"\n- \"What if the data keeps growing daily\u2014how would you keep the index updated?\"\n- \"Why wouldn't you just index records one at a time?\"\n\n---\n\n*Expected response time: 3",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core skills (bulk indexing from PostgreSQL to Elasticsearch, mapping) for entry-level search indexing role; minor ding as role emphasizes learning but question fits perfectly (-1 generic feel avoided). Clarity: Crystal clear, well-scoped open-ended walkthrough with practical focus, no ambiguity or missing constraints. Discriminative: Excellent at revealing practical understanding, trade-offs (batch vs single, issues like memory/failures), and thought process to separate juniors with real knowledge from weak ones.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-432",
        "role_title": "Backend Engineer - Data Validation",
        "question": "# Technical Screening Question\n\n**Scenario-based question:**\n\n\"You're building a data validation checkpoint in Great Expectations for a daily customer transaction pipeline in S3. The pipeline should catch:\n- Missing values in the `transaction_id` column\n- Transaction amounts that are statistical outliers (beyond 3 standard deviations)\n- Schema changes (new or dropped columns)\n\nWalk me through how you'd set this up. What Great Expectations components would you use, and how would you handle the case where a legitimate spike in transaction amounts (like a holiday sale) triggers false alerts?\"\n\n---\n\n## Why this question works:\n\n\u2705 **Practical** \u2013 Real scenario mid-level engineers face  \n\u2705 **Multi-layered** \u2013 Tests GE knowledge + data validation thinking + problem-solving  \n\u2705 **Seniority-appropriate** \u2013 Expects familiarity with GE but allows room to explore reasoning  \n\u2705 **Time-bound** \u2013 Can be thoroughly answered in 3-4 minutes  \n\u2705 **Reveals depth** \u2013 Follow-up on the \"false alerts\" part shows if they understand monitoring trade-offs",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core data validation (schema, stats, missing values) and lineage-adjacent pipeline quality using Great Expectations, perfectly matching role; no red flags. Clarity: Crystal clear scenario with specific requirements and well-scoped follow-up; no ambiguity or bundling. Discriminative: Tests applied GE knowledge, setup reasoning, and trade-offs (false positives), strongly separating candidates with practical depth.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-431",
        "role_title": "Staff Engineer - Real-time Collaboration",
        "question": "# Technical Screening Question\n\n**Design a real-time presence system for a collaborative editor where multiple users can see who else is currently viewing the document. How would you handle:**\n\n1. **User join/leave events** \u2014 What's your approach to detecting and broadcasting these events reliably?\n2. **Network failures** \u2014 How would you distinguish between a user who temporarily disconnected vs. one who left intentionally?\n3. **Scalability trade-offs** \u2014 Would you use Redis pub/sub, a presence channel pattern, or something else? Why?\n\n**Follow-up based on their answer:** If your solution uses heartbeats/timeouts, walk me through how you'd tune the timeout window and what happens if it's too aggressive or too lenient.\n\n---\n\n## Why This Question Works\n\n- **Seniority-appropriate**: Tests architectural thinking and trade-off reasoning, not implementation details\n- **Practical**: Real problem they'd solve day-one; not theoretical CRDT minutiae\n- **Reveals depth**: Their answer shows understanding of distributed systems, operational concerns, and when to use which tools\n- **Conversational**: Natural follow-ups exist; you can dig into their",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Directly tests core real-time collaboration skills (presence systems) from role description. Crystal clear with specific sub-questions and well-scoped for screening (-0 red flags). Excellent separation via trade-offs, distributed systems depth, and tunable parameters; follow-up probes deeper understanding. Minor deduct for slight bundling but structured well.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-433",
        "role_title": "Senior Backend Engineer - Platform APIs",
        "question": "# Technical Screening Question\n\n**Scenario-based question (2-5 minutes):**\n\n\"You're designing an internal platform API that allows teams to provision and manage database instances. Multiple services will call this API concurrently, and you need to ensure that:\n1. A database instance can only be provisioned once, even if multiple requests arrive simultaneously\n2. The provisioning process takes 30-60 seconds\n3. Callers need to know the status without polling repeatedly\n\nWalk me through how you'd design this in Go. What patterns or mechanisms would you use to handle the concurrency and reduce polling overhead?\"\n\n---\n\n## Why This Works:\n\n\u2713 **Practical** \u2013 Tests real platform API design decisions (idempotency, concurrency, async status tracking)\n\n\u2713 **Senior-level** \u2013 Expects knowledge of sync primitives, request deduplication, and pub/sub or streaming patterns\n\n\u2713 **Go-specific** \u2013 Opens discussion of mutexes, channels, context handling\n\n\u2713 **Platform-focused** \u2013 Directly relevant to service provisioning, a core platform concern\n\n\u2713 **Time-bound** \u2013 Candidates can sketch a",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core platform API skills (provisioning, concurrency) in Go, matching role's service catalog/resource provisioning focus; minor deduction as role doesn't explicitly list Go (-1). Clarity: Crystal clear scenario, specific requirements, well-scoped with time bound. Discriminative: Probes deep applied knowledge of Go sync primitives, idempotency, async patterns (e.g., channels, pub/sub), revealing senior-level trade-offs vs. basic recall.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-436",
        "role_title": "Backend Engineer - API Documentation",
        "question": "# Technical Screening Question\n\n## Question:\n\nYou're building an API documentation system that needs to auto-generate TypeScript SDK code from OpenAPI specs. Your team has discovered that some API endpoints have optional parameters with complex nested objects, and the generated SDK code is either too verbose or missing type safety for these optional fields.\n\n**How would you approach this problem? Walk me through:**\n1. How you'd modify the OpenAPI schema to properly represent optional nested structures\n2. What TypeScript patterns you'd use in the code generator to handle these cases\n3. One trade-off you'd have to consider between SDK usability and spec accuracy\n\n---\n\n## Why This Question Works:\n\n\u2713 **Practical:** Tests real problems encountered in API documentation tooling  \n\u2713 **Multi-faceted:** Requires knowledge of OpenAPI, TypeScript, and code generation  \n\u2713 **Mid-level appropriate:** Doesn't expect perfect answers, but reveals depth of experience  \n\u2713 **Time-bound:** Answerable conversationally in 3-4 minutes  \n\u2713 **Discussion-friendly:** Opens dialogue about trade-offs and decision-making  \n\n**Follow-up prom",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Directly targets core role skills (OpenAPI specs, SDK automation, TypeScript) with practical nested object issue in API tooling. Crystal clear structure with numbered parts and scoped constraints. Strong separation via trade-off analysis revealing deep understanding; minor deduct for bundled sub-questions but well-scoped overall.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-434",
        "role_title": "Principal Engineer - Database Strategy",
        "question": "# Screening Question\n\n**You're advising a company that currently runs a monolithic PostgreSQL database serving a single region. They're expanding to 3 new geographic regions and need <100ms latency for all users. Walk me through how you'd approach this problem\u2014what are the key trade-offs you'd evaluate, and what factors would drive your technology recommendation between solutions like read replicas, CockroachDB, or a sharded approach?**\n\n---\n\n## Why This Works\n\n**Tests Principal-Level Skills:**\n- Strategic thinking (trade-offs, not just \"what tool is best\")\n- Distributed systems depth (consistency models, latency, operational complexity)\n- Business acumen (understanding constraints drive decisions)\n- Mentorship capability (clear communication of complex reasoning)\n\n**Practical & Focused:**\n- Reflects a real architectural challenge\n- Answerable in 2-5 minutes without deep dives\n- Reveals how they'd actually guide a team through decisions\n\n**Distinguishes Principals:**\n- Junior engineers list features; principals discuss trade-offs\n- Expected to mention: RPO/RTO, consistency requirements, cost, team capability, migration risk",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Relevance: Directly tests core role skills in multi-region DB strategy, migration, tech selection with PostgreSQL context; no red flags. Clarity: Crystal clear, well-scoped with specific constraints (regions, latency); minor -1 for no explicit time/format. Discriminative: Excellent at separating principals via trade-offs, applied knowledge, and reasoning depth vs rote recall.",
          "composite": 9.666666666666666
        }
      },
      {
        "role_id": "be-435",
        "role_title": "Junior Backend Engineer - Queue Consumers",
        "question": "# Screening Question\n\n**You're building a RabbitMQ consumer that processes payment transactions from a queue. The consumer receives a message, updates a PostgreSQL database, and then acknowledges the message. However, you notice that sometimes the same payment gets processed twice.**\n\n**Walk me through:**\n1. **What could cause duplicate processing?**\n2. **How would you fix it?**\n3. **What's the difference between auto-acknowledge and manual acknowledge, and which would you use here?**\n\n---\n\n## Why This Question Works\n\n\u2705 **Practical & Real-world** \u2014 Duplicate processing is a common junior engineer problem  \n\u2705 **Right Difficulty** \u2014 Tests understanding of message queue fundamentals without requiring deep distributed systems knowledge  \n\u2705 **Covers Key Concepts** \u2014 Acknowledgment patterns, idempotency, and failure scenarios (all listed in the role description)  \n\u2705 **Open-ended** \u2014 Reveals how they think through problems, not just memorized answers  \n\u2705 **Time-appropriate** \u2014 Can be thoroughly answered in 3-4 minutes\n\n---\n\n## What You're Looking For\n\n**Strong Answer",
        "scores": {
          "relevance": 7,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Tests core acknowledgment patterns and idempotency from role, but RabbitMQ/PostgreSQL are specific unlisted techs (-2); not generic (-3 avoided). Clarity: Crystal clear, well-scoped numbered parts, no bundling issues. Discriminative: Excellent separation via causes/fixes/trade-offs, reveals applied thinking beyond recall.",
          "composite": 8.333333333333334
        }
      },
      {
        "role_id": "be-437",
        "role_title": "Senior Backend Engineer - Secrets Management",
        "question": "# Screening Question\n\n**You're designing a secrets rotation system for database credentials across 50+ microservices in Kubernetes. Walk me through how you'd architect this using Vault, and specifically explain how you'd handle the scenario where a service fails to pick up a rotated credential before the old one expires.**\n\n---\n\n## Rationale\n\nThis question is effective because it:\n\n1. **Tests practical architecture thinking** - Requires understanding of real-world secrets management complexity, not memorized facts\n\n2. **Appropriate for senior level** - Expects thoughtful design decisions around failure modes, not just \"how to use Vault\"\n\n3. **Covers multiple key skills**:\n   - Vault knowledge (rotation policies, lease management)\n   - Kubernetes operational awareness (deployment, service communication)\n   - Go context (they'd likely implement the consumer application)\n   - Security thinking (credential expiry, availability vs security tradeoffs)\n\n4. **Reveals problem-solving approach** - How they handle edge cases shows maturity:\n   - Do they think about grace periods?\n   - Do they consider monitoring/alerting?\n   - Do they discuss rollback strategies?",
        "scores": {
          "relevance": 7,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Tests core secrets rotation but Vault not explicitly listed in role (subtract 2); covers rotation/expiry well. Clarity: Well-scoped architecture question with specific failure scenario, minor missing constraints like time limit (-1). Discriminative: Excellent at testing trade-offs, failure handling, and practical depth for senior roles.",
          "composite": 8.0
        }
      },
      {
        "role_id": "be-439",
        "role_title": "Backend Engineer - Billing Integration",
        "question": "# Screening Question\n\n**\"Walk me through how you would handle a scenario where a customer upgrades their subscription mid-billing cycle and needs to be prorated. What information would you need from Stripe, and how would you structure the logic to calculate and apply the credit?\"**\n\n---\n\n## Why This Question Works:\n\n**Practical & Role-Specific**\n- Directly tests core responsibility (proration, invoicing)\n- Real problem they'll solve day-one\n\n**Right Difficulty Level**\n- Mid-level should understand subscription concepts\n- Allows room to discuss Stripe APIs, calculation logic, and edge cases\n- Not trivial (requires billing domain knowledge) but not PhD-level\n\n**Reveals Key Competencies**\n- Stripe API familiarity (proration_behavior, line items)\n- Billing math (daily rates, cycle dates)\n- System design thinking (data flow, state management)\n- Communication skills (explaining trade-offs)\n\n**Answerable in 2-5 Minutes**\n- Can sketch approach quickly without deep implementation details\n- Follow-ups available: \"What about failed payments?\" or \"How would you test this?\"",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core focus on proration/subscriptions/Stripe, no red flags. Clarity: Crystal clear scenario with specific ask, minor vagueness on exact Stripe API version but well-scoped. Discriminative: Tests applied knowledge of billing logic, Stripe integration, trade-offs/edge cases, strong separation without being too broad or recall-based.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-438",
        "role_title": "Staff Engineer - ML Serving Infrastructure",
        "question": "# Technical Screening Question\n\n**\"Walk me through how you'd design a batching strategy for an ML serving system handling requests with variable latency requirements. Specifically, consider a scenario where you have both latency-sensitive requests (sub-100ms SLA) and batch-optimized requests (up to 1s acceptable). How would you structure this in Triton, and what metrics would you track to validate it's working?\"**\n\n---\n\n## Why This Question Works:\n\n**Tests practical expertise:**\n- Understanding of dynamic batching trade-offs\n- Actual Triton configuration knowledge\n- Ability to handle competing constraints (latency vs throughput)\n\n**Appropriate for staff level:**\n- Requires architectural thinking, not just implementation\n- Evaluates observability mindset (metrics/monitoring)\n- Shows how they'd make real production trade-offs\n\n**Realistic scenario:**\n- This is a common real-world problem at scale\n- Reveals how they think about SLAs and system design\n\n**Answerable in timeframe:**\n- Senior candidates can sketch the solution quickly\n- Follow-ups naturally extend discussion if needed (Kubernetes scheduling, TensorRT optimizations,",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Directly tests core role skills (batching strategies, latency optimization for ML serving) with Triton (common in ML infra). Crystal clear scenario with specific SLAs, scopes to Triton structure and metrics. Strongly differentiates via trade-offs, architectural thinking, and observability\u2014no red flags.",
          "composite": 9.666666666666666
        }
      },
      {
        "role_id": "be-440",
        "role_title": "Junior Backend Engineer - Metrics Collection",
        "question": "# Screening Question\n\n**You're instrumenting a Go service that processes user requests. You need to track how long each request takes. Should you use a Prometheus Counter, Gauge, or Histogram for this metric? Explain why, and describe what data would be available in Prometheus/Grafana afterward.**\n\n---\n\n## Why This Works\n\n\u2705 **Appropriate for Junior Level:** Tests understanding of metric types without requiring deep expertise\n\n\u2705 **Practical:** Directly mirrors Day 1 tasks\u2014instrumenting a real service\n\n\u2705 **Observable Reasoning:** Answer reveals if they understand *why* metrics matter, not just syntax\n\n\u2705 **Time-Bound:** 2-3 minutes for a solid response\n\n---\n\n## What You're Looking For\n\n**Strong Answer** includes:\n- Histogram (correct choice)\n- Explains why: need distribution/percentiles, not just a single value\n- Mentions what becomes available: latency buckets, quantiles (p99, p95), average\n- Maybe: \"Could query request latency spikes in Grafana\"\n\n**Acceptable Answer:**\n- Histogram identified + basic explanation of buckets",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core focus on counter/gauge/histogram patterns for service instrumentation. Clarity: Crystal clear with specific scenario, single focused question, no ambiguity. Discriminative: Requires understanding trade-offs between metric types and explains queryable data (buckets/quantiles), distinguishing conceptual grasp from rote recall; ideal for entry-level.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-442",
        "role_title": "Backend Engineer - Event Streaming",
        "question": "# Screening Question\n\n**You're designing a real-time notification system where one event needs to be delivered to potentially thousands of subscribers. A naive approach fans out the event to every subscriber's queue, but this causes memory issues and slow writes at scale.**\n\n**Walk me through how you'd optimize this using NATS. What patterns or features would you leverage, and why?**\n\n---\n\n## Why This Question Works\n\n**Practical & Role-Specific:**\n- Directly addresses the \"fan-out\" requirement in the job description\n- Tests real understanding of NATS capabilities (subjects, subscriptions, subject hierarchies)\n\n**Mid-Level Appropriate:**\n- Not a gotcha question; assesses experience with actual architectural decisions\n- Expects knowledge of trade-offs, not perfection\n\n**Reveals Key Insights:**\n- Do they know NATS subjects/wildcards reduce fanout complexity?\n- Can they articulate the difference between queue groups vs. broadcast subscriptions?\n- Do they think about memory/performance implications?\n- Can they discuss subject-based filtering as a solution?\n\n**Answerable in 2-5 Minutes:**\n- Candidate can sketch the approach",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests fan-out core to role; assumes NATS in stack (no red flags). Clarity: Well-scoped, unambiguous with context. Discriminative: Probes trade-offs (queue groups vs broadcast, subjects/wildcards), separates experienced users from superficial knowledge in 2-5 min.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-441",
        "role_title": "Senior Backend Engineer - Autocomplete Systems",
        "question": "# Technical Screening Question\n\n## Autocomplete Latency Under Load\n\n**Scenario:**\nYou're building an autocomplete service that needs to handle 100K QPS with sub-50ms latency. Your current implementation queries Elasticsearch for prefix matches, applies personalization filters based on user history (cached in Redis), and ranks results.\n\nUnder load testing, you're seeing:\n- Elasticsearch queries: ~15ms (p99)\n- Redis lookups: ~2ms (p99)\n- Personalization logic: ~8ms (p99)\n- **Total latency: ~80ms (p99) \u2014 exceeding your 50ms SLA**\n\n**Questions:**\n1. Walk me through how you'd diagnose where the latency is actually being lost\n2. What optimizations would you prioritize, and why?\n3. What tradeoffs would you consider?\n\n---\n\n## Why This Works\n\n\u2705 **Tests senior-level thinking:** Requires understanding of systems bottlenecks, not just API knowledge  \n\u2705 **Practical:** Reflects real performance challenges in high-scale autocomplete  \n\u2705 **Open-ended:**",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Relevance: Directly tests core autocomplete optimization, prefix matching, personalization, and exact latency/QPS from role (-1 for no explicit role tech stack mention). Clarity: Crystal clear scenario with precise metrics and focused 3-part structure. Discriminative: Excellent probe of diagnosis, prioritization, and tradeoffs, strongly separates seniors who grasp bottlenecks from juniors.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-443",
        "role_title": "Senior Backend Engineer - Data Lake Architecture",
        "question": "# Screening Question\n\n**You're designing a data lake where business users need to query historical snapshots of customer data from 30 days ago, but the underlying schema has evolved significantly (columns added, renamed, and removed). How would you use Apache Iceberg's features to support this requirement, and what are the tradeoffs you'd consider?**\n\n---\n\n## Why This Question Works\n\n**Tests practical expertise:**\n- Time travel queries (core Iceberg feature)\n- Schema evolution handling (critical for production data lakes)\n- Understanding of snapshots and metadata layers\n\n**Appropriate for senior level:**\n- Requires experience designing systems, not just using APIs\n- Allows discussion of tradeoffs and architectural decisions\n- Open-ended enough to reveal depth of understanding\n\n**Realistic scenario:**\n- Common real-world requirement in mature data lakes\n- Shows how they think about user experience + infrastructure\n\n**Natural follow-ups** (if needed):\n- \"How would your answer change if users queried data from 2 years ago?\"\n- \"What monitoring would you put in place?\"\n- \"How does this interact with your partitioning strategy?\"",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Relevance: Directly tests core role skills (Iceberg table formats, schema evolution, time travel) with no red flags. Clarity: Crystal clear scenario and scope, minor deduction for no explicit time/format constraints but well-formed for screening. Discriminative: Excellent at revealing deep applied knowledge, tradeoffs, and architectural thinking to separate seniors from juniors.",
          "composite": 9.666666666666666
        }
      },
      {
        "role_id": "be-444",
        "role_title": "Backend Engineer - Service Templates",
        "question": "# Technical Screening Question\n\n**You're designing a Cookiecutter template for new Go microservices. A developer will run it once, and the generated service should be immediately deployable to Kubernetes with ArgoCD, including basic observability and security practices.**\n\n**Walk me through:**\n1. **What files/structure would you include** in the template to support this?\n2. **How would you handle variation** \u2014 for example, some services need databases, others don't?\n3. **What's one thing you'd bake into defaults** (vs. leaving for the developer to configure), and why?\n\n---\n\n## Why This Question Works\n\n\u2705 **Practical** \u2014 Tests real templating decisions, not memorization  \n\u2705 **Scope** \u2014 Covers the full stack (scaffolding, CI/CD, K8s, observability)  \n\u2705 **Mid-level appropriate** \u2014 Expects system thinking, not expert-level optimization  \n\u2705 **Reveals priorities** \u2014 Shows what the candidate considers \"essential\" vs. \"nice-to-have\"  \n\u2705 **Conversational** \u2014 Natural follow-ups on trade",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (templating, CI/CD, observability, security, K8s/ArgoCD) with no red flags (-0). Clarity: Well-scoped with 3 focused parts, unambiguous for screening despite multi-part (-0). Discriminative: Tests applied decisions, trade-offs, and priorities, strongly separating candidates via reasoning.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-445",
        "role_title": "Junior Backend Engineer - Database Migrations",
        "question": "# Screening Question\n\n**You need to add a new required column to an existing production table with millions of rows. Write out the steps you would take to do this safely using Alembic, and explain why you can't simply use `Column('new_col', String, nullable=False)` in your migration.**\n\n---\n\n## Why This Works\n\n\u2705 **Practical & Real** \u2014 This is a genuine problem junior engineers face on their first day\n\n\u2705 **Right Level** \u2014 Tests understanding of:\n- Alembic basics (writing migrations)\n- PostgreSQL constraints\n- Production safety thinking (zero-downtime concerns)\n- Schema versioning logic\n\n\u2705 **Time-Appropriate** \u2014 Takes 2-4 minutes for a competent junior to explain\n\n\u2705 **Open-Ended** \u2014 Reveals:\n- Whether they know `nullable=False` + no default = backfill problem\n- If they understand multi-step migration patterns\n- Whether they think about existing data\n- Communication clarity on database concepts\n\n---\n\n## What You're Looking For\n\n**Good Answer Includes:**\n- \"Add the column as nullable first\"\n- \"",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core focus on database migrations, zero-downtime patterns, and Alembic (no red flags). Clarity: Crystal clear with specific scenario and explanation prompt; minor deduction for lacking explicit time/format constraints. Discriminative: Strongly separates candidates by testing applied knowledge of multi-step migration patterns, production safety, and trade-offs (not recall); open-ended but scoped well.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-446",
        "role_title": "Staff Engineer - Distributed Tracing",
        "question": "# Technical Screening Question\n\n**Design a sampling strategy for a distributed tracing system handling 100k requests/second across multiple services. Walk me through how you'd decide between head-based and tail-based sampling, what trade-offs you'd consider, and how you'd handle the challenge of maintaining trace coherence when different spans are sampled at different rates.**\n\n---\n\n## Why This Question Works\n\n**Tests Staff-Level Thinking:**\n- Requires understanding of architectural trade-offs (not just \"what is sampling\")\n- Demonstrates systems thinking across infrastructure components\n- Shows ability to balance operational constraints with observability goals\n\n**Practical & Real:**\n- This is an actual problem at scale (100k RPS is genuine staff-level territory)\n- Candidates will naturally discuss real concerns: cost, trace completeness, debugging capability\n\n**Open-Ended:**\n- Allows candidates to showcase depth (mentioning Jaeger sampling plugins, ClickHouse retention strategies, Kafka buffering, SLO correlation)\n- No single \"correct\" answer\u2014reveals how they think through complexity\n\n**Natural Follow-ups:**\n- \"How would you implement priority-based sampling for errors?\"\n-",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Relevance: Directly targets core role responsibilities (distributed tracing at scale, sampling strategies) with realistic 100k RPS; no red flags. Clarity: Crystal clear, well-scoped for screening, precise trade-offs specified; minor deduction for lacking explicit time constraint. Discriminative: Excellent test of deep trade-offs, systems thinking, and applied knowledge at staff level, strong separation of strong vs weak candidates; open-ended but focused.",
          "composite": 9.666666666666666
        }
      },
      {
        "role_id": "be-447",
        "role_title": "Backend Engineer - Load Shedding",
        "question": "# Screening Question\n\n**\"Walk me through how you would implement a load shedding mechanism that drops low-priority requests when your service is under high load. What signals would you monitor to decide when to start shedding, and how would you prevent cascading failures downstream?\"**\n\n---\n\n## Why This Question Works\n\n**Tests core competencies:**\n- Load shedding patterns and decision logic\n- Backpressure signal understanding (queue depth, latency, CPU)\n- Graceful degradation thinking\n- Awareness of downstream impact\n\n**Seniority-appropriate:**\n- Mid-level should have opinions on *when* to shed and *what* metrics matter\n- Not asking for production-ready code, just architectural thinking\n- Room to discuss trade-offs (false positives vs. user experience)\n\n**Practical depth:**\n- Reveals whether they've actually dealt with throttling vs. just read about it\n- Follow-ups are natural: \"How would Redis fit in?\" or \"How do you test this?\"\n- Can assess if they think about observability (Prometheus integration)\n\n**Time-efficient:**\n- Candidate can outline approach in 2-3",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core focus on load shedding, priority queues, throttling, and backpressure\u2014no red flags, directly tests listed skills. Clarity: Crystal clear and well-scoped for screening, minor deduction only for lacking explicit time constraint. Discriminative: Tests applied architectural thinking, trade-offs, and real-world signals, strongly separates experienced candidates; time-efficient as noted.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-448",
        "role_title": "Senior Backend Engineer - Webhook Infrastructure",
        "question": "# Screening Question\n\n**Design a webhook retry mechanism that handles delivery failures while preventing duplicate deliveries to the customer. What would be your approach to storing state, and how would you handle the edge case where a webhook succeeds but the success acknowledgment is lost?**\n\n---\n\n## Why This Question Works\n\n**Practical Knowledge:** Tests understanding of the core challenge in webhook infrastructure\u2014idempotency and reliability under failure conditions.\n\n**Seniority Appropriate:** Expects the candidate to think about:\n- Idempotency keys and deduplication strategies\n- State management (database, cache, or both)\n- Distributed system failure modes\n- Trade-offs between consistency and operational complexity\n\n**Real-World Relevance:** This exact problem appears in production webhook systems and requires senior-level systems thinking.\n\n**Conversational:** Opens into natural follow-ups:\n- \"How would Kafka fit into this design?\"\n- \"What happens under high volume?\"\n- \"How do you monitor delivery success?\"\n\n**Time Appropriate:** Can be outlined in 2-3 minutes, with depth added based on candidate confidence.",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Directly targets core webhook delivery challenges (retry, idempotency, state, failure modes) from role description. Crystal clear with specific edge case; minor deduct for lacking explicit constraints like scale. Excellent at distinguishing senior systems thinkers via trade-offs and deep understanding.",
          "composite": 9.666666666666666
        }
      },
      {
        "role_id": "be-449",
        "role_title": "Junior Backend Engineer - Auth Integration",
        "question": "# Technical Screening Question\n\n**\"Walk me through how you would implement a refresh token flow in a Node.js application. Specifically, explain:**\n\n1. **Where** you'd store the refresh token (client-side vs server-side) and why\n2. **When** you'd use it (what triggers a refresh?)\n3. **One security concern** you'd need to handle\n\nYou don't need to write code\u2014just explain your approach.\"\n\n---\n\n## Rationale\n\n\u2705 **Tests practical OAuth2 understanding** - Refresh tokens are core to the role  \n\u2705 **Right difficulty level** - Junior engineers should know the basics but may not have all details polished  \n\u2705 **Security-focused** - Demonstrates awareness of auth risks (token rotation, storage, expiration)  \n\u2705 **2-5 minute response** - Conceptual explanation, not a coding exercise  \n\u2705 **Reveals real experience** - Distinguishes between memorized facts and applied knowledge\n\n**Red flags:** Confusion between access/refresh tokens, storing refresh tokens insecurely client-side, or inability to name any security concern.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 8,
          "reasoning": "Relevance: Directly tests core OAuth refresh token flow central to role's token handling focus, matches Node.js context, minor deduction for not being social login specific (-1). Clarity: Well-structured with numbered points, clear scope (no code), unambiguous. Discriminative: Tests applied reasoning on storage/ triggers/security trade-offs, separates practical knowledge from trivia, good for entry-level but not elite depth.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-450",
        "role_title": "Principal Engineer - ML Platform Strategy",
        "question": "# Screening Question\n\n**Describe a situation where you had to choose between multiple ML orchestration or workflow tools for your organization. Walk me through your decision framework\u2014what factors did you weigh, and how did you handle stakeholder concerns about the choice?**\n\n---\n\n## Why This Works\n\n**Tests Principal-Level Thinking:**\n- Requires systems thinking (not just technical merits)\n- Evaluates decision-making under trade-offs\n- Assesses ability to balance technical, organizational, and governance concerns\n\n**Practical & Concrete:**\n- Grounded in real experience (Kubernetes, Kubeflow, MLflow, Ray context)\n- Answerable in 2-5 minutes with specific example\n- Naturally surfaces their approach to architecture and strategy\n\n**Reveals Key Competencies:**\n- **Platform strategy:** How they evaluate tools holistically\n- **Governance:** Maintainability, compliance, cost considerations\n- **Mentorship/Communication:** How they aligned teams around the decision\n- **Technical depth:** Understanding of tool trade-offs (e.g., Kubeflow vs Ray vs MLflow for different workloads)\n\n**Follow-ups",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core ML platform strategy, tooling selection, governance, and stakeholder alignment from role; minor ding for assuming specific experience (-1). Clarity: Crystal clear behavioral question with precise scope for screening. Discriminative: Probes deep trade-offs, frameworks, and real-world application, strongly separating principal-level thinkers from juniors.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-451",
        "role_title": "Backend Engineer - Subscription Billing",
        "question": "# Screening Question\n\n**Walk me through how you'd handle a customer downgrading from a $100/month annual plan to a $50/month annual plan mid-cycle. What are the key considerations, and how would you implement the credit/refund logic?**\n\n---\n\n## Why This Question Works\n\n**Tests practical expertise:**\n- Proration calculations (core to the role)\n- Subscription state management\n- Financial correctness and edge cases\n\n**Appropriate for mid-level:**\n- Assumes familiarity with billing concepts\n- Doesn't require memorized formulas\n- Evaluates problem-solving approach, not just implementation details\n\n**Real-world relevance:**\n- Directly reflects daily work in subscription billing\n- Reveals how they think about atomicity, refunds, and audit trails\n\n---\n\n## What You're Looking For\n\n**Strong answers typically include:**\n- How they calculate prorated amounts (days remaining, daily rate)\n- Where the credit goes (instant refund vs. account balance vs. future invoice)\n- Data consistency concerns (state transitions, idempotency)\n- How they'd validate the calculation and prevent double-refunds\n-",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Directly tests core role skills in proration, plan changes, and billing logic from the role description (no red flags). Crystal clear scenario with strong real-world scope, minor deduction for lacking explicit constraints like time or format. Excellent separation of strong candidates via trade-offs, edge cases, and implementation reasoning (not recall or trivial).",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-453",
        "role_title": "Backend Engineer - Notification Delivery",
        "question": "# Screening Question\n\n**You're designing a notification delivery system where thousands of devices need to receive a push notification within seconds. Walk me through how you'd handle the following scenario:**\n\n*A user sends a notification targeting 50,000 devices. Some devices have both FCM (Android) and APNS (iOS) tokens registered. You need to:*\n1. *Retrieve all device tokens efficiently*\n2. *Batch them appropriately for FCM and APNS APIs*\n3. *Handle partial failures (some batches fail, some succeed)*\n\n**What would your approach be, and what would you consider regarding database queries, batching strategy, and failure handling?**\n\n---\n\n## Why This Question Works\n\n\u2705 **Practical & Real** \u2013 Directly mirrors the actual job responsibility  \n\u2705 **Mid-Level Appropriate** \u2013 Tests architectural thinking without requiring deep expertise  \n\u2705 **Multi-Faceted** \u2013 Covers database efficiency, API integration, and resilience  \n\u2705 **Time-Boxed** \u2013 Can be answered thoroughly in 3-4 minutes  \n\u2705 **Open-Ended** \u2013 Reveals their",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core needs (device registration, batching, delivery tracking) with FCM/APNS directly relevant; no red flags. Clarity: Crystal clear, well-scoped scenario with specific numbered asks, minor deduction for no explicit time/format constraints. Discriminative: Excellent at separating candidates via trade-offs in queries/batching/failures, reveals architectural depth without being too broad or recall-based.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-452",
        "role_title": "Senior Backend Engineer - Faceted Search",
        "question": "# Screening Question\n\n**You're implementing faceted search for an e-commerce platform with millions of products. Users filter by category, price range, brand, and ratings simultaneously. Your aggregation queries are returning accurate facet counts, but query latency has increased from 200ms to 2+ seconds as the product catalog grew.**\n\n**Walk me through how you would diagnose and optimize this. What are the key bottlenecks you'd investigate in Elasticsearch, and what specific techniques would you apply?**\n\n---\n\n## Why This Works\n\n\u2705 **Practical & Real** - Reflects actual performance challenges at scale  \n\u2705 **Multi-layered** - Tests diagnosis, Elasticsearch knowledge, and optimization strategy  \n\u2705 **Senior-appropriate** - Expects understanding of aggregations, query planning, and tradeoffs  \n\u2705 **2-5 min range** - Answerable without deep implementation details  \n\u2705 **Open-ended** - Reveals depth of experience and problem-solving approach\n\n## What You're Evaluating\n\n- Understanding of Elasticsearch aggregation performance (shard-level work, query overhead)\n- Knowledge of practical optimizations (",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core focus on optimizing aggregation queries and facet count accuracy at scale in faceted search; no red flags. Clarity: Crystal clear scenario with specific symptoms, well-scoped for screening; minor deduction for lacking explicit time/format constraints. Discriminative: Strongly separates seniors via diagnosis, trade-offs (e.g., shard overhead, sampling), and ES-specific techniques; open-ended but focused.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-454",
        "role_title": "Junior Backend Engineer - ETL Scripts",
        "question": "# Screening Question\n\n**You have a CSV file with 500,000 customer records. One column contains dates in mixed formats (some are \"2024-01-15\", others are \"01/15/2024\"). You need to load this into PostgreSQL, but the database expects a standard datetime format. Walk me through how you'd handle this using pandas, and what you'd do if some rows have invalid dates.**\n\n---\n\n## Why This Question Works:\n\n\u2705 **Practical & Real** - Common ETL problem junior engineers actually face\n\n\u2705 **Right Difficulty** - Tests pandas basics + problem-solving, not memorization\n\n\u2705 **Multi-faceted** - Covers:\n- Data loading (read_csv)\n- Data cleaning/transformation (pd.to_datetime)\n- Error handling (invalid dates)\n- Database awareness (schema requirements)\n\n\u2705 **Time-appropriate** - Can explain solution in 2-5 minutes\n\n---\n\n## What You're Looking For:\n\n**Strong answer includes:**\n- `pd.read_csv()` to load the file\n- `pd.to_datetime()` with `infer_datetime",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core ETL skills (data transformation, error handling) with pandas for entry-level role; minor ding for assuming pandas without explicit mention (-1). Clarity: Crystal clear scenario, specific tech, well-scoped walkthrough with no ambiguity. Discriminative: Requires applied knowledge of pandas functions, error handling trade-offs (e.g., infer_datetime, errors='coerce'), distinguishes problem-solvers from rote learners.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-455",
        "role_title": "Staff Engineer - Developer Portal",
        "question": "# Technical Screening Question\n\n**You're designing the plugin architecture for a developer portal where different teams will contribute integrations (e.g., deployment tools, monitoring systems, secret management). Walk me through how you'd structure this to ensure:**\n\n1. **Plugin isolation** \u2014 a broken plugin doesn't crash the portal\n2. **Shared concerns** \u2014 plugins can access common APIs (auth, service catalog, notifications) without duplicating code\n3. **Team autonomy** \u2014 teams can develop and deploy plugins independently without coordinating with a central platform team\n\n**What are the key architectural decisions you'd make, and what trade-offs would you accept?**\n\n---\n\n## Why This Works\n\n**For Staff Level:** Tests architectural thinking and systems design, not implementation details. Expects them to reason about trade-offs and constraints.\n\n**Practical:** Directly relevant to Backstage plugin patterns and real portal challenges. Draws on their experience scaling developer tools.\n\n**2-5 Minute Response:** Can outline approach without deep code, but reveals:\n- Understanding of module/plugin boundaries\n- How they think about dependency injection and composition\n- Knowledge of deployment/versioning strategies\n- Awareness of operational concerns (",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core role responsibility (plugin architecture for developer portal), matches Backstage-like patterns; minor generic deduction avoided as it's role-specific (-0). Clarity: Well-scoped with 3 precise requirements, clear expectations for decisions/trade-offs, screening-appropriate depth. Discriminative: Probes deep architectural reasoning, trade-offs, and applied knowledge (isolation, shared deps, autonomy), strongly separates staff-level thinkers from juniors; fits 2-5 min response.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-456",
        "role_title": "Backend Engineer - Query Optimization",
        "question": "# Screening Question\n\n**You're analyzing a slow query on a PostgreSQL table with 10M rows. The query filters by `user_id` and `created_at`, but takes 8 seconds. When you run `EXPLAIN ANALYZE`, you see a Sequential Scan instead of an Index Scan, even though an index exists on `(user_id, created_at).**\n\n**Walk me through:**\n1. **Why the planner might choose Sequential Scan over the index**\n2. **How you'd investigate and fix this**\n3. **What tools or commands you'd use to confirm your fix works**\n\n---\n\n## Why This Works\n\n\u2705 **Practical & Real** - Mirrors actual performance debugging  \n\u2705 **Mid-Level Appropriate** - Requires index knowledge + planner understanding, not deep internals  \n\u2705 **Multi-faceted** - Tests diagnostics (EXPLAIN), root cause analysis, and solution validation  \n\u2705 **Time-Bound** - Can be answered thoroughly in 3-4 minutes  \n\u2705 **Follow-ups Ready** - Natural paths to dig deeper (statistics, query re",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core focus on slow query detection, index recommendations, and plan analysis in PostgreSQL (10/10, no red flags). Clarity: Crystal clear, well-scoped multi-part structure with real scenario; minor deduction for lacking explicit time/format constraints (-1). Discriminative: Excellent at separating candidates via trade-offs (planner decisions, stats, fixes), applied knowledge beyond recall (9/10).",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-458",
        "role_title": "Backend Engineer - API Gateway Plugins",
        "question": "# Technical Screening Question\n\n**Scenario:**\n\nYou're building a Kong plugin that needs to:\n1. Extract a user ID from the request header\n2. Check if that user is rate-limited using Redis\n3. Allow the request through or reject it with a 429 status\n\nWalk me through how you'd structure this plugin in Lua. What are the key lifecycle phases you'd use, and how would you interact with Redis?\n\n---\n\n## Why This Question Works:\n\n\u2705 **Practical** \u2014 Tests real plugin development, not Kong trivia  \n\u2705 **Scoped** \u2014 Can be answered thoroughly in 3-5 minutes  \n\u2705 **Multi-layered** \u2014 Evaluates:\n- Understanding of Kong's request/response lifecycle\n- Lua syntax and plugin structure\n- Redis client usage\n- HTTP status code handling\n\n\u2705 **Mid-level appropriate** \u2014 Assumes familiarity with Kong/Lua but doesn't require advanced optimization\n\n---\n\n## What to Listen For:\n- Mention of `access` phase (where business logic runs before proxying)\n- Use of Kong's Redis connector or similar approach\n- Proper",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Directly tests core Kong plugin development (Lua structure, lifecycle phases, Redis integration) matching API gateway auth/rate-limiting needs. Crystal clear scenario with focused scope, no ambiguity. Strong separation via applied knowledge of phases (access), Redis interaction, and trade-offs, not mere recall. Minor ding for no explicit time constraint but well-scoped for screening.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-459",
        "role_title": "Junior Backend Engineer - Container Security",
        "question": "# Technical Screening Question\n\n**Scenario-Based Question:**\n\n\"You're setting up Trivy to scan Docker images in your CI/CD pipeline. A developer builds an image from `ubuntu:latest` and the scan reports several high-severity vulnerabilities. Walk me through:\n\n1. What those vulnerabilities likely are and why they appear in a base image\n2. How you would approach fixing this\u2014what would you suggest to the developer?\n3. What's one limitation of just scanning and how might you enforce better practices going forward?\"\n\n---\n\n## Why This Works:\n\n\u2705 **Practical** \u2013 Mirrors real junior-level tasks (running scans, interpreting results, suggesting fixes)\n\n\u2705 **Layered** \u2013 Tests three levels: understanding (what vulnerabilities are), problem-solving (how to fix), and systems thinking (policy enforcement)\n\n\u2705 **Time-appropriate** \u2013 Answers in 2-5 minutes; no deep architectural knowledge required\n\n\u2705 **Seniority-aligned** \u2013 Doesn't expect mastery of Kubernetes or complex orchestration; focuses on container fundamentals they'll actually work with day one\n\n\u2705 **",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core container scanning (Trivy/Docker), vulnerability detection, base image management, and policy enforcement for entry-level role (-0 red flags). Clarity: Well-scoped scenario with numbered parts, unambiguous, screening-appropriate depth. Discriminative: Excellent separation via layered testing (understanding -> problem-solving -> systems thinking/trade-offs), not recall/yes-no, practical for juniors.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-457",
        "role_title": "Senior Backend Engineer - SLO Automation",
        "question": "# Screening Question\n\n**You're designing a burn rate alerting system for SLOs in Kubernetes. A service has a 99.9% availability SLO, and you need to alert when the error budget is being consumed too quickly\u2014not just when the SLO is breached.**\n\n**Walk me through:**\n1. **How would you define \"burn rate\"** in this context, and why is it better than just checking current error rates?\n2. **What would be a reasonable alerting strategy?** (e.g., at what burn rate multiples would you trigger alerts, and over what time windows?)\n3. **How would you implement this in Prometheus/Sloth,** or if you've used similar tools, what challenges did you encounter?\n\n---\n\n## Why This Question Works\n\n\u2705 **Practical & Real** \u2013 Senior engineers actually design these systems  \n\u2705 **Multi-layered** \u2013 Tests conceptual understanding, judgment, and implementation knowledge  \n\u2705 **Time-appropriate** \u2013 A strong candidate answers in 3-4 minutes with specifics  \n\u2705 **Reveals depth** \u2013 Distinguishes between someone who's",
        "scores": {
          "relevance": 9,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core SLO/burn rate alerting skills for the role's infrastructure focus, matches Kubernetes/Prometheus context (-1 for minor unlisted 'Sloth' tool). Clarity: Well-structured but multiple bundled sub-questions (-2) and lacks strict time/format constraints. Discriminative: Excellent at separating candidates via definitions, judgment on thresholds/windows, and real implementation trade-offs/challenges.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-460",
        "role_title": "Backend Engineer - Experiment Tracking",
        "question": "# Technical Screening Question\n\n**You're building an experiment tracking service where data scientists log thousands of metrics per experiment run. A team member proposes storing all metrics in a single PostgreSQL table with columns: `run_id`, `metric_name`, `metric_value`, `timestamp`.**\n\n**What issues might this approach have as the system scales, and how would you redesign it?**\n\n---\n\n## Why This Question Works:\n\n\u2705 **Tests practical architecture thinking** \u2013 Assesses ability to anticipate scaling issues, not memorize syntax\n\n\u2705 **Mid-level appropriate** \u2013 Expects knowledge of database design and ML workflows, but not deep distributed systems expertise\n\n\u2705 **Real-world relevance** \u2013 Directly mirrors decisions made when building experiment tracking systems (similar to MLflow's actual design challenges)\n\n\u2705 **Open-ended** \u2013 Reveals how candidates think about tradeoffs (normalization vs. query performance, time-series data handling, retention policies)\n\n---\n\n## What You're Listening For:\n\n- **Good signals:** Time-series considerations, index strategies, schema normalization, partitioning by run_id or time, separate tables for different metric",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core skills for ML experiment tracking infrastructure (metric logging, scaling), matches role exactly, no red flags. Clarity: Crystal clear scenario and ask, well-scoped for screening, minor deduction for no explicit time/format constraints. Discriminative: Excellent at revealing trade-offs (schema, partitioning, time-series), strong separation of mid-level thinkers, open-ended but focused.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-461",
        "role_title": "Senior Backend Engineer - Fraud Detection",
        "question": "# Screening Question\n\n**Walk me through how you'd design a real-time fraud detection system that needs to make a pass/fail decision on a payment transaction in under 100ms. What are the key components, and where would you use Redis, Kafka, and your ML model in this pipeline?**\n\n---\n\n## Why This Question Works:\n\n\u2705 **Tests core competencies:**\n- Real-time system design (critical for payments)\n- Understanding of the tech stack and *why* each tool fits\n- ML model deployment trade-offs\n- Latency constraints in production\n\n\u2705 **Reveals seniority through depth:**\n- Junior: \"Run the model and return a score\"\n- Senior: Discusses caching strategies, async scoring, fallback rules, batch vs. real-time tradeoffs, model serving architecture\n\n\u2705 **Practical & answerable:** Candidates can outline their approach in 3-4 minutes and go deeper on areas they know well\n\n\u2705 **Conversation starter:** Natural follow-ups on model versioning, handling feature freshness, manual review workflows, etc.",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core fraud detection pipeline skills with exact tech stack (Redis, Kafka, ML) and real-time constraints from role. Clarity: Crystal clear with 100ms scope, but minor deduction for no explicit answer format/depth. Discriminative: Strong separation via trade-offs, component reasoning, and latency handling; reveals seniority without being too broad.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-462",
        "role_title": "Backend Engineer - Alert Routing",
        "question": "# Screening Question\n\n**You're designing an alert routing system where multiple alerts from different services need to be deduplicated and grouped before being sent to PagerDuty. An alert for a database connection timeout might come from 5 different services simultaneously, but you only want to trigger one PagerDuty incident.**\n\n**How would you approach deduplication in this system? Walk me through:**\n1. **What would you use as a deduplication key?**\n2. **Where in your architecture would you implement this check\u2014before or after Alertmanager?**\n3. **How would you handle the storage and TTL of these deduplication records?**\n\n---\n\n## Why This Question Works\n\n- **Practical & Real:** Directly mirrors the actual problem described in the job\n- **Shows System Design Thinking:** Evaluates architectural decisions, not just syntax\n- **Mid-Level Appropriate:** Requires Go/backend experience but doesn't demand expert-level mastery\n- **Reveals Technical Depth:** Answers expose knowledge of:\n  - Alert fingerprinting/grouping concepts\n  - Trade-offs between Alertmanager config vs custom logic",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (alert deduplication/grouping) with PagerDuty mention; minor ding for assuming Alertmanager familiarity not explicit in role (-1). Clarity: Crystal clear, well-scoped multi-part structure with no ambiguity. Discriminative: Excellent separation via trade-offs (keys, architecture placement, storage/TTL), reveals deep applied knowledge beyond recall.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-463",
        "role_title": "Junior Backend Engineer - Full-text Search",
        "question": "# Screening Question\n\n**You're building a search feature for a product catalog. A user searches for \"red shoes\" and you need to return results ranked by relevance. Walk me through how you'd implement this in PostgreSQL using full-text search, and explain what a `tsvector` is and why it matters.**\n\n---\n\n## Evaluation Criteria\n\n**Strong Answer** includes:\n- Explains `tsvector` as a preprocessed text format optimized for search (tokenization, lemmatization)\n- Mentions creating a `tsvector` column or generating it on-the-fly with `to_tsvector()`\n- References `tsquery` for converting search input\n- Discusses ranking (e.g., `ts_rank()` or `ts_rank_cd()`) to order results by relevance\n- Shows awareness of indexing with GIN or GIST for performance\n\n**Acceptable Answer** includes:\n- Core concept of `tsvector` as processed text\n- Basic query matching using `@@` operator\n- At least mentions ranking or indexing\n\n**Red Flags:**\n- Confuses `tsvector` with regular string",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests PostgreSQL full-text search core skills (tsvector, ranking) from role description, no red flags. Clarity: Crystal clear and well-scoped for screening, minor deduction for lacking explicit time constraint. Discriminative: Requires applied knowledge and trade-offs (e.g., indexing choices), strong separation via detailed strong/acceptable criteria provided.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-465",
        "role_title": "Backend Engineer - CDC Pipelines",
        "question": "# Screening Question\n\n**\"Walk me through how you would handle a schema change (e.g., adding a new column) in a source PostgreSQL table when you have an active Debezium CDC pipeline running to Snowflake. What could go wrong, and how would you ensure no data is lost?\"**\n\n---\n\n## Why This Question Works\n\n**Practical & Real:** Schema evolution is a constant challenge in production CDC pipelines\u2014not theoretical.\n\n**Mid-Level Appropriate:** Tests understanding of:\n- Debezium's behavior during schema changes\n- Downstream implications (Snowflake compatibility)\n- Data loss risks and recovery strategies\n- Operational problem-solving\n\n**Opens Conversation:** Candidates can discuss:\n- Signal tables / schema history topics\n- Backfilling strategies\n- Rolling deployments or pipeline restarts\n- Exactly-once semantics under schema changes\n\n**Time-Boxed:** Naturally fits a 3-5 minute discussion with follow-ups like:\n- \"How would you test this?\"\n- \"What if the column is NOT NULL?\"\n- \"How do you monitor for pipeline lag during this?\"\n\nThis reveals both depth",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's CDC pipelines, schema changes, PostgreSQL/Debezium/Snowflake stack. Clarity: Crystal clear scenario with example, well-scoped for screening; minor deduction for no explicit time constraint. Discriminative: Tests deep applied knowledge of schema evolution risks, recovery, trade-offs\u2014strong separation of mid-level expertise.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-467",
        "role_title": "Backend Engineer - Connection Pooling",
        "question": "# Screening Question\n\n**You're implementing a connection pooling layer for a high-traffic application using PgBouncer. Your application has 50 concurrent users, but you're seeing connection timeouts and occasional \"too many connections\" errors even though your PostgreSQL max_connections is set to 200.**\n\n**Walk me through how you would:**\n1. **Diagnose** what's happening (what metrics/logs would you check?)\n2. **Configure** PgBouncer pool_size and reserve_pool_size to fix it\n3. **Explain** the tradeoff you're making with your chosen pool size\n\n---\n\n## Why This Question Works\n\n\u2705 **Practical** \u2013 Tests real troubleshooting skills, not theoretical knowledge  \n\u2705 **Appropriate Level** \u2013 Mid-level should understand pooling concepts and basic PgBouncer config  \n\u2705 **Multi-layered** \u2013 Assesses diagnosis, configuration, and architectural thinking  \n\u2705 **Time-bound** \u2013 Answerable with concrete examples in 3-4 minutes  \n\u2705 **Role-relevant** \u2013 Directly tests the core responsibility (pool",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core connection pooling, PgBouncer config, sizing, and tradeoffs matching role's pool sizing/failover focus (-0 red flags). Clarity: Well-scoped with numbered parts, specific scenario, no ambiguity. Discriminative: Requires diagnosis (applied knowledge), config decisions, and tradeoff explanation, strongly separating candidates with deep understanding from rote learners.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-464",
        "role_title": "Staff Engineer - Presence Systems",
        "question": "# Screening Question\n\n**Design a heartbeat protocol for a presence system that needs to handle 100K concurrent users. Walk me through:**\n\n1. **What information would you include in each heartbeat message, and why?**\n2. **How would you handle a client that stops sending heartbeats\u2014what's your timeout strategy?**\n3. **How would you reduce bandwidth while keeping presence data reasonably fresh?**\n\n**Follow-up:** *If a user has flaky network connectivity and heartbeats are dropping 30% of the time, how would that change your design?*\n\n---\n\n## Why This Works\n\n\u2705 **Practical & Real** \u2014 Staff engineers make these architectural tradeoffs daily  \n\u2705 **Shows Systems Thinking** \u2014 Reveals how they balance freshness, cost, and reliability  \n\u2705 **Appropriate Scope** \u2014 Not a deep-dive, but exposes their reasoning at scale  \n\u2705 **Seniority Signal** \u2014 Listen for mentions of:\n- Exponential backoff, jitter\n- State machine thinking (connected \u2192 stale \u2192 offline)\n- Tradeoffs between accuracy and resource cost\n-",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Directly targets core role skills (heartbeat protocols, presence at scale, connection management) with no red flags. Crystal clear structure with focused sub-questions and scoped follow-up; no ambiguity or overload. Strong separation via trade-offs (bandwidth, timeouts, flaky networks), state machines, backoff\u2014reveals senior systems thinking vs rote knowledge.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-466",
        "role_title": "Senior Backend Engineer - GitOps Tooling",
        "question": "# Technical Screening Question\n\n**\"Walk me through how you would design a reconciliation loop for a GitOps tool that detects and corrects drift between desired state (in Git) and actual state (in a Kubernetes cluster). What are the key components, and what challenges would you anticipate around frequency, performance, and handling conflicts?\"**\n\n---\n\n## Why This Question Works\n\n**Practical & Senior-Level:**\n- Tests hands-on understanding of core GitOps concepts, not just terminology\n- Requires systems thinking (architecture, trade-offs, edge cases)\n- Appropriate for someone who's built or debugged similar systems\n\n**Observable Skills:**\n- Understands reconciliation patterns (controller loops, watch mechanisms)\n- Thinks about operational trade-offs (sync frequency vs. load, false positives)\n- Considers real-world complications (failed reconciliations, user-initiated changes, resource limits)\n- Likely mentions: controllers, informers/watches, exponential backoff, conflict resolution strategies\n\n**Time-Appropriate:**\n- Can sketch a solid answer in 2-5 minutes\n- Allows follow-ups on specifics (ArgoCD's approach",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Directly targets core GitOps reconciliation loops, drift detection, and trade-offs matching role's declarative infra focus (no red flags). Crystal clear with specific scope (components, challenges: frequency/performance/conflicts), minor deduction for lacking explicit time/format constraints. Strong separation via trade-offs, edge cases, and applied systems knowledge; not recall/trivia, scannable in <10min.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-468",
        "role_title": "Junior Backend Engineer - CI Pipelines",
        "question": "# Technical Screening Question\n\n**\"Walk me through how you would set up a GitHub Actions workflow that builds a Docker image and runs Python unit tests in parallel across 3 different test suites. What would you include to make sure the workflow doesn't rebuild the Docker image if the Dockerfile hasn't changed?\"**\n\n---\n\n## Why This Question Works:\n\n**Tests practical skills:**\n- GitHub Actions workflow syntax (job configuration, steps)\n- Docker integration in CI pipelines\n- Job parallelization (matrix strategy)\n- Caching strategies (layer caching, dependency caching)\n\n**Appropriate for junior level:**\n- Not asking for advanced optimization techniques\n- Tests fundamentals they'd use day-one\n- Allows candidates to show what they know without requiring production experience\n\n**Realistic scenario:**\n- Mirrors actual tasks in the role (Docker builds + test parallelization)\n- Shows how they think about pipeline efficiency\n\n**Flexible depth:**\n- Junior candidates can answer at basic level (matrix jobs, basic caching)\n- Strong candidates naturally mention docker/build-cache action, artifacts, or conditional steps\n- Easy to probe deeper with follow-ups",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Directly tests core role skills (GitHub Actions workflows, Docker builds, test parallelization, caching) for entry-level CI pipeline engineer. Crystal clear with specific scenario and efficiency constraint; minor deduction for lacking explicit time/format constraints. Strong separation via trade-offs in caching/parallelization strategies, flexible for junior levels without being trivial.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-470",
        "role_title": "Backend Engineer - GraphQL Federation",
        "question": "# Screening Question: Backend Engineer - GraphQL Federation\n\n## Question:\n\n**You have two federated subgraphs: a \"Users\" subgraph and an \"Orders\" subgraph. The Users subgraph defines a `User` entity with a `@key` directive, and the Orders subgraph needs to reference User data.**\n\n**Walk me through:**\n1. **How would you define the `@key` directive on the User entity, and why is it necessary?**\n2. **How would you extend the User type in the Orders subgraph to add order-related fields?**\n3. **What would happen if a gateway query requests both user information and their orders - briefly describe the query planning process.**\n\n---\n\n## Why This Works:\n\n\u2705 **Practical & Scenario-Based** \u2014 Tests real federation patterns candidates will implement daily\n\n\u2705 **Multi-Layered** \u2014 Assesses understanding of keys, entity references, and gateway query execution\n\n\u2705 **Time-Appropriate** \u2014 Answerable in 3-4 minutes with concrete code examples or explanations\n\n\u2705 **Mid-Level Calibration** \u2014 Doesn",
        "scores": {
          "relevance": 7,
          "clarity": 8,
          "discriminative": 8,
          "reasoning": "Relevance: Directly tests core GraphQL federation skills (schema stitching, entity resolution, query planning) but deduct -3 for generic multi-part structure applicable to many backend roles. Clarity: Well-scoped and unambiguous, but -2 for bundling 3 questions into one. Discriminative: Strong separation via applied knowledge of federation mechanics and trade-offs, not just recall; scenario-based with good depth for screening.",
          "composite": 7.666666666666667
        }
      },
      {
        "role_id": "be-472",
        "role_title": "Backend Engineer - Vector Search",
        "question": "# Screening Question\n\n**You're implementing a hybrid search system that combines keyword matching with semantic similarity. A user searches for \"affordable running shoes,\" but your vector search returns results for \"expensive athletic footwear\" with high semantic similarity, while keyword search correctly filters by price. How would you structure this to balance both signals, and what are the trade-offs you'd consider?**\n\n---\n\n## Why This Works:\n\n\u2705 **Practical** \u2013 Tests real-world problem-solving in vector search systems  \n\u2705 **Mid-level appropriate** \u2013 Requires understanding of embeddings + ranking, but not deep ML theory  \n\u2705 **Time-scoped** \u2013 Can be answered meaningfully in 2-5 minutes  \n\u2705 **Role-specific** \u2013 Directly relevant to hybrid search, index management, and semantic limitations  \n\u2705 **Open-ended** \u2013 Reveals thinking about weighting strategies, re-ranking, and system design\n\n## What You're Listening For:\n\n- Understanding that semantic similarity \u2260 intent matching\n- Awareness of ranking/weighting approaches (score normalization, fusion algorithms)\n- Knowledge of when keyword search catches what vectors",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core hybrid search, semantic retrieval, and vector limitations from role (-0 red flags). Clarity: Crystal clear scenario with precise scope for screening. Discriminative: Probes trade-offs in weighting/fusion/ranking, revealing deep applied understanding vs rote knowledge.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-473",
        "role_title": "Senior Backend Engineer - Refund Processing",
        "question": "# Screening Question\n\n**Design a refund processing system that needs to handle partial refunds and ensure ledger consistency. Walk me through:**\n\n1. **How would you structure the data model** to support multiple partial refunds against a single transaction, ensuring you can always answer \"what's the refund status of this order?\"\n\n2. **What consistency challenges do you anticipate** when a partial refund is initiated but the provider callback hasn't arrived yet? How would you handle a callback that arrives out of order?\n\n3. **How would you use Kafka and/or a database transaction** to ensure the ledger entry and refund record stay in sync, especially if the service crashes between these two operations?\n\n---\n\n## Why This Question Works\n\n\u2713 **Practical depth:** Tests real architectural decisions (data modeling, eventual consistency, idempotency)  \n\u2713 **Seniority-appropriate:** Expects knowledge of distributed systems trade-offs, not just syntax  \n\u2713 **Stack-relevant:** Naturally incorporates Kafka, PostgreSQL, and ledger thinking  \n\u2713 **Open-ended:** Senior engineers show depth through how they prioritize concerns (consistency vs.",
        "scores": {
          "relevance": 7,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Tests core refund/ledger skills but assumes Kafka/PostgreSQL not explicitly listed (-2 for unlisted tech, -1 generic distributed design). Clarity: Crystal clear, well-scoped sub-questions with precise intent. Discriminative: Excellent at probing trade-offs in consistency, out-of-order events, and sync mechanisms, strongly separates seniors.",
          "composite": 8.333333333333334
        }
      },
      {
        "role_id": "be-476",
        "role_title": "Backend Engineer - Live Updates",
        "question": "# Technical Screening Question\n\n## Question:\n\n**\"Walk us through how you'd implement a feature where different users see different real-time updates based on their assigned dashboard. For example, User A should only receive updates for Dashboard 1, while User B only gets updates for Dashboard 2. How would you use Socket.io rooms to prevent data leakage, and what would you do if a user's dashboard assignment changes while they're connected?\"**\n\n---\n\n## Why This Works:\n\n\u2705 **Practical & Real-World** \u2014 Directly mirrors the job's core responsibility  \n\u2705 **Tests Multiple Competencies** \u2014 Room management, broadcasting logic, reconnection handling  \n\u2705 **Appropriate Depth** \u2014 Mid-level should know Socket.io basics but may need to think through edge cases  \n\u2705 **Timeboxed** \u2014 Can be answered with a clear architecture in 3-4 minutes  \n\u2705 **Reveals Problem-Solving** \u2014 Shows how they handle permission boundaries and state transitions\n\n---\n\n## What You're Looking For:\n\n- **Strong Answer:** Uses Socket.io `join()`/`leave()` rooms, considers leaving",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Perfect relevance to role's core needs (rooms, selective broadcasting, reconnections). Crystal clear with specific Socket.io focus and edge case, well-scoped for screening. Excellent at distinguishing via trade-offs in room management and state changes; minor subtract for lacking explicit time/format constraints.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-469",
        "role_title": "Senior Backend Engineer - Chaos Engineering",
        "question": "# Screening Question\n\n**You're designing a chaos experiment to test the resilience of a microservices platform. Walk me through how you would approach injecting a network partition between two critical services for 2 minutes, then validate that the system recovered correctly. What would you monitor, and how would you ensure you don't accidentally take down the entire platform?**\n\n---\n\n## Why This Question Works\n\n**Practical & Open-Ended:** Tests real chaos engineering thinking\u2014not just tool knowledge\n- Covers blast radius control (critical for senior level)\n- Requires understanding of observability (Prometheus/Grafana)\n- Allows candidates to discuss Chaos Mesh implementation naturally\n\n**Senior-Level Indicators to Listen For:**\n- Mention of blast radius limiting (namespace selectors, label targeting)\n- Observability strategy (what metrics matter, when to abort)\n- Recovery validation (how do you know when it's safe?)\n- Rollback/circuit breaker mentality\n- Discussion of staged rollout or dry-run approaches\n\n**Time-Appropriate:** 2-5 minute response with natural follow-ups available (Chaos Mesh manifest details, Go-based tool",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Directly tests core chaos engineering skills (failure injection, blast radius control, recovery validation) matching role; crystal clear with specific scenario, duration, and constraints; strong separation via trade-offs in monitoring, safety, and validation\u2014senior indicators like namespace targeting distinguish experts. Minor ding for assuming microservices without role confirmation, but excellent overall.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-475",
        "role_title": "Junior Backend Engineer - Search Analytics",
        "question": "# Technical Screening Question\n\n**\"Walk me through how you would track and analyze search queries in a system. Specifically, if you wanted to identify which search terms users click on the most, what data would you need to collect, and how would you structure it in a database?\"**\n\n---\n\n## Evaluation Rubric\n\n**Strong Answer (Hire Signal):**\n- Mentions capturing both query text and click events with timestamps\n- Suggests linking queries to results clicked via some identifier (query ID, session ID, etc.)\n- Proposes aggregating/counting clicks by query term\n- References a reasonable schema (e.g., separate tables for queries and clicks, or a denormalized approach)\n\n**Acceptable Answer (Proceed):**\n- Understands the need to log queries and clicks\n- Can describe a basic approach to counting popular queries\n- May lack detail on schema design but shows foundational thinking\n\n**Weak Answer (Red Flag):**\n- Only mentions logging queries without connecting to user actions\n- No consideration of how to relate queries to clicks\n- Vague on database structure or data relationships\n\n---\n\n## Why This Question Works\n\n\u2713 Tests real, day-",
        "scores": {
          "relevance": 9,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core search analytics skills (query logging, click tracking) for the role (-1 for no specific tech stack mention). Clarity: Clear intent and specific focus, but lacks explicit constraints like scale or format (8). Discriminative: Strong separation via trade-offs in data collection, linking, and schema design; rubric shows good candidate differentiation.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-474",
        "role_title": "Backend Engineer - Dashboards API",
        "question": "# Technical Screening Question\n\n**You're building an API endpoint that retrieves a dashboard with templated variables (like `$datasource` and `$environment`). A user requests the dashboard with query parameters `?datasource=prometheus&environment=prod`.**\n\n**Walk me through:**\n1. How would you handle variable substitution in the dashboard template?\n2. What would you consider for caching this endpoint, and what cache invalidation strategy would you use?\n3. What potential issues might arise if multiple dashboards share variable definitions?\n\n---\n\n## Why This Question Works:\n\n\u2705 **Practical & Real** \u2014 Directly reflects work they'd do (templating, variables, caching)  \n\u2705 **Mid-Level Appropriate** \u2014 Tests design thinking beyond just coding syntax  \n\u2705 **Stack-Relevant** \u2014 Naturally leads to discussing Redis caching, PostgreSQL storage, and Grafana patterns  \n\u2705 **Multi-Layered** \u2014 Allows candidates to show depth while keeping it conversational (2-5 min answer)  \n\u2705 **Reveals Problem-Solving** \u2014 Shows how they think about performance, data consistency",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core dashboard templating, variable substitution, caching, and sharing\u2014matches role exactly (-0 red flags). Clarity: Well-scoped with concrete scenario, clear numbered parts, no ambiguity or bundling issues. Discriminative: Multi-part probes applied knowledge, trade-offs (caching/invalidation), and edge cases (shared vars), strongly separates mid-level thinkers.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-477",
        "role_title": "Senior Backend Engineer - Data Catalog",
        "question": "# Technical Screening Question\n\n**Scenario-based question:**\n\n\"Walk me through how you would design a metadata extraction pipeline that automatically captures lineage information when data flows through multiple Kafka topics into PostgreSQL tables. \n\nSpecifically:\n- How would you identify and track the relationship between source topics and destination tables?\n- What metadata would you extract and store in Apache Atlas?\n- How would you handle schema changes or topic reconfigurations without breaking the lineage?\"\n\n---\n\n## Why this works:\n\n\u2705 **Practical & Role-Specific** \u2014 Directly applies to data catalog, lineage tracking, and the exact tech stack (Kafka \u2192 PostgreSQL \u2192 Atlas)\n\n\u2705 **Senior-Level** \u2014 Requires architectural thinking, not just tool knowledge; tests design decisions and trade-offs\n\n\u2705 **Answerable in 2-5 mins** \u2014 Candidate can outline approach without writing code; allows follow-ups to probe depth\n\n\u2705 **Reveals Real Experience** \u2014 Distinguishes between someone who's built similar systems vs. someone who's only read documentation",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfect match to role's data catalog, lineage tracking, and exact tech stack (Kafka, PostgreSQL, Atlas). Clarity: Crystal clear scenario with specific sub-questions; minor deduction for lacking explicit time/format constraints. Discriminative: Excellent at testing architectural trade-offs, real-world handling of schema changes, and deep understanding vs. superficial knowledge.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-471",
        "role_title": "Staff Engineer - Zero Trust Architecture",
        "question": "# Screening Question\n\n**You're designing a zero trust architecture for a large microservices platform running on Kubernetes. Your team wants to enforce mutual TLS (mTLS) between all services, but you've identified a bootstrapping problem: when a new service pod starts, it needs a valid certificate to communicate, but it can't authenticate to get one without already having credentials.**\n\n**Walk me through how you'd solve this using SPIFFE, and explain the trade-offs of your approach compared to alternatives like sidecar injection or init containers.**\n\n---\n\n## Why This Question Works\n\n**For Staff Level:**\n- Requires systems thinking (bootstrapping paradox isn't trivial)\n- Tests understanding of SPIFFE's design philosophy, not just syntax\n- Expects them to articulate trade-offs\u2014a staff-level responsibility\n\n**Practical & Scoped:**\n- Solves a real problem they'll face on day one\n- Answerable in 2-5 minutes without whiteboarding\n- Reveals how they think about identity federation\n\n**What You're Listening For:**\n- \u2705 Mention of **Workload API** and how SPIFFE si",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Relevance: Perfectly matches role's zero trust, mTLS, and identity focus; directly tests SPIFFE for microservices auth. Clarity: Crystal clear with precise scenario and scope, minor truncation in 'why' section irrelevant. Discriminative: Excellent at separating via systems thinking, trade-offs, and deep SPIFFE understanding\u2014no recall pitfalls.",
          "composite": 9.666666666666666
        }
      },
      {
        "role_id": "be-478",
        "role_title": "Backend Engineer - Cost Attribution",
        "question": "# Technical Screening Question\n\n**Scenario:** Your cost attribution system needs to allocate costs for a shared Kubernetes cluster across 10 teams. Some pods have clear team labels, but about 30% lack proper tagging. Additionally, you need to handle shared infrastructure costs (control plane, networking) that don't map to individual pods.\n\n**Question:** Walk me through how you would design a solution to:\n1. Handle the untagged pods in a way that's fair but traceable\n2. Allocate shared infrastructure costs across teams\n3. What data would you store in PostgreSQL, and how would you structure it to make cost queries flexible?\n\n---\n\n## Why This Works:\n\n\u2705 **Tests practical mid-level experience** \u2013 Real problem they'll face, not theoretical  \n\u2705 **Covers core stack** \u2013 Kubernetes labels, PostgreSQL schema design, cost logic  \n\u2705 **Reveals decision-making** \u2013 How they balance accuracy, traceability, and maintainability  \n\u2705 **Answerable in timeframe** \u2013 High-level design, not implementation details  \n\u2705 **Role-appropriate** \u2013 Directly tied to \"cost",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (K8s tagging, cost allocation, PostgreSQL for reports); no red flags (-0). Clarity: Well-scoped scenario with 3 clear sub-questions; no bundling or ambiguity issues (-0). Discriminative: Probes trade-offs in fairness/traceability, allocation logic, and schema design for flexibility; strong separation of deep thinkers (-0). Exceptional question for the role.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-480",
        "role_title": "Junior Backend Engineer - Deployment Automation",
        "question": "# Screening Question\n\n**You need to deploy a web application to three different AWS environments (dev, staging, production). Each environment requires different configuration settings (database URLs, API keys, logging levels, etc.), but the application code itself remains the same.**\n\n**How would you approach this using Ansible, and what would be your strategy for managing these environment-specific configurations?**\n\n---\n\n## Why This Question Works:\n\n\u2705 **Practical & Real-World** \u2014 Directly mirrors actual deployment scenarios  \n\u2705 **Appropriate Level** \u2014 Doesn't require advanced expertise, but shows if they understand core concepts  \n\u2705 **Covers Key Skills** \u2014 Tests understanding of Ansible, configuration management, and deployment strategy  \n\u2705 **Open-Ended** \u2014 Allows candidates to demonstrate their thinking process  \n\u2705 **Time-Appropriate** \u2014 Answerable in 2-5 minutes with room for follow-ups\n\n---\n\n## What You're Looking For:\n\n**Strong Answer Indicators:**\n- Mentions Ansible inventory groups (dev, staging, prod)\n- References `group_vars` or `host_vars` for environment-specific configs\n-",
        "scores": {
          "relevance": 7,
          "clarity": 9,
          "discriminative": 8,
          "reasoning": "Relevance: Tests core configuration management and deployment skills for the role, but Ansible isn't explicitly listed in role description (-2); not generic but assumes specific tool. Clarity: Crystal clear scenario, well-scoped, precise expectations. Discriminative: Requires applied Ansible knowledge and trade-offs (e.g., group_vars strategy), distinguishes candidates familiar with practical deployment from theorists.",
          "composite": 8.0
        }
      },
      {
        "role_id": "be-479",
        "role_title": "Principal Engineer - Data Architecture",
        "question": "# Screening Question\n\n**You're architecting a system that needs to handle both real-time transactional writes (millions of orders/day) and complex analytical queries (daily reports on 6+ months of historical data). Walk me through how you'd approach this problem\u2014what are the key trade-offs you'd consider, and how would PostgreSQL and ClickHouse fit into your solution?**\n\n---\n\n## Why This Question Works\n\n**Tests Principal-Level Thinking:**\n- Requires understanding of OLTP vs OLAP tradeoffs (not just knowing the difference)\n- Evaluates strategic decision-making, not just technical knowledge\n- Appropriate for someone who will guide architecture decisions for a team\n\n**Practical & Realistic:**\n- Common real-world scenario (lambda/kappa architecture patterns)\n- Lets candidate showcase depth in the tech stack provided\n- Can discuss: data pipeline design, eventual consistency, tool selection rationale\n\n**Time-Appropriate:**\n- Can be answered substantively in 2-5 minutes\n- Natural follow-ups available (Kafka's role, scaling considerations, etc.)\n- No \"gotcha\" trivia\u2014open-ended enough",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's data architecture for analytics/transactional workloads, directly tests PostgreSQL/ClickHouse in context (no red flags). Clarity: Crystal clear scenario and scope, minor deduction for no explicit time/format constraints. Discriminative: Excellent at probing trade-offs, strategic thinking, and applied knowledge to separate principal-level candidates from others.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-481",
        "role_title": "Backend Engineer - Canary Deployments",
        "question": "# Screening Question\n\n**You're implementing a canary deployment where you need to automatically rollback if the new version's error rate exceeds the stable version's by more than 5%. Walk me through how you'd set this up with Istio and Prometheus\u2014specifically, what metrics you'd query, how you'd compare them, and where you'd implement the decision logic.**\n\n---\n\n## Why This Works\n\n**Practical Application:** Tests real canary deployment thinking\u2014the candidate must understand the full feedback loop from metrics collection through automated decision-making.\n\n**Appropriate Depth for Mid-Level:** \n- Assumes familiarity with Istio traffic splitting and Prometheus queries\n- Doesn't require deep implementation details, but reveals gaps quickly\n- Mid-level engineers should know *where* logic lives (Prometheus rule, ArgoCD hook, custom controller)\n\n**Reveals Key Competencies:**\n- Metric selection (error rate, how to calculate per-version)\n- Istio concepts (VirtualService traffic weights, how to query per-canary metrics)\n- Integration thinking (how do these components talk?)\n- Automation mindset (not manual monitoring)\n\n**",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core canary deployment skills (traffic shifting, metrics, rollback) with exact tech stack (Istio, Prometheus) from role; no red flags (-0). Clarity: Precise scope on metrics, comparison, logic placement; well-formed for screening, no bundling or vagueness (-0). Discriminative: Requires applied knowledge of integrations, trade-offs in metric selection/automation; distinguishes mid-level expertise effectively (-0).",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-483",
        "role_title": "Backend Engineer - WAF Integration",
        "question": "# Screening Question\n\n**You're integrating ModSecurity into an Nginx reverse proxy to protect an API. The security team reports that legitimate requests from a partner's webhook service are being blocked by ModSecurity rules, causing false positives. Walk me through your approach to diagnosing and resolving this issue while maintaining security.**\n\n---\n\n## Why This Works\n\n**Practical depth:** Tests real-world WAF troubleshooting that mid-level engineers encounter regularly\n\n**Seniority-appropriate:** Requires understanding of:\n- ModSecurity rule mechanics and logging\n- Nginx configuration and request flow\n- How to balance security vs. availability\n- Root cause analysis methodology\n\n**Reveals important competencies:**\n- Can they interpret WAF logs effectively?\n- Do they understand rule exclusions and tuning?\n- How do they approach security trade-offs?\n- Can they communicate with security teams?\n\n**Answerable in timeframe:** Candidates can outline key steps (check audit logs \u2192 identify triggering rule \u2192 implement exception) without needing to write code",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core WAF integration, rule management, and balancing security/availability from role (-1 for assuming ModSecurity/Nginx not explicitly listed). Clarity: Crystal clear scenario with well-scoped troubleshooting task. Discriminative: Requires applied knowledge of logs, rules, exclusions, and trade-offs, strongly separating capable engineers.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-484",
        "role_title": "Staff Engineer - Model Registry",
        "question": "# Technical Screening Question\n\n**\"Walk me through how you would design a model registry system that handles concurrent model uploads from multiple teams while maintaining version integrity and preventing accidental deployments of untested models. What are the key components you'd implement, and where would you expect bottlenecks or failure modes?\"**\n\n---\n\n## Why This Question Works:\n\n**Practical & Architectural:** Tests real-world system design rather than memorized facts about MLflow.\n\n**Seniority-Appropriate:** Expects staff-level thinking about concurrency, governance, and trade-offs\u2014not just implementation details.\n\n**Conversational:** Naturally opens follow-ups on:\n- Database schema decisions (PostgreSQL locking, transactions)\n- Workflow orchestration (approval gates, automation)\n- Scalability concerns (S3 artifacts, metadata storage)\n- Safety mechanisms (version tags, rollback strategies)\n\n**Time-Bound:** Candidate can outline a solid answer in 2-5 minutes, with depth emerging naturally in discussion.\n\n**Tech Stack Coverage:** Touches Python/MLflow, PostgreSQL, S3, and implicit Kubernetes deployment concerns without forcing artificial references.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Directly tests core role skills in model registry architecture, versioning, deployment automation, and governance. Crystal clear with focused scope on concurrency, integrity, and failure modes. Excellent at probing trade-offs, bottlenecks, and deep architectural thinking to separate strong architects from average engineers. Minor ding on relevance for not explicitly naming role's tech stack (-1).",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-485",
        "role_title": "Junior Backend Engineer - Payment Webhooks",
        "question": "# Technical Screening Question\n\n**\"Walk me through how you would implement idempotency for a payment webhook handler. Specifically, what would you store, where would you store it, and how would you handle a duplicate webhook arriving?\"**\n\n---\n\n## Why This Question Works:\n\n\u2705 **Tests core competency** - Idempotency is fundamental to payment processing and directly mentioned in the role focus\n\n\u2705 **Practical and realistic** - Handles a real problem they'll face (payment providers retry webhooks)\n\n\u2705 **Right difficulty level** - Junior engineers may not know the answer, but can reason through it with guidance\n\n\u2705 **Opens conversation** - Allows follow-ups about trade-offs (Redis vs. DB), TTL strategies, or race conditions\n\n\u2705 **Evaluates system thinking** - Shows if they understand *why* this matters (financial accuracy, duplicate charges)\n\n---\n\n## What You're Looking For:\n\n- **Good answer**: \"Store the webhook ID in Redis/DB, check before processing, return success if duplicate\"\n- **Great answer**: Mentions TTL, eventual consistency, or idempotency keys from payment provider",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests idempotency, a core skill explicitly listed in role description for payment webhooks (no red flags). Clarity: Crystal clear and well-scoped, minor deduction for lacking explicit constraints like tech stack. Discriminative: Strong separation via applied knowledge, trade-offs (storage, TTL, duplicates), and reasoning; not trivial recall.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-486",
        "role_title": "Backend Engineer - Span Processing",
        "question": "# Screening Question\n\n**You're designing a tail sampling processor in Go that needs to decide whether to keep or drop traces based on error rates and latency percentiles. The sampler receives spans from Kafka and must make decisions within 100ms to avoid blocking the pipeline.**\n\n**Walk me through:**\n\n1. **How would you structure the sampler to aggregate span data efficiently?** (What data do you need to track, and how do you avoid memory bloat?)\n\n2. **What challenges arise when sampling decisions depend on data you might not have received yet?** (e.g., a trace's final latency or whether it contains errors)\n\n3. **How would you handle the case where a sampling decision arrives too late\u2014after some spans have already been dropped?**\n\n---\n\n## Why This Question Works\n\n- **Practical**: Tests real pain points in building observability pipelines\n- **Multi-layered**: Can't just recite OpenTelemetry docs\u2014requires systems thinking\n- **Level-appropriate**: Mid-level engineers should know basic concepts but might struggle with the edge cases\n- **Time-bounded**: Candidate can answer parts of it at different depths (2-5 min",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (tail sampling, span processing) with Go, Kafka, and distributed tracing constraints; minor ding for not mentioning storage/attributes (-1). Clarity: Well-scoped with 100ms limit, clear multi-part structure without bundling overload. Discriminative: Probes deep systems thinking, trade-offs (memory, incomplete data, late decisions), strong separation beyond rote knowledge.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-482",
        "role_title": "Senior Backend Engineer - API Monetization",
        "question": "# Screening Question\n\n**\"Walk me through how you would design a rate-limiting and quota enforcement system for an API monetization platform. Specifically, consider: (1) how you'd track usage across different pricing tiers in real-time, (2) where you'd store this data, and (3) how you'd handle the trade-off between accuracy and performance when a customer approaches their quota limit.\"**\n\n---\n\n## Why This Question Works\n\n**Practical & Relevant:** Directly addresses core responsibilities (quota management, usage tracking) without being contrived.\n\n**Seniority-Appropriate:** Senior engineers should think about architectural trade-offs, not just implement a basic counter. Expect discussion of:\n- Redis for low-latency tracking vs. PostgreSQL for durability\n- Eventual consistency vs. strict enforcement\n- Rate limiting algorithms (sliding window, token bucket, etc.)\n- Handling distributed systems challenges\n\n**Reveals Key Competencies:**\n- System design thinking\n- Understanding of Go/Redis/PostgreSQL in real-world constraints\n- Awareness of billing/monetization edge cases (overage handling, billing cycles)\n- Communication of complex trade-offs\n\n**",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core needs (quota management, usage tracking, pricing tiers) with no red flags. Clarity: Crystal clear, well-scoped with specific numbered points, minor deduction for no explicit time/format constraints. Discriminative: Excellent at probing trade-offs, distributed systems thinking, and tech choices (Redis/PostgreSQL implied), strongly separates strong candidates.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-489",
        "role_title": "Backend Engineer - Data Quality",
        "question": "# Technical Screening Question\n\n**Scenario-based question:**\n\n\"You're building a data quality check in Great Expectations for a critical fact table in Snowflake that's loaded daily via Airflow. The table should have at least 100k rows, but this morning it only has 50k rows. Walk me through:\n\n1. How would you structure a Great Expectations validator to catch this volume anomaly?\n2. What would you check first in Airflow to diagnose the root cause?\n3. How would you prevent this from silently failing again?\"\n\n---\n\n## Why This Works\n\n\u2705 **Practical & Real:** Mid-level engineers face this exact scenario  \n\u2705 **Multi-faceted:** Tests Great Expectations, Airflow troubleshooting, and systems thinking  \n\u2705 **Observable Depth:** Reveals whether they understand:\n- Data quality framework mechanics (not just syntax)\n- Pipeline debugging methodology\n- Proactive monitoring vs. reactive fixes\n\n\u2705 **Time-bound:** Can be answered substantively in 3-4 minutes  \n\u2705 **Seniority-appropriate:** Doesn't require deep",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core data quality (volume anomalies) and key tools (Snowflake, Airflow) from role; minor ding as Great Expectations unmentioned but fits perfectly (-1). Clarity: Crystal clear scenario with numbered parts, well-scoped for screening. Discriminative: Tests applied knowledge, troubleshooting methodology, and trade-offs (reactive vs proactive), strongly separating mid-level expertise.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-487",
        "role_title": "Senior Backend Engineer - Search Ranking",
        "question": "# Technical Screening Question\n\n**\"Walk me through how you'd approach building a learning-to-rank model for search relevance. Specifically, what features would you engineer, and how would you handle the challenge of getting reliable training labels at scale?\"**\n\n## Why This Question Works:\n\n\u2713 **Tests Core Competency** - Directly assesses learning-to-rank expertise and understanding of the full pipeline\n\n\u2713 **Practical & Realistic** - Mirrors actual senior-level decisions (feature selection, labeling strategies)\n\n\u2713 **Multiple Dimensions** - Reveals:\n- Feature engineering thinking (BM25 scores, click data, semantic similarity, etc.)\n- Label generation approaches (clicks, ratings, implicit signals, crowdsourcing tradeoffs)\n- Awareness of real-world constraints (scale, data quality, cold-start problems)\n\n\u2713 **Senior-Level Depth** - Expects thoughtful tradeoffs and system-level thinking, not just textbook answers\n\n\u2713 **Natural Follow-ups** - Easy to probe deeper:\n- \"How would you validate model quality before A/B testing?\"\n- \"What do you",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core focus on learning-to-rank pipelines, feature engineering, and implicit label challenges (no red flags). Clarity: Crystal clear and well-scoped for screening, minor deduction for lacking explicit time/format constraints. Discriminative: Tests deep applied knowledge, trade-offs (e.g., labeling at scale), and senior-level thinking to separate strong candidates.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-488",
        "role_title": "Backend Engineer - Gaming Backend",
        "question": "# Technical Screening Question\n\n**Design a real-time state synchronization system for a multiplayer game where 100 players are in the same match. A player performs an action (e.g., moves their character), and all other players need to see that update within 100ms.**\n\n**Walk me through:**\n1. How would you structure the message flow from one client's action to all other clients?\n2. Where would you use Redis vs. PostgreSQL in this system?\n3. What's one potential issue with naive broadcasting, and how would you handle it?\n\n---\n\n## Why This Question Works\n\n\u2713 **Practical & Concrete** \u2014 Directly mirrors the role's core responsibility  \n\u2713 **Mid-Level Appropriate** \u2014 Doesn't require deep systems knowledge, but reveals architectural thinking  \n\u2713 **Tech Stack Validation** \u2014 Naturally incorporates Go, WebSocket, Redis, and PostgreSQL  \n\u2713 **Time-Bound** \u2014 Candidate can outline a solid approach in 3-4 minutes  \n\u2713 **Reveals Gaps** \u2014 Shows whether they've thought about latency, consistency, and scaling\n\n---\n\n## What",
        "scores": {
          "relevance": 7,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Tests core state synchronization but assumes unlisted tech (Go/WebSocket; Redis/PostgreSQL mentioned but not confirmed in role; -2 for extras). Clarity: Crystal clear, well-scoped with numbered parts and 100ms constraint. Discriminative: Excellent at probing architecture, trade-offs (e.g., naive broadcasting pitfalls), and DB choices to separate strong thinkers from basics.",
          "composite": 8.333333333333334
        }
      },
      {
        "role_id": "be-490",
        "role_title": "Senior Backend Engineer - Environment Management",
        "question": "# Technical Screening Question\n\n**\"Walk us through how you'd design a Kubernetes namespace isolation strategy for preview environments that need to be automatically cleaned up after 48 hours. What would you consider for resource quotas, network policies, and the TTL mechanism itself?\"**\n\n---\n\n## Why This Question Works\n\n**Practical & Real:** Directly mirrors the job's core responsibility of building an environment management system.\n\n**Multi-layered:** Tests understanding across several domains:\n- Kubernetes architecture (namespaces, resource quotas, network policies)\n- Infrastructure automation (TTL cleanup logic)\n- System design thinking (tradeoffs between isolation, cost, complexity)\n\n**Seniority-appropriate:** Expects a senior engineer to think holistically about constraints and edge cases, not just implement a feature.\n\n**Time-boxed:** Can be answered substantively in 2-5 minutes while allowing for follow-ups on specifics (e.g., \"How would you handle in-flight requests during cleanup?\" or \"Why network policies vs. service mesh?\").\n\n**Signals you're listening for:**\n- Understanding of resource limits and quota enforcement\n- Awareness of cleanup complexity (final",
        "scores": {
          "relevance": 10,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core needs (namespace isolation, quotas, TTL cleanup) with exact Kubernetes tech. Clarity: Clear intent but lacks explicit constraints like timebox or answer format (-2 for 'design a system' style). Discriminative: Excellent at testing trade-offs, applied K8s knowledge, and edge cases to separate seniors from juniors; appropriately scoped for screening.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-491",
        "role_title": "Junior Backend Engineer - Read Replicas",
        "question": "# Screening Question\n\n**Scenario & Question:**\n\nYour application uses PostgreSQL with a read replica. You need to route a user's login request to the primary database, but route their \"view order history\" request to a read replica to reduce load.\n\nWrite pseudocode or a short Python example showing:\n1. How you'd structure the routing logic to direct queries to primary vs. replica\n2. One potential issue that could occur and how you'd handle it\n\n---\n\n## Why This Question Works\n\n\u2713 **Practical**: Directly mirrors the job responsibility (query routing)  \n\u2713 **Junior-appropriate**: Tests understanding of *why* read replicas are useful and basic routing patterns, not deep system design  \n\u2713 **Time-bound**: Can sketch a solution in 2-5 minutes  \n\u2713 **Reveals thinking**: Shows if candidate understands read-write separation, and whether they think about edge cases (e.g., replication lag where users see stale data)  \n\u2713 **Tech-relevant**: Uses Python + PostgreSQL context from the stack\n\n---\n\n## What You're Looking For\n\n**Good signals:**\n- Routes writes to primary",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core focus on read replica routing and replication lag (no red flags, directly tests PostgreSQL context). Clarity: Crystal clear scenario, well-scoped for screening with numbered asks (minor -1 for lacking explicit time/format constraint). Discriminative: Strong separation via applied routing logic + edge case reasoning (e.g., lag handling), reveals junior-level trade-offs beyond recall.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-493",
        "role_title": "Staff Engineer - Reliability Standards",
        "question": "# Screening Question\n\n**Describe a time when you designed or implemented a production readiness standard or checklist for a team. Walk me through:**\n\n1. **What specific reliability gaps or incidents** prompted you to create it?\n2. **What 3-4 key criteria did you include**, and why those over others?\n3. **How did you handle resistance or adoption challenges** from engineering teams?\n4. **What metrics or signals told you it was actually working?**\n\n---\n\n## Why This Question Works\n\n\u2705 **Practical & Scoped** \u2013 Tests real-world experience with tangible artifacts (standards/checklists), not theoretical SRE knowledge\n\n\u2705 **Staff-Level Signal** \u2013 Distinguishes between someone who *follows* standards vs. someone who *designs and drives adoption* of them across teams\n\n\u2705 **Reveals Key Competencies:**\n- Diagnosis of reliability problems\n- Prioritization and decision-making\n- Stakeholder influence and change management\n- Measurement and iteration mindset\n\n\u2705 **Conversational** \u2013 Opens natural follow-ups on specific choices (observability, deployment safety, runbook quality, etc",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Directly tests core role responsibilities (defining reliability standards, production readiness reviews, incident frameworks) with behavioral depth on design, adoption, and measurement. Crystal clear structure with numbered prompts; highly discriminative for staff-level SRE experience distinguishing implementers from leaders. Minor subtract for bundled sub-questions but well-scoped overall.",
          "composite": 9.666666666666666
        }
      },
      {
        "role_id": "be-492",
        "role_title": "Backend Engineer - CD Pipelines",
        "question": "# Screening Question\n\n**Describe how you would structure a GitOps-based CD pipeline for promoting an application across dev \u2192 staging \u2192 production environments using ArgoCD. Walk us through your approach to handling approval gates and rollbacks.**\n\n---\n\n## Evaluation Rubric\n\n**Strong Answer (Hire):**\n- Explains separate ArgoCD Application manifests per environment\n- Mentions Git as source of truth and how promotion works (e.g., updating image tags in Git)\n- Describes approval mechanisms (manual sync, pull request reviews, or notification integrations)\n- Addresses rollback strategy (Git history, previous commits, or ArgoCD sync to prior state)\n- Shows awareness of practical concerns (drift detection, secrets management)\n\n**Acceptable Answer (Proceed):**\n- Outlines basic environment separation in ArgoCD\n- Explains Git-driven promotion workflow\n- Mentions one approval/rollback mechanism\n- May lack depth on operational edge cases\n\n**Weak Answer (Pass):**\n- Vague about ArgoCD mechanics or how Git triggers deployments\n- No clear approval or rollback strategy\n- Confuses GitOps with traditional CI/",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core GitOps CD pipelines, multi-environment promotion, approvals, and rollbacks with ArgoCD (exact match to role; no red flags). Clarity: Crystal clear and well-scoped for screening, minor deduction for no explicit time/format constraints. Discriminative: Excellent separation via trade-offs (approvals, rollbacks, drift), supported by rubric; not trivial recall or overly broad.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-495",
        "role_title": "Senior Backend Engineer - Audit Logging",
        "question": "# Screening Question\n\n**Design an audit log entry that must be tamper-proof and queryable. Walk me through:**\n\n1. **What data would you include** in each audit log entry beyond the obvious (who, what, when)?\n2. **How would you detect tampering** if someone modified a log entry in the database?\n3. **What's the tradeoff** between immutability guarantees and query performance on large datasets?\n\n**Follow-up:** If you had to choose between storing logs in PostgreSQL vs. ClickHouse for this use case, what factors would drive your decision?\n\n---\n\n## Why This Question Works\n\n\u2705 **Practical & Specific** \u2014 Tests real decisions they'd make on day one (hashing chains, checksums, columnar vs. row storage)\n\n\u2705 **Senior-Level** \u2014 Expects understanding of tradeoffs, not just textbook answers (compliance needs vs. query speed)\n\n\u2705 **Security-Focused** \u2014 Directly assesses tamper-proof thinking without being a riddle\n\n\u2705 **Time-Appropriate** \u2014 Can answer core concept in 2-3 min, elaborate in",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core audit logging skills (tamper-proofing, querying, retention implications) for the role (-0 red flags). Clarity: Well-structured with numbered parts and scoped follow-up, unambiguous for screening. Discriminative: Probes trade-offs, applied security knowledge, and DB choice factors, strongly separating senior candidates who understand hashing/immutability vs. columnar storage.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-494",
        "role_title": "Backend Engineer - API Mocking",
        "question": "# Screening Question\n\n**Scenario:**\n\nYou're building an API mock for a payment service that needs to:\n1. Match requests by specific headers (e.g., `X-API-Key`) and request body fields\n2. Return different responses based on the matched request\n3. Simulate realistic delays and occasional failures\n\n**Question:**\n\nWalk me through how you would configure a WireMock stub to handle a POST request to `/api/payments` that:\n- Only matches when `X-API-Key` header is present AND the request body contains `\"amount\": 100`\n- Returns a success response (HTTP 200) with a transaction ID\n- Returns a failure response (HTTP 400) for requests with `\"amount\": 0`\n\nWhat challenges might you encounter if you later needed to verify that a client called this endpoint with the correct payload during testing?\n\n---\n\n## What This Assesses\n\n\u2705 **Practical WireMock knowledge** \u2013 Request matching strategies and stub configuration  \n\u2705 **API mocking reasoning** \u2013 Understanding when/why to use specific matching rules  \n\u2705 **Contract testing awareness** \u2013 Recognizing the need for verification",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 8,
          "reasoning": "Relevance: Directly tests WireMock for API mocking, core to role's request matching/response templating; minor deduct for not explicitly covering contract verification (-1). Clarity: Precise scenario, unambiguous matching/response specs, well-scoped. Discriminative: Requires applied WireMock config knowledge and foresight on verification challenges (e.g., statefulness, assertions); good separation but not elite trade-off depth.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-496",
        "role_title": "Junior Backend Engineer - Model Serving",
        "question": "# Screening Question\n\n**You need to create a FastAPI endpoint that accepts an image, runs it through a pre-trained ML model, and returns predictions. The model takes 2 seconds to load from disk, but your endpoint might receive multiple requests per second.**\n\n**Walk me through:**\n1. How would you avoid reloading the model on every request?\n2. What would happen if two requests arrive simultaneously and hit your endpoint?\n3. How would you handle a request that times out while waiting for model inference?\n\n---\n\n## Why This Works\n\n\u2705 **Practical**: Tests real deployment scenarios (model lifecycle, concurrency, timeouts)  \n\u2705 **Appropriate Level**: Junior engineer should know basic patterns; doesn't require advanced optimization  \n\u2705 **Reveals Thinking**: Shows understanding of FastAPI startup events, async/concurrency, and basic operational concerns  \n\u2705 **Conversational**: Naturally leads to follow-ups (model size, batching, monitoring) without being yes/no\n\n## What You're Looking For\n\n- **Strong answer**: Loading model in startup event, async handlers, request timeouts via `timeout` parameter\n- **Good answer",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core ML model deployment skills (loading, concurrency) for the entry-level API role, matches FastAPI implicitly via context (-0 red flags). Clarity: Well-structured with 3 focused sub-questions, unambiguous scope for screening (-0 red flags). Discriminative: Probes applied knowledge of startup events, async handling, and timeouts with trade-offs, strongly separates candidates revealing practical thinking.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-497",
        "role_title": "Backend Engineer - Payout Systems",
        "question": "# Technical Screening Question\n\n**You're building a payout system that needs to process 100,000 seller payouts daily. Each payout requires:**\n- Validating the seller's bank account against an external bank API\n- Calculating final amounts (accounting for fees and adjustments)\n- Storing the transaction record\n- Publishing an event for downstream systems\n\n**Walk me through your approach:**\n1. How would you structure this to avoid bottlenecks?\n2. What would you consider the critical path vs. non-critical operations?\n3. How would you handle failures in the bank validation API?\n\n---\n\n## Why This Works\n\n\u2705 **Practical**: Mirrors real payout system design decisions  \n\u2705 **Mid-level appropriate**: Requires understanding of batch processing, system design, and fault tolerance\u2014not deep expertise  \n\u2705 **Tech-relevant**: Opens discussion on async processing (Kafka), caching (Redis), and database design (PostgreSQL)  \n\u2705 **Time-bound**: Can be answered in 3-4 minutes with follow-ups  \n\u2705 **Reveals thinking**: Shows whether they understand concurrency, resilience",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core payout system skills (batch processing, validation, reconciliation) matching role; minor ding for assuming unlisted tech like Kafka/Redis (-1). Clarity: Well-structured with clear scenario, numbered parts, scoped for screening. Discriminative: Probes trade-offs in bottlenecks, critical path, failures\u2014reveals deep system design thinking and resilience understanding.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-499",
        "role_title": "Principal Engineer - Search Platform",
        "question": "# Screening Question\n\n**\"Tell me about a time you designed or significantly improved a search infrastructure that handled scale challenges. What were the bottlenecks you identified, how did you prioritize them, and what was the outcome? If you were advising a team on whether to invest in a new search platform versus optimizing their existing one, what factors would influence that decision?\"**\n\n---\n\n## Why This Works\n\n**Tests practical knowledge:**\n- Real experience with search systems at scale\n- Ability to diagnose performance issues (indexing, query latency, throughput)\n- Understanding of tradeoffs in search architecture\n\n**Appropriate for Principal level:**\n- Requires strategic thinking beyond just technical implementation\n- Assesses mentorship/advisory capability (\"advising a team\")\n- Evaluates decision-making frameworks, not just tools\n\n**Reveals key competencies:**\n- Search platform expertise (core requirement)\n- Systems thinking (bottleneck identification and prioritization)\n- Leadership maturity (guiding technology decisions)\n- Communication (explaining complex tradeoffs clearly)\n\n**Realistic time frame:** 3-4 minutes for a strong answer\n\n**Red flags to listen for:**",
        "scores": {
          "relevance": 9,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core search platform strategy, scale challenges, and advisory decision-making from role description (-1 for no specific tech stack mention). Clarity: Clear behavioral + hypothetical structure, well-scoped for screening (-2 for bundling two related but distinct questions). Discriminative: Strong separation via real experience, bottleneck analysis, prioritization, tradeoffs, and strategic factors; reveals depth vs superficial answers.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-498",
        "role_title": "Senior Backend Engineer - Profiling Infrastructure",
        "question": "# Technical Screening Question\n\n**You're implementing continuous profiling for a Go microservice that processes 10,000 requests/second. The profiling data is being stored in ClickHouse, but engineers are reporting that flame graph queries take 30+ seconds to load when comparing profiles across a 1-hour window.**\n\n**Walk me through how you'd approach diagnosing this performance issue. What are the key factors you'd investigate, and what optimizations might you consider\u2014both at the profiling collection level and the storage/query level?**\n\n---\n\n## Why This Question Works\n\n\u2713 **Practical & Real:** Reflects an actual senior-level challenge in profiling infrastructure  \n\u2713 **Depth:** Tests understanding across the full stack (Pyroscope \u2192 ClickHouse \u2192 Grafana frontend)  \n\u2713 **Seniority-Appropriate:** Expects architectural thinking, not just syntax knowledge  \n\u2713 **Time-Bounded:** Can explore 2-5 minutes; natural stopping points at each layer  \n\u2713 **Open-Ended:** Reveals how they prioritize (sampling rates vs. schema vs. indexing strategy)",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (profiling, flame graphs, storage optimization in ClickHouse) for high-throughput Go services; minor ding for assuming Pyroscope/Grafana not explicitly listed (-1). Clarity: Crystal clear scenario with specific constraints (10k req/s, 1hr window, 30s queries), well-scoped. Discriminative: Excellent at separating seniors via trade-offs (sampling, schema, indexing, collection optimizations); reveals prioritization and depth without being too broad.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-500",
        "role_title": "Staff Engineer - Streaming Architecture",
        "question": "# Screening Question\n\n**You're designing a real-time notification system that needs to fan out a single user event to potentially thousands of subscribers across multiple geographic regions. The system uses Kafka as the event source and must support both WebSocket connections and traditional HTTP polling clients.**\n\n**Walk me through how you'd approach the architecture\u2014specifically:**\n1. **How would you structure the Kafka topic(s) and consumer groups?**\n2. **Where would you use Redis, and for what specific purposes?**\n3. **What delivery guarantee would you target, and what trade-offs does that involve?**\n\n**Feel free to make reasonable assumptions about scale, latency requirements, or failure modes.**\n\n---\n\n## Why This Works\n\n- **Practical depth**: Tests whether they've actually designed multi-consumer streaming systems, not just used them\n- **Appropriate scope**: A Staff engineer should reason through infrastructure tradeoffs (exactly-once vs. at-least-once, centralized vs. distributed fan-out, caching strategy)\n- **Tech stack coverage**: Naturally incorporates Kafka, Redis, and real-time concerns without feeling forced\n- **Reveals thinking**: Their assumptions and tradeoffs",
        "scores": {
          "relevance": 7,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core fan-out and streaming skills from role, but penalizes -2 for mentioning Redis (not in role description) and -1 for generic design feel. Clarity: Crystal clear, well-scoped with specific numbered points and assumption guidance. Discriminative: Excellent at probing trade-offs, applied architecture knowledge, and reasoning depth to separate strong Staff engineers.",
          "composite": 8.333333333333334
        }
      }
    ]
  },
  "baseline": {
    "model": "baseline",
    "num_samples": 100,
    "avg_relevance": 8.5,
    "avg_clarity": 8.04,
    "avg_discriminative": 8.48,
    "avg_composite": 8.340000000000003,
    "results": [
      {
        "role_id": "be-401",
        "role_title": "Backend Engineer - Training Infrastructure",
        "question": "\"Design a scalable and fault-tolerant system to manage distributed machine learning training jobs using Ray and Kubernetes. Your solution should handle job scheduling, GPU resource allocation, and recovery from failures. Provide a high-level architecture, including how Ray clusters are managed, how jobs are scheduled, and how failures are detected and recovered. Explain how you would ensure scalability and fault tolerance across a growing number of training jobs and GPUs.\"",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core needs (distributed ML infra, scheduling, GPU allocation, orchestration) using relevant tools (Ray, K8s); no red flags. Clarity: Crystal clear, well-scoped high-level design with specific components; minor deduction for lacking explicit time/format constraints. Discriminative: Tests deep applied knowledge, trade-offs in scalability/fault-tolerance, architecture decisions; strong separation of strong vs weak candidates; not too broad for screening.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-402",
        "role_title": "Backend Engineer - Model Deployment",
        "question": "\"When deploying a machine learning model using KServe, how would you design a deployment strategy that includes automated rollback in case of a failed inference request, and what role does Redis play in this process?\"",
        "scores": {
          "relevance": 4,
          "clarity": 7,
          "discriminative": 7,
          "reasoning": "Relevance: Low due to KServe and Redis not mentioned in role (red flag -2, generic -3); tests deployment/rollback but wrong specific tech. Clarity: Clear intent and well-formed, but lacks explicit constraints (-1 for minor jargon). Discriminative: Good test of applied design with trade-offs, differentiates ML deployment expertise.",
          "composite": 6.0
        }
      },
      {
        "role_id": "be-403",
        "role_title": "Backend Engineer - Inference Optimization",
        "question": "\"Design a Python script that uses TensorRT to optimize a model for inference latency. The script should:\n- Load a PyTorch model (e.g., ResNet50) and convert it to a TensorRT engine.\n- Apply dynamic batching to reduce GPU memory usage.\n- Include a simple benchmark to measure inference latency before and after optimization.\n- Output a report showing the latency improvement and GPU memory per batch.\"",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core inference optimization skills (quantization implied via TensorRT, batching, GPU memory) for latency-sensitive models; minor ding as role doesn't explicitly list TensorRT but fits perfectly (-1 red flag avoided). Clarity: Crystal clear with specific requirements, well-scoped steps, no ambiguity. Discriminative: Requires applied knowledge of model conversion, optimization techniques, benchmarking, and trade-offs in batching/memory\u2014strong separation of strong vs weak candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-404",
        "role_title": "Senior Backend Engineer - Feature Platform",
        "question": "\"Design a feature pipeline for a real-time feature serving system using Feast and Redis. The pipeline should support both online and offline feature computation. Explain how you would structure the feature store, how you would handle feature versioning, and how you would ensure consistency between online and offline feature values. Include considerations for feature discovery and how you would monitor system health.\"",
        "scores": {
          "relevance": 9,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core feature platform skills (computation, serving, discovery) with Feast (feature store standard); minor ding for specifying Redis (not in role desc, -2) but still highly relevant. Clarity: Well-formed and unambiguous, but 'design a pipeline' lacks explicit time/format constraints (-2). Discriminative: Excellent at probing trade-offs (versioning, consistency), architecture, monitoring; distinguishes deep Feast expertise from superficial knowledge.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-405",
        "role_title": "Senior Backend Engineer - Model Monitoring",
        "question": "\"Design a drift detection system for a production ML model using Python and Prometheus/Grafana. The system should detect data drift, model drift, and concept drift. Explain how you would collect, store, and visualize the metrics. Include a high-level architecture and at least one concrete example of how you'd implement a drift detection algorithm in Python (e.g., Kullback-Leibler divergence, Kolmogorov-Smirnov test, or a statistical test).\"",
        "scores": {
          "relevance": 9,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core role skills (drift detection, monitoring, alerting via Prometheus/Grafana); minor ding for not explicitly covering performance tracking/alerting (-1). Clarity: Well-scoped with specific drifts, components, and example required; lacks minor constraints like time complexity (-1, -1 for design scope). Discriminative: Tests architecture, metrics pipeline, visualization, and concrete algo implementation with trade-offs, strongly separates skilled ML engineers.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-406",
        "role_title": "Senior Backend Engineer - LLM Serving",
        "question": "\"Design and implement a system to optimize token-level batching in vLLM for a high-throughput LLM serving environment. The system should handle dynamic request arrivals, manage KV cache efficiently, and support streaming responses. Provide a high-level design including key components, and describe how you would evaluate its performance and scalability.\"",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core role responsibilities (batching optimization, KV cache, streaming) and specifies vLLM from LLM serving context. Clarity: Well-scoped high-level design with clear requirements, minor deduction for lacking explicit time/format constraints. Discriminative: Tests applied system design, trade-offs in dynamic batching/KV management, and evaluation\u2014strong separation of strong vs weak candidates.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-407",
        "role_title": "Staff Backend Engineer - ML Infrastructure",
        "question": "\"Design a scalable and fault-tolerant ML training pipeline using Ray, Kubernetes, and Spark. Your design should include:  \n- A high-level architecture diagram (text-based or ASCII)  \n- How you would handle data preprocessing and feature engineering  \n- How you would manage training jobs and distributed execution  \n- How you would ensure fault tolerance and recovery  \n- How you would monitor and optimize training jobs for cost and performance\"",
        "scores": {
          "relevance": 7,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Tests core ML infra skills (pipelines, scaling, fault tolerance, optimization) but assumes unmentioned tech (Ray, Spark) and ignores serving/cost strategy focus (-2 for tech not in role). Clarity: Well-structured with specific components, minor vagueness on diagram format and scope. Discriminative: Excellent at probing trade-offs, architecture, and applied ML ops knowledge, strongly separates candidates.",
          "composite": 8.0
        }
      },
      {
        "role_id": "be-408",
        "role_title": "Junior Backend Developer - Health Endpoints",
        "question": "\"Design and implement a health check endpoint in Go that returns a JSON response indicating the status of liveness, readiness, and metrics. The endpoint should:\n- Return a 200 OK status with a JSON body when healthy\n- Return a 503 Service Unavailable status when unhealthy\n- Include a `status` field (value: 'healthy' or 'unhealthy')\n- Include a `liveness` field (value: 'healthy' or 'unhealthy') \n- Include a `readiness` field (value: 'healthy' or 'unhealthy')\n- Include a `metrics` field (value: 'available' or 'unavailable') \n- Be implemented in a minimal Go HTTP server (no external dependencies)\n- Include error handling for unexpected issues\n- Return 503 when the system is unhealthy (e.g., due to a failed internal check)",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 8,
          "reasoning": "Relevance: Directly tests core role skills (liveness/readiness/metrics endpoints) in Go (implied by role context), minor deduct for not explicitly stating Go in question (-1). Clarity: Crystal clear specs, well-scoped for screening, no bundling or ambiguity. Discriminative: Tests applied HTTP/JSON/Go skills with error handling and state logic, distinguishes solid entry-level devs from weak ones; slightly implementation-heavy but feasible.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-409",
        "role_title": "Junior Backend Engineer - File Upload",
        "question": "Design and implement a FastAPI endpoint that allows users to upload a file. The endpoint should:\n1. Accept a multipart form with `file` and `metadata` fields.\n2. Validate the file (ensure it's not empty and has a valid MIME type).\n3. Store the file in AWS S3 using a unique key (e.g., `user/{user_id}/file_{timestamp}.ext`).\n4. Store metadata in a PostgreSQL database.\n5. Return a success response with the file's URL and stored metadata.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core file upload API skills, multipart handling, validation, S3 storage, and DB integration from role (-1 for minor assumption of user_id source, not generic). Clarity: Precise requirements with clear fields, validation, storage, and response specs. Discriminative: Requires integrating FastAPI, AWS SDK, SQLAlchemy/psycopg2, validation logic, error handling, and trade-offs like key generation/security\u2014strong separation of capable entry-level engineers.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-410",
        "role_title": "Backend Engineer - Bulk Operations",
        "question": "\"Design a Go backend API endpoint to handle bulk data imports for a system that manages user profiles. The endpoint should support:\n- Accepting a batch of user profile updates via a JSON array\n- Ensuring idempotency (no duplicate processing of the same user updates)\n- Handling partial success (some updates may succeed, others may fail)\n- Processing the data in a background job via Kafka\n- Using Redis to track processed user IDs to ensure idempotency\n- Returning a detailed response with success/failure counts and error details for each update",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (bulk ops, partial success, idempotency) with specified tech (Go, Kafka, Redis); minor ding for assuming user profiles. Clarity: Crystal clear with precise requirements and scope. Discriminative: Requires applied knowledge of async processing, idempotency patterns, error handling, and tech integration, forcing trade-offs and strong separation.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-411",
        "role_title": "Backend Engineer - Admin APIs",
        "question": "\"Design and implement a secure admin API endpoint in Python using FastAPI that allows an admin user to create a new user account. The endpoint must:\n- Require a valid admin token (JWT) for access\n- Validate the admin token using a secret key\n- Accept a user\u2019s email and password (hashed using bcrypt)\n- Store the user in PostgreSQL with a unique constraint on email\n- Prevent duplicate email creation\n- Log all admin actions (user creation) to a Redis cache with a structured log entry (timestamp, user email, admin token ID, action type), and persist the log to a PostgreSQL audit table\n- Include error handling for invalid tokens, duplicate emails, and other common issues",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core admin API skills (auth, auditing, workflows) and exact tech stack (FastAPI, JWT, bcrypt, PostgreSQL, Redis) from role; minor ding for not explicitly mentioning privileged access beyond JWT (-1). Clarity: Crystal clear with precise requirements and error handling specified; well-scoped for screening. Discriminative: Excellent separation via applied knowledge of security, DB constraints, logging pipeline, and error handling; tests trade-offs in auth/logging without being too broad.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-412",
        "role_title": "Backend Engineer - Analytics APIs",
        "question": "Design and implement a Go-based analytics API that serves time-series data from ClickHouse with caching using Redis. Your solution should support efficient aggregation queries (e.g., daily, hourly) and handle data freshness and cache invalidation. Describe your design decisions, including how you handle cache invalidation, query performance, and data consistency.",
        "scores": {
          "relevance": 7,
          "clarity": 7,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core time-series, aggregation, and caching skills but assumes unlisted tech (Go, ClickHouse, Redis) (-2). Clarity: Clear intent but lacks scoping constraints like time/format (-1). Discriminative: Excellent at probing trade-offs in caching, invalidation, performance, and consistency, strongly separating candidates.",
          "composite": 7.666666666666667
        }
      },
      {
        "role_id": "be-413",
        "role_title": "Senior Backend Engineer - API Gateway Logic",
        "question": "Design and implement a Lua script in Envoy that validates incoming HTTP requests and transforms the request headers based on a custom business rule. The rule is:  \n- If the `X-Client-Id` header is present and starts with `PROD-`, then:\n  - Add a `X-Environment: production` header.\n  - Strip the `X-Client-Id` header and set it to `PROD-<client-id>` (e.g., `PROD-123`).\n- If the `X-Client-Id` header is present and starts with `TEST-`, then:\n  - Add a `X-Environment: staging` header.\n  - Strip the `X-Client-Id` header and set it to `TEST-<client-id>` (e.g., `TEST-123`).\n- If the `X-Client-Id` header is missing, do nothing.\n- All other headers must be preserved.",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core Envoy/Lua skills for API gateway extensions, request transformation, and custom routing rules. No red flags. Clarity: Crystal clear with precise business rules and constraints; minor ambiguity on 'strip and set' intent but well-scoped. Discriminative: Requires applied Lua/Envoy knowledge, header manipulation, conditionals, and edge cases for strong separation; not pure recall.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-414",
        "role_title": "Senior Backend Engineer - Long-Polling",
        "question": "Design and implement a long-polling endpoint in Go that efficiently handles concurrent clients, manages connection timeouts, and gracefully degrades when Redis is unavailable. The endpoint should support:\n1. A simple `POST /poll` that accepts a `pollId` and returns a message or `204 No Content` if no message is available.\n2. Connection timeout handling (e.g., 30 seconds) with automatic reconnection on timeout.\n3. Timeout cleanup after 30 seconds of inactivity.\n4. Fallback to polling every 10 seconds if Redis is unreachable (use a simple retry mechanism).\n5. Connection management using Redis to track active clients and avoid duplicate polling.\n6. Use Redis as a message store (store messages in Redis with TTL of 300 seconds).\n7. Use PostgreSQL to store client session metadata (e.g., `poll_id`, `client_id`, `last_active`, `status`).\n8. Implement a simple health check at `/health` that returns 200 if both Redis and PostgreSQL are reachable.",
        "scores": {
          "relevance": 4,
          "clarity": 3,
          "discriminative": 2,
          "reasoning": "Relevance: Superficially mentions Go and long-polling but heavily diverges into Redis/PostgreSQL management, health checks, and full system design beyond core role focus (-3 generic design, -2 extras not in role). Clarity: Multiple bundled requirements (8 points) make it ambiguous and unscoped for screening (-2); no constraints on implementation depth/format (-2). Discriminative: Far too broad (>10min, full system impl) for screening, tests everything at once with weak separation (-2).",
          "composite": 3.0
        }
      },
      {
        "role_id": "be-415",
        "role_title": "Senior Backend Engineer - Multipart APIs",
        "question": "Design and implement a resumable upload system using FastAPI, Python, and S3. The system should support chunked uploads, store upload metadata in S3, and allow resuming from any point. Include a simple example of how a client would initiate and resume an upload.",
        "scores": {
          "relevance": 10,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core focus on resumable uploads, chunk management, and S3 (core tech stack). Clarity: Clear intent and scope, but lacks explicit constraints (e.g., chunk size limits, time limits for screening). Discriminative: Tests deep applied knowledge of APIs, state management, and trade-offs in resumable systems; strong separation between good and weak candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-416",
        "role_title": "Staff Backend Engineer - External APIs",
        "question": "\"Design a scalable and secure external API using Go and OpenAPI, with Kong as the gateway, deployed on Kubernetes. Your design should include key components, security considerations, rate limiting, logging, and observability. Provide a high-level architectural overview, and explain how you would implement API monetization for partners. Include how you'd structure the API to support partner enablement and integration patterns.\"",
        "scores": {
          "relevance": 9,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core role skills (API strategy, monetization, partner enablement, integration patterns) with relevant tech (Go, OpenAPI, Kong, K8s); minor ding for assuming specific tools not in desc (-1). Clarity: Well-scoped high-level design with clear components listed, but lacks time/format constraints (-1) and bundles multiple aspects. Discriminative: Excellent at testing applied architecture, trade-offs (security, scaling, monetization), and role-specific strategy, strongly separating strong candidates.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-417",
        "role_title": "Junior Backend Developer - Event Sourcing",
        "question": "\"Design a simple event handler for a `UserCreated` event in an event-sourced system using Kafka and PostgreSQL. The event should be consumed by a handler that persists the user's state in a PostgreSQL database. Implement the handler class, describe how it would handle state transitions (e.g., from `UNKNOWN` to `ACTIVE`), and explain how you\u2019d ensure data consistency between Kafka and PostgreSQL.\"",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core event handling, state management, and persistence for event sourcing role (-1 for specifying Kafka/PostgreSQL not in role desc). Clarity: Well-scoped design with specific event, tech, and consistency focus. Discriminative: Requires applied knowledge of handlers, state transitions, and exactly-once semantics, distinguishing strong candidates via trade-offs.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-418",
        "role_title": "Junior Backend Engineer - Service Clients",
        "question": "> \"Design and implement a gRPC client in Go that supports automatic retry with backoff and circuit breaker pattern. The client should handle transient errors (e.g., connection timeout, network errors) and gracefully degrade when the service is unavailable. Use Protobuf for the service interface and Docker to containerize the client. Provide a minimal working example with a simple service interface and a client that demonstrates retry and circuit breaker behavior.\"",
        "scores": {
          "relevance": 3,
          "clarity": 6,
          "discriminative": 7,
          "reasoning": "Relevance: Superficially mentions tech stack (Go, gRPC) but demands full implementation of core skills (retry, circuit breaker) PLUS unrelated extras (Protobuf service, Docker containerization) not in role; generic design question (-3), tests minor/unlisted tech (-2). Clarity: Understandable but vague on retry/backoff params, circuit breaker thresholds, expected depth; multiple components bundled (-2), missing constraints. Discriminative: Good test of applied patterns with trade-offs, but overly broad for screening (>10min effort).",
          "composite": 5.333333333333333
        }
      },
      {
        "role_id": "be-419",
        "role_title": "Backend Engineer - Retry Logic",
        "question": "Imagine you're implementing a retry mechanism for a distributed service that handles order processing. The service may fail due to transient issues like network latency or database timeouts. You need to implement a retry strategy that uses exponential backoff with jitter, and includes a retry budget to prevent infinite retries on persistent failures.",
        "scores": {
          "relevance": 10,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core focus on retry infrastructure, exponential backoff, jitter, and retry budgets. Clarity: Clear scenario and requirements, but lacks explicit constraints (e.g., max retries, time limits, code format). Discriminative: Strong test of applied knowledge and trade-offs in retry design, distinguishes candidates who understand distributed systems retries from those with superficial knowledge.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-420",
        "role_title": "Backend Engineer - Graceful Shutdown",
        "question": "*In a Kubernetes environment, how would you implement a graceful shutdown for a Go gRPC server that handles in-flight requests and supports connection draining, especially when receiving a SIGTERM signal?*",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core role requirements (graceful shutdown, connection draining, in-flight requests, SIGTERM) with exact tech stack (Kubernetes, Go, gRPC). No red flags. Clarity: Precise and well-scoped for screening; minor deduction as it lacks explicit time/format constraints. Discriminative: Requires applied knowledge of signal handling, gRPC lifecycle, and K8s deployment patterns, testing trade-offs effectively without being trivial or overly broad.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-421",
        "role_title": "Senior Backend Engineer - Security Automation",
        "question": "*You're tasked with creating a Python script that automates the rotation of database secrets stored in HashiCorp Vault. The script should:*\n1. *Authenticate to Vault using a Kubernetes service account (Kubernetes `ServiceAccount` with `vault` role bound via `RoleBinding`)*\n2. *Read a list of database secrets from a configuration file (e.g., JSON or YAML)*\n3. *For each secret, generate a new random password (12 characters, lowercase, alphanumeric) and rotate it in Vault*\n4. *Log the operation with timestamps and success/failure status*\n5. *Fail fast if any step fails, and output a human-readable error message*",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core security automation (secrets rotation in Vault) and Python scripting for cloud infra; minor ding as Vault auth is specific but fits role (-1). Clarity: Precise requirements with clear steps, auth method, constraints, and fail-fast policy. Discriminative: Requires applied knowledge of Vault API, K8s auth, error handling, and secure practices; distinguishes via trade-offs in auth/secrets handling.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-422",
        "role_title": "Junior Backend Engineer - API Testing",
        "question": "Write a pytest test for a FastAPI endpoint that returns a list of users. The endpoint should accept a query parameter `page` and `limit`, and return a JSON response with a `users` array and `total` count. Your test should:\n- Use `test_client` to make a request\n- Test the response structure and data\n- Include a fixture for the test database\n- Verify that the query parameters are correctly applied (e.g., page=1, limit=10)\n- Handle edge cases like invalid page or limit values (e.g., negative numbers, non-numeric)",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests pytest, FastAPI testing with test_client, fixtures, and query param handling\u2014core to API test writing role (-1 for not explicitly mentioning contract testing). Clarity: Precise requirements, well-scoped with specific actions and edge cases. Discriminative: Requires applied knowledge of fixtures, client usage, assertions, and edge case reasoning to separate capable entry-level testers.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-423",
        "role_title": "Staff Engineer - Incident Response Platform",
        "question": "> \"Design a system to manage on-call rotations for a team of 10 engineers, where each engineer is on-call for 5 days, then off for 3 days. The system must handle automatic rotation, provide real-time visibility into on-call schedules, and support manual overrides. It should be implemented in Go, with a PostgreSQL database to store schedules and a Slack API to notify engineers when their shift begins and ends. Explain your approach, including data model, how you'd handle edge cases (e.g., engineers on leave, shift overlaps), and how you'd implement the rotation logic.\"",
        "scores": {
          "relevance": 7,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Tests core on-call optimization skill but too narrowly focused on rotations (minor aspect) vs broader incident response/SLO tooling; specifies unmentioned Go/PostgreSQL/Slack (-2). Clarity: Well-scoped with specific constraints/tech, minor ambiguity on full implementation depth. Discriminative: Excellent at probing design, data modeling, edge cases, and logic trade-offs, strongly separates candidates.",
          "composite": 8.0
        }
      },
      {
        "role_id": "be-424",
        "role_title": "Backend Engineer - Message Serialization",
        "question": "\"Design a message serialization system for a distributed system that uses Kafka as the messaging backbone. The system must support schema evolution with backward and forward compatibility, using Avro with Schema Registry. Handle schema validation at both producer and consumer sides. Provide a concrete example of how a schema change would be handled when a new field is added to a message, and explain how the system ensures backward compatibility. Also, describe how you would validate schemas at runtime and what monitoring or alerting strategies you would implement to detect schema mismatches.\"",
        "scores": {
          "relevance": 9,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core schema evolution, compatibility, and validation skills; Kafka/Avro/Schema Registry are spot-on for message serialization infra (-1 for assuming Kafka not explicitly listed). Clarity: Well-formed and unambiguous, but multiple components bundled (-2). Discriminative: Excellent on trade-offs, concrete examples, runtime validation, and monitoring; strong separation of deep vs shallow knowledge.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-425",
        "role_title": "Senior Backend Engineer - Graph Analytics",
        "question": "Design a scalable graph analytics pipeline to compute user-based recommendations using Neo4j and Apache Spark. The pipeline should handle a billion-edge dataset and support real-time updates. Describe how you would structure the pipeline, the data flow, and how you would optimize traversal queries for performance and scalability. Include considerations for handling real-time updates and potential bottlenecks.",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's graph analytics, social/rec pipelines, billion-edge scale, traversal optimization, and implied tech (Neo4j/Spark common for graphs). No red flags. Clarity: Well-scoped with specific tech/data size/updates; minor deduction as no explicit time/format constraints. Discriminative: Probes deep applied knowledge of pipeline design, trade-offs (real-time vs batch, bottlenecks), optimization\u2014strong separation of strong vs weak candidates.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-426",
        "role_title": "Junior Backend Engineer - DevOps Scripting",
        "question": "Write a Bash script that builds and pushes a Docker image to a private Docker registry (e.g., Docker Hub or AWS ECR). The script should:\n- Accept a Docker image name (e.g., \"myapp:latest\") as a command-line argument\n- Validate that the image name is provided and has a valid format (e.g., contains a tag)\n- Use `docker build` to build the image from a Dockerfile in the current directory\n- Push the built image to a private Docker registry using `docker push`\n- Include error handling for common issues (e.g., missing Dockerfile, failed build, failed push)\n- Output a success or failure message at the end",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core DevOps automation skills (Bash scripting, Docker builds/push, private registries) essential for deployment scripts/CI pipelines; minor ding as role doesn't explicitly list private registries (-1). Clarity: Crystal clear with specific requirements, validation, and error handling; well-scoped for screening. Discriminative: Requires applied knowledge of Bash args/parsing, Docker CLI, validation, and error handling; distinguishes those who can write robust scripts from superficial knowledge.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-427",
        "role_title": "Backend Engineer - Feature Store",
        "question": "\"Design a feature store pipeline that ingests streaming data from Kafka, computes features using a Python function, stores them in Redis for real-time serving, and persists them to PostgreSQL for historical tracking. The pipeline should support feature versioning and handle data drift detection. Explain how you would implement feature versioning and data drift detection in this system.\"",
        "scores": {
          "relevance": 9,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core role skills (feature store, computation, versioning, real-time serving) with relevant tech; minor ding for assuming Kafka/Redis/PostgreSQL not explicitly listed (-1). Clarity: Well-scoped design question with specific components, but lacks explicit constraints like scale or time (-2). Discriminative: Excellent at testing applied ML infra knowledge, trade-offs in versioning/drift, distinguishes strong candidates.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-428",
        "role_title": "Senior Backend Engineer - Payment Orchestration",
        "question": "*Design a failover strategy for a payment orchestration system that routes transactions to multiple payment providers (e.g., Stripe, PayPal, Braintree). The system must ensure transaction success, minimize cost, and maintain low latency. Consider provider availability, transaction cost, and success rates. Provide a step-by-step plan including how you would evaluate provider health, dynamically route transactions, and handle failures. Include considerations for cost optimization and transaction latency.*",
        "scores": {
          "relevance": 9,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core role skills (failover, routing, cost optimization, success rates) with relevant providers; minor generic feel but no red flags (-0). Clarity: Well-scoped with clear requirements and step-by-step ask, but 'design' lacks explicit time/format constraints (-1, -1 red flag). Discriminative: Excellent at probing trade-offs (availability/cost/latency), dynamic routing, health checks\u2014separates deep thinkers from superficial answers.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-429",
        "role_title": "Backend Engineer - Log Aggregation",
        "question": "**\"Design a scalable log aggregation pipeline using Go and Kafka to ingest, parse, and store logs in ClickHouse. Include key considerations for performance, reliability, and observability.\"**",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly matches role's core focus on high-volume log pipelines (millions/sec), parsing/enrichment/retention; uses relevant tech (Go, Kafka, ClickHouse) with no red flags. Clarity: Well-scoped design question with specific components and considerations; minor deduction for lacking explicit time/format constraints. Discriminative: Requires deep trade-offs in scalability/performance/reliability for high-throughput systems, strongly separates strong candidates.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-430",
        "role_title": "Junior Backend Engineer - Search Indexing",
        "question": "You're building a search index for a product catalog with 100,000 products. Each product has a title, description, and price. You need to index these records into Elasticsearch using Python. How would you design the indexing pipeline to ensure efficient, reliable, and scalable performance?",
        "scores": {
          "relevance": 7,
          "clarity": 6,
          "discriminative": 8,
          "reasoning": "Relevance: Tests core indexing skills and Elasticsearch (implied by search index role), but generic/not entry-level specific and no Python details in role (-1 generic, -2). Clarity: Clear intent but lacks constraints on time, batch size, error handling scope (-2 missing constraints). Discriminative: Good for applied pipeline design with trade-offs (efficiency/reliability/scalability), separates those understanding bulk indexing basics.",
          "composite": 7.0
        }
      },
      {
        "role_id": "be-431",
        "role_title": "Staff Engineer - Real-time Collaboration",
        "question": "Design a real-time collaboration system that supports multiple users editing a shared document simultaneously. The system must handle concurrent edits, resolve conflicts when users edit overlapping regions, and ensure that all users see the same content in real time. Use CRDTs to manage conflict resolution. You must support a simple document structure (e.g., a text document with line breaks and text segments). Describe your design choices, including how you would store the document state, how you would handle concurrent edits, and how you would resolve conflicts using a CRDT.",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core focus on real-time collaboration, conflict resolution, and operational transforms via CRDTs (no red flags). Clarity: Crystal clear scope with simple document structure and specific design aspects requested; minor deduction as no explicit time/format constraints. Discriminative: Tests deep applied knowledge of CRDTs, trade-offs in concurrent editing, and system design choices, strongly separating strong architects from weaker candidates.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-432",
        "role_title": "Backend Engineer - Data Validation",
        "question": "\"Design and implement a data validation framework using Great Expectations to validate data from a PostgreSQL table (e.g., `sales` table) before loading it into S3. The validation should check for missing values, data type correctness, and statistical outliers (e.g., sales > 1000000). Include a minimal example of how this would be used in an Airflow DAG to trigger validation and fail the pipeline if any validation fails.\"",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core data validation (schema, stats, outliers) and pipeline integration with key tools (Great Expectations, PostgreSQL, S3, Airflow); minor ding as GE not explicitly listed but perfectly matches role focus (-1). Clarity: Crystal clear, well-scoped with specific table, checks, and Airflow integration; no bundling or ambiguity. Discriminative: Requires applied design/implementation knowledge, trade-offs in validation/outliers, and pipeline failure handling; strong separation of strong vs weak candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-433",
        "role_title": "Senior Backend Engineer - Platform APIs",
        "question": "\"Design a scalable, secure, and consistent service catalog API in Go using gRPC that allows developers to discover, provision, and manage platform services (e.g., databases, storage, compute). The API must support versioned service definitions, support both synchronous and asynchronous provisioning, and provide a way to list available services with filtering by category, region, and tags. Additionally, the API must support rate limiting and authentication via JWT tokens. Describe your design decisions, including how you would handle versioning, asynchronous provisioning, and security.\"",
        "scores": {
          "relevance": 9,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core role responsibilities (service catalog, provisioning APIs) with Go/gRPC matching platform API focus; minor ding for assuming gRPC specificity (-1). Clarity: Well-scoped with clear requirements, but lacks explicit time/format constraints for screening (-2). Discriminative: Excellent at probing trade-offs in versioning, async flows, security, and scalability, distinguishing strong API designers.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-434",
        "role_title": "Principal Engineer - Database Strategy",
        "question": "\"Design a database strategy for a multi-region, globally distributed application using PostgreSQL, CockroachDB, Redis, and Kafka. Your strategy must include:  \n- A clear rationale for selecting each database technology (PostgreSQL, CockroachDB, Redis, Kafka)  \n- A detailed architecture for data replication, consistency, and availability across regions  \n- A plan for handling data migration from a monolithic PostgreSQL database to this distributed setup  \n- A discussion of operational excellence and monitoring for database performance and health  \n- A brief mention of how you would mentor junior engineers in database strategy and architecture\"",
        "scores": {
          "relevance": 4,
          "clarity": 8,
          "discriminative": 3,
          "reasoning": "Relevance: Tangentially related to core role (multi-region DB strategy) but generic and bundles unrelated techs (Redis, Kafka not core DBs); forces use of unmentioned CockroachDB/Redis/Kafka (-2 each, generic -3). Clarity: Well-structured and unambiguous, but lacks time/format constraints for screening (-1, multiple bundled parts -2 mitigated by bullets). Discriminative: Too broad (design full system + migration + ops + mentoring >10min -2), tests recall over deep tradeoffs, weak separation.",
          "composite": 5.0
        }
      },
      {
        "role_id": "be-435",
        "role_title": "Junior Backend Engineer - Queue Consumers",
        "question": "\"Design and implement a Python consumer for a RabbitMQ queue that processes messages with the following requirements:  \n1. Acknowledge messages only after successful processing (with a 5-second timeout for processing).  \n2. Implement idempotency by using a PostgreSQL database to track processed message IDs.  \n3. Handle message failures gracefully by moving failed messages to a dead-letter queue (DLQ).  \n4. Ensure the consumer restarts automatically after a crash (using Docker and a health check).  \n5. Include basic logging and error handling.",
        "scores": {
          "relevance": 7,
          "clarity": 6,
          "discriminative": 8,
          "reasoning": "Relevance: Directly tests core skills (acks, DLQ, idempotency) and exact tech (Python, RabbitMQ, PostgreSQL), but Docker/health checks go beyond entry-level focus (-2 for minor tech not core). Clarity: Clear requirements but vague on implementation scope, depth, and exact Docker setup; multiple bundled aspects (-2). Discriminative: Strong on applied knowledge/trade-offs in message handling, separates good entry-level candidates well.",
          "composite": 7.0
        }
      },
      {
        "role_id": "be-436",
        "role_title": "Backend Engineer - API Documentation",
        "question": "\"Design and implement a function in TypeScript that takes an OpenAPI 3.0 schema object and generates a human-readable, markdown-formatted documentation block for a given endpoint. The function should include:  \n- Path parameters, query parameters, request body, and response schema  \n- Example values for all parameters and responses  \n- Versioning support (e.g., `/v1/users`)  \n- Proper handling of nested schemas  \n- Markdown formatting with headers and code blocks  \n- Error handling for invalid schema structures\"",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests OpenAPI schema handling, documentation generation, versioning, and nested schemas\u2014core to API docs tooling role (-1 for no explicit SDK mention). Clarity: Precise scope with exact requirements listed, unambiguous for screening. Discriminative: Requires applied TypeScript skills, design trade-offs (e.g., example gen, nesting, errors), distinguishes strong implementers from weak ones.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-437",
        "role_title": "Senior Backend Engineer - Secrets Management",
        "question": "\"Design a secure secrets management solution using HashiCorp Vault and Kubernetes, including automatic secret rotation for database credentials, encryption-at-rest for stored secrets, and audit logging. How would you configure Vault to ensure that Kubernetes pods can securely access secrets without exposing them in logs or configuration files? Explain your approach to managing access policies, rotating database credentials automatically, and handling audit logs.\"",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core focus on secrets management, Vault, rotation, encryption-at-rest, and audit logging in Kubernetes context. Clarity: Crystal clear and well-scoped for screening, minor deduction for lacking explicit time/format constraints. Discriminative: Tests deep applied knowledge, trade-offs in Vault config, policies, and integration, strongly separating strong candidates.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-438",
        "role_title": "Staff Engineer - ML Serving Infrastructure",
        "question": "\"Design a scalable and fault-tolerant ML serving solution using Triton Inference Server with Kubernetes. Your design should include model versioning, dynamic batching, and latency optimization strategies. Include a high-level architecture diagram and explain how you would implement dynamic batching and model versioning. Also, describe how you would monitor system health and detect model drift.\"",
        "scores": {
          "relevance": 10,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core needs (ML serving, batching, versioning, latency) with Triton/K8s as relevant tools. Clarity: Clear and well-scoped but bundles diagram request and multiple components (-2). Discriminative: Tests deep applied knowledge, trade-offs in design/monitoring/drift; strong separation for experts.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-439",
        "role_title": "Backend Engineer - Billing Integration",
        "question": "\"Design and implement a Python function that handles the proration of a subscription price change in a billing system using Stripe. The function should:\n- Accept the old and new subscription prices, the subscription start date, and the current date.\n- Calculate the prorated amount for the current period based on the time elapsed between the start date and the current date.\n- Account for the fact that proration only applies to the current billing period, not future periods.\n- Return the prorated amount to be applied to the invoice.\"",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core proration skill for billing/subscription role using Stripe (mentioned), minor ding for no full role desc tech stack but spot-on focus (-1). Clarity: Crystal clear inputs/outputs, well-scoped function, no ambiguity. Discriminative: Requires applied math/logic for proration (time ratios, period boundaries), tests deep billing understanding and edge cases like partial periods, strong separation.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-440",
        "role_title": "Junior Backend Engineer - Metrics Collection",
        "question": "You're adding metrics to a Go service using Prometheus. The service has a health check endpoint that returns a 200 status code when healthy and a 500 status code when unhealthy. You want to expose the health status as a metric, so you decide to use a Gauge. You set up the metric with a constant value of 1 for healthy and 0 for unhealthy. However, you notice that the metric doesn't update when the service becomes unhealthy \u2014 it stays at 1 even when the service fails.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 8,
          "reasoning": "Relevance: Directly tests core metrics skills (Gauge pattern) and Go/Prometheus from role, entry-level appropriate (-0 red flags). Clarity: Crystal clear scenario with specific issue, well-scoped. Discriminative: Tests applied Prometheus knowledge (likely Set vs. Inc), reveals understanding of metric mutation; good separation but not extreme trade-offs.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-441",
        "role_title": "Senior Backend Engineer - Autocomplete Systems",
        "question": "**\"Design a distributed autocomplete system that supports real-time personalization and achieves sub-50ms latency at 100K QPS. How would you structure the data pipeline from ingestion to user query processing, and what specific optimizations would you apply to ensure low latency and high throughput? Please describe how you'd handle personalization (e.g., user-specific recommendations) and how you\u2019d balance consistency and performance in a distributed environment.\"**",
        "scores": {
          "relevance": 10,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core requirements (autocomplete, personalization, exact latency/QPS targets). Clarity: Clear intent and well-scoped, but lacks explicit time/format constraints for screening (-1) and slightly bundles pipeline + optimizations + personalization (-1). Discriminative: Excellent at testing trade-offs, architecture, and deep optimizations; distinguishes strong systems thinkers from weak ones.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-442",
        "role_title": "Backend Engineer - Event Streaming",
        "question": "Design a system to fan out real-time updates from a PostgreSQL database to multiple downstream services using NATS and Redis. The system must support filtering by event type and allow replay of events from a specific point in time. Describe how you would structure the event ingestion pipeline, handle filtering, and ensure durability and replayability.",
        "scores": {
          "relevance": 9,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (fan-out, filtering, replay) with PostgreSQL (likely DB in role context); minor ding for specifying NATS/Redis not in role desc (-2 red flag adjusted). Clarity: Well-scoped design question with specific requirements, but lacks explicit constraints like scale/throughput (-1 from red flag). Discriminative: Requires applied knowledge of pipelines, trade-offs in durability/filtering/replay with specific tech, strongly separates candidates.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-443",
        "role_title": "Senior Backend Engineer - Data Lake Architecture",
        "question": "\"Design a data pipeline for ingesting and processing daily sales data into a data lake using Apache Iceberg. The data comes from a variety of sources (e.g., point-of-sale systems, web analytics) and is stored in S3. The pipeline must support schema evolution, allow efficient partition pruning for performance, and enable time travel queries for auditability. Describe how you would structure the pipeline, including the data flow, table format configuration, partitioning strategy, and how you would handle schema evolution and time travel.\"",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core needs (data lake, Iceberg table formats, schema evolution, partition pruning, time travel). No red flags. Clarity: Crystal clear, well-scoped design question with explicit requirements; minor deduction for no strict time/format constraints. Discriminative: Tests applied architecture knowledge, trade-offs in partitioning/schema/time travel, strongly separates experienced candidates.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-444",
        "role_title": "Backend Engineer - Service Templates",
        "question": "\"Design a service template using Cookiecutter that generates a Go-based microservice with best practices, including proper directory structure, configuration management, logging, and CI/CD setup via GitHub Actions. The template should also include a minimal observability layer (e.g., logging with structured format and metrics via Prometheus) and enforce security best practices (e.g., environment-specific configuration, secure secrets handling, and use of default TLS settings). Describe how you would structure the template, what files would be included, and how GitHub Actions would validate and deploy the service to Kubernetes with ArgoCD.\"",
        "scores": {
          "relevance": 9,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (service scaffolding with Cookiecutter, CI/CD, observability, security) and exact tech (Go, GitHub Actions, Kubernetes/ArgoCD); minor ding for not explicitly mentioning 'templates with best practices' but covers it (-1). Clarity: Well-formed and specific, but 'design a service template' lacks minor constraints like expected detail level or time (-2). Discriminative: Excellent at testing applied knowledge, trade-offs in structure/secrets/observability/CI/CD, distinguishes deep expertise from superficial knowledge.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-445",
        "role_title": "Junior Backend Engineer - Database Migrations",
        "question": "You're working on a Python application using SQLAlchemy and Alembic for database migrations. You need to create a migration that adds a new column `email` to the `users` table, but you want to ensure that the migration handles the case where some users may already have an email in the database.",
        "scores": {
          "relevance": 9,
          "clarity": 6,
          "discriminative": 7,
          "reasoning": "Relevance: Directly tests Alembic/SQLAlchemy migrations and handling existing data, core to role's database migration focus (-1 for generic 'handles the case' without zero-downtime/rollback specifics). Clarity: Clear scenario but vague on 'handles the case' (backfill? nullability? constraints?), missing explicit constraints (-2 for vagueness, -1 for ambiguous jargon). Discriminative: Good test of applied migration knowledge like upgrade/downgrade with defaults/backfills, separates those understanding real-world pitfalls from basics.",
          "composite": 7.333333333333333
        }
      },
      {
        "role_id": "be-446",
        "role_title": "Staff Engineer - Distributed Tracing",
        "question": "\"Design a sampling strategy for a high-throughput, distributed system using OpenTelemetry and Jaeger, where traces are generated at 10,000 transactions per second (TPS) and must be stored in ClickHouse. Consider the trade-offs between different sampling strategies (e.g., rate-based, probabilistic, deterministic) and the impact on observability, performance, and cost. Propose a hybrid sampling strategy that balances trace visibility, performance, and cost, and explain how you would implement and validate this strategy in production.\"",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core focus on distributed tracing, sampling strategies at scale, with exact tech stack (OpenTelemetry, Jaeger, ClickHouse). No red flags. Clarity: Crystal clear, well-scoped with specific TPS and trade-offs; minor deduction for lacking explicit time constraint. Discriminative: Tests deep trade-offs, design, implementation, and validation\u2014distinguishes architects from juniors effectively.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-447",
        "role_title": "Backend Engineer - Load Shedding",
        "question": "*Design a load shedding mechanism for a high-traffic API endpoint that uses Envoy as a proxy and Redis for rate limiting. The endpoint experiences random spikes in traffic, causing service degradation. When the system detects a spike, it should automatically trigger load shedding to prioritize critical requests (e.g., user authentication) over non-critical ones (e.g., analytics reporting).*",
        "scores": {
          "relevance": 9,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core load shedding, prioritization, and adaptive handling from role; minor ding for specifying Envoy/Redis not in role desc (-1). Clarity: Clear intent and scenario, but lacks explicit constraints like time or output format (-1). Discriminative: Excellent at probing trade-offs in detection, prioritization, and integration, separating strong systems thinkers.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-448",
        "role_title": "Senior Backend Engineer - Webhook Infrastructure",
        "question": "\"Design a resilient webhook delivery system in Go using Kafka and PostgreSQL/Redis for delivery guarantees, retry logic, and analytics. Specifically, how would you ensure message delivery is guaranteed, handle failures, and track delivery status with analytics?\"",
        "scores": {
          "relevance": 10,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core role requirements (guaranteed delivery, retry logic, analytics) with relevant tech (Go, Kafka, PostgreSQL/Redis). Clarity: Clear intent and specific, but lacks scoping constraints like time or detail level (-2 red flag). Discriminative: Tests applied system design, trade-offs in resilience/failures/analytics, strong separation of strong vs weak candidates (not pure recall).",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-449",
        "role_title": "Junior Backend Engineer - Auth Integration",
        "question": "*Design a secure OAuth2 flow for a social login (e.g., Google) integrated into a Node.js + TypeScript application using PostgreSQL. Include how you would handle token storage, refresh, and prevent replay attacks.*",
        "scores": {
          "relevance": 9,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core OAuth2 flows, token handling, refresh, and social login from role; matches Node.js/TS stack (assuming common for entry-level), PostgreSQL storage relevant (-1 minor for no explicit Node/TS mention in role). Clarity: Clear and well-scoped for screening, but lacks explicit constraints like time or response format (-2). Discriminative: Excellent at probing security trade-offs, applied knowledge of refresh/prevention, distinguishes deep vs superficial understanding.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-450",
        "role_title": "Principal Engineer - ML Platform Strategy",
        "question": "\"Design a scalable and secure ML platform strategy for a large enterprise with distributed teams across multiple geographies, including considerations for data governance, model lifecycle management, and team enablement. Your design must address how to ensure consistent model quality, compliance with regulatory requirements (e.g., GDPR, HIPAA), and integration with existing CI/CD pipelines. Please describe your approach to tooling selection, governance models, and how you would onboard new ML engineers to the platform.\"",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core responsibilities (strategy, tooling, governance, enablement) with no red flags. Clarity: Crystal clear and well-scoped for screening, minor deduction for lacking explicit time/format constraints. Discriminative: Tests deep applied knowledge, trade-offs (scalability/security/governance), and strategic thinking to separate strong strategists from juniors.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-451",
        "role_title": "Backend Engineer - Subscription Billing",
        "question": "\"Design a system to handle proration for a subscription billing model where users can change plans mid-month. Your design should include how usage metering is integrated, how proration is calculated, and how it's stored and updated in the database. Use the following constraints:  \n- All calculations must be precise to the second (e.g., 10.5 seconds of usage = 10.5).  \n- The system must support plan changes at any time, not just at the start of a billing period.  \n- You must use the provided tech stack: Java, PostgreSQL, Kafka, Spring Boot, Redis.  \n- Consider edge cases like partial billing periods and overlapping changes.  \n- Avoid using external libraries for financial calculations.\"",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (proration, usage metering, plan changes) with exact tech stack; minor ding for over-specifying stack (-1). Clarity: Well-scoped with precise constraints, unambiguous despite system design scope. Discriminative: Probes deep applied knowledge, trade-offs, edge cases, and tech integration for strong separation.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-452",
        "role_title": "Senior Backend Engineer - Faceted Search",
        "question": "\"Design and implement a faceted search system that supports dynamic filtering and accurate facet counts at scale. Consider performance, correctness, and scalability when designing your solution. Focus on the challenges of maintaining facet count accuracy when dealing with large datasets and high query loads. How would you ensure accurate facet counts without sacrificing performance?\"",
        "scores": {
          "relevance": 10,
          "clarity": 6,
          "discriminative": 8,
          "reasoning": "Relevance: Perfectly matches role's core focus on faceted search, dynamic filtering, and facet accuracy at scale (no red flags). Clarity: Clear intent but 'design and implement' is overly broad for screening without constraints like time, tech stack, or scope (-2 for lacking constraints). Discriminative: Strong on trade-offs (accuracy vs performance at scale), separates candidates with deep understanding from superficial ones.",
          "composite": 8.0
        }
      },
      {
        "role_id": "be-453",
        "role_title": "Backend Engineer - Notification Delivery",
        "question": "\"Design a system to deliver push notifications in real-time to mobile devices using Go, Redis, and FCM/APNS. Your solution should handle device registration, batching, and delivery tracking. Explain how you\u2019d ensure reliability, scalability, and performance, and how you\u2019d handle failure scenarios.\"",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core requirements (device registration, batching, delivery tracking) and specifies Go/Redis/FCM/APNS. Clarity: Well-scoped system design with specific components; minor deduction for lacking explicit time/format constraints. Discriminative: Tests deep applied knowledge of trade-offs, reliability, scalability, and failures, strongly separating strong candidates.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-454",
        "role_title": "Junior Backend Engineer - ETL Scripts",
        "question": "Write a Python script using pandas and PostgreSQL that reads a CSV file, performs a basic data transformation (e.g., filtering, aggregating), and writes the result back to a PostgreSQL table. The script should include error handling for common issues like missing files, invalid data types, and database connection failures. Use Airflow to schedule the script to run daily at 2 AM.",
        "scores": {
          "relevance": 3,
          "clarity": 4,
          "discriminative": 5,
          "reasoning": "Relevance: Directly mentions pandas/PostgreSQL/ETL (core) but Airflow scheduling is advanced for entry-level role focused on learning basics (-3 generic ETL, -2 Airflow not entry-level core). Clarity: Bundled multi-part (script + Airflow) without constraints (-2), vague on transformation specifics. Discriminative: Tests ETL basics but Airflow adds unnecessary barrier for screening; weak separation at entry-level.",
          "composite": 4.0
        }
      },
      {
        "role_id": "be-455",
        "role_title": "Staff Engineer - Developer Portal",
        "question": "*Design a scalable plugin architecture for a developer portal built with Backstage using React and Node.js. Your design should support dynamic service discovery and documentation generation, and enable plug-in teams to onboard new services with minimal friction. Include considerations for plugin lifecycle, configuration, and extensibility. Describe how you would structure the core plugin system to support modular development and ensure maintainability across a growing team.*",
        "scores": {
          "relevance": 9,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core role responsibilities (plugin architecture, service discovery, documentation, onboarding) and exact tech stack (Backstage, React, Node.js); minor generic feel but no red flags. Clarity: Well-formed and unambiguous, but lacks explicit screening constraints (e.g., time/depth/format). Discriminative: Excellent at testing trade-offs, architectural thinking, and deep Backstage knowledge to separate strong architects from average devs.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-456",
        "role_title": "Backend Engineer - Query Optimization",
        "question": "\"Explain how you would instrument and analyze a PostgreSQL database to detect and diagnose slow queries using pganalyze, and walk through a real-world scenario where you used this approach to improve performance. Include specific SQL queries, indexes, and plan analysis steps.\"",
        "scores": {
          "relevance": 7,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Tests core slow query detection and plan analysis but assumes pganalyze (not mentioned in role) and demands personal real-world experience (-3 generic red flag partially, -2 unlisted tech). Clarity: Well-formed and scoped but bundles explanation, scenario, and specifics without time/format constraints (-2). Discriminative: Excellent at separating via applied trade-offs, tools, and real experience; not trivial recall.",
          "composite": 8.0
        }
      },
      {
        "role_id": "be-457",
        "role_title": "Senior Backend Engineer - SLO Automation",
        "question": "\"Design a system that automatically detects and alerts on SLO burn rate violations in a Kubernetes environment using Go, Prometheus, and PostgreSQL. Include how you would track error budgets, handle alerting, and ensure reliability in production. Be specific about how you'd use Sloth for SLO definitions and how you'd integrate with Kubernetes monitoring (e.g., via Prometheus).\"",
        "scores": {
          "relevance": 7,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core SLO/burn rate/error budget skills and key tech (Kubernetes, Prometheus, Go, PostgreSQL), but deduct -2 for Sloth (unmentioned in role) and -1 for generic design scope. Clarity: Well-formed and specific, minor deduction for lacking time/format constraints. Discriminative: Excellent at testing applied system design, integrations, trade-offs, and production reliability, strongly separating candidates.",
          "composite": 8.0
        }
      },
      {
        "role_id": "be-458",
        "role_title": "Backend Engineer - API Gateway Plugins",
        "question": "What are the key differences between Kong's `on_pre_balance` and `on_post_balance` lifecycle events in a plugin, and how would you use them to implement a custom authentication flow that validates user session state and enriches the response with user metadata?",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests Kong plugin development (core to API gateway plugins for auth/transformation), no red flags. Clarity: Precise and well-scoped, minor deduction only if constraints like code format unspecified. Discriminative: Requires deep understanding of Kong lifecycle, trade-offs in auth flow placement, and applied implementation for strong candidate separation.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-459",
        "role_title": "Junior Backend Engineer - Container Security",
        "question": "\"Walk me through how you would set up a CI/CD pipeline to scan container images for vulnerabilities using Trivy, and explain how you would ensure these scans are both effective and non-disruptive to the development workflow.\"",
        "scores": {
          "relevance": 7,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Tests core container scanning skills for the role but specifies Trivy (unmentioned in role desc, -2) and CI/CD (relevant but entry-level focus is implementation/learning vuln detection). Clarity: Well-formed with clear intent, though lacks minor constraints like tools/time. Discriminative: Excellent at probing applied knowledge, trade-offs (effective vs non-disruptive), and practical workflow integration, strongly separating candidates.",
          "composite": 8.0
        }
      },
      {
        "role_id": "be-460",
        "role_title": "Backend Engineer - Experiment Tracking",
        "question": "\"Using MLflow, design and implement a Python script that tracks a simple machine learning experiment. The script should:  \n1. Initialize an MLflow experiment with a unique name (e.g., 'experiment_123').  \n2. Log hyperparameters (e.g., learning rate, batch size) and metrics (e.g., accuracy, loss).  \n3. Save a model artifact (e.g., a trained scikit-learn model) to MLflow's artifact store.  \n4. Ensure the experiment is properly tracked and closed.  \n5. Use Docker to containerize the script so it can be run in a production-like environment.  \n6. Provide instructions on how to run the script locally and in Docker.\"",
        "scores": {
          "relevance": 5,
          "clarity": 6,
          "discriminative": 5,
          "reasoning": "Relevance: Directly tests MLflow (relevant for experiment tracking) but adds Docker (unmentioned in role) and goes beyond core metric logging/model artifacts (-2); generic scripting could apply broadly (-3, net 10-5=5). Clarity: Clear steps but bundles multiple tasks (script + Docker + instructions) without time/format constraints (-2); vague on 'simple ML' details (7-2=5, adjusted to 6). Discriminative: Tests applied MLflow knowledge (good) but Docker/containerization dilutes ML infra focus and risks over-broad scope (>10min, -2); basic implementation over trade-offs (7-2=5).",
          "composite": 5.333333333333333
        }
      },
      {
        "role_id": "be-461",
        "role_title": "Senior Backend Engineer - Fraud Detection",
        "question": "\"Design a real-time fraud scoring pipeline for a payment system that must handle 10K transactions per second. The pipeline should integrate a rule engine with a machine learning model and support manual review queues. Describe how you would structure the data flow, key components, and how you'd balance real-time performance with model accuracy and reliability.\"",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core skills (fraud pipelines, rule engines, ML integration, manual queues) with exact tech focus. Clarity: Crystal clear and well-scoped with 10K TPS constraint, though minor deduction for unspecified output format/depth. Discriminative: Excellent at testing architecture trade-offs (performance vs accuracy/reliability), deep understanding, and applied knowledge for strong candidate separation; appropriately scoped for screening (<10min high-level).",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-462",
        "role_title": "Backend Engineer - Alert Routing",
        "question": "\"Design a system to route alerts from a distributed system to the appropriate on-call team using Alertmanager, with support for dynamic escalation policies, silence management, and deduplication. Explain how you would structure the routing logic, manage silences, and ensure alerts are not duplicated during high-volume events.\"",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Directly tests core role skills (routing, escalation, silence, deduplication) using specified Alertmanager; crystal clear with focused scope on key components; probes applied knowledge, trade-offs in high-volume scenarios for strong differentiation. Minor ding for assuming Alertmanager familiarity without role confirmation, but aligns closely.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-463",
        "role_title": "Junior Backend Engineer - Full-text Search",
        "question": "You are tasked with implementing a simple full-text search feature in a Python application using PostgreSQL. The application has a table `products` with columns `id`, `name`, and `description`. Users can search for products by entering a query string. When a query is submitted, return a list of products matching the search term, ranked by relevance.",
        "scores": {
          "relevance": 9,
          "clarity": 8,
          "discriminative": 8,
          "reasoning": "Relevance: Directly tests PostgreSQL full-text search core to role (tsvector/ranking implied), minor ding for not explicitly naming tsvector (-1). Clarity: Clear task and goal, but lacks constraints on output format, query handling, or depth (-2). Discriminative: Requires applied knowledge of FTS implementation and ranking, separates those who know SQL/Python integration from theorists; not trivia but needs trade-offs for 10.",
          "composite": 8.333333333333334
        }
      },
      {
        "role_id": "be-464",
        "role_title": "Staff Engineer - Presence Systems",
        "question": "Design a presence system that tracks real-time user presence and typing indicators using Go, Redis, and WebSockets. The system must support millions of concurrent users. Describe your architecture, data model, and how you would handle scale and reliability.",
        "scores": {
          "relevance": 9,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (presence, typing indicators, scale) and specifies Go/Redis/WebSockets (assuming role match); minor deduction for not explicitly mentioning heartbeats/state sync. Clarity: Well-scoped for screening but lacks precise constraints (e.g., exact scale metrics, latency targets). Discriminative: Excellent at probing architecture, trade-offs, scaling/reliability decisions for strong separation.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-465",
        "role_title": "Backend Engineer - CDC Pipelines",
        "question": "*Design a CDC pipeline using Debezium and Kafka to replicate data from a PostgreSQL database to Snowflake. Include the configuration of Debezium's connector, handling of schema changes, and an approach to ensure exactly-once delivery. Explain how you would handle initial data synchronization (snapshot) and ensure data consistency during schema changes.*",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Directly tests core CDC pipeline skills (schema changes, snapshots, exactly-once) with exact tech stack (Debezium, Kafka, PostgreSQL, Snowflake) from role. Crystal clear and well-scoped for screening; minor deduction for no explicit time/format constraints but highly specific. Strong trade-offs in exactly-once and consistency distinguish experts.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-466",
        "role_title": "Senior Backend Engineer - GitOps Tooling",
        "question": "\"Design a drift detection mechanism in a GitOps pipeline using ArgoCD and Helm. Your solution should detect when a deployed Kubernetes manifest diverges from its Git source, and automatically trigger a rollback to the last known good state if drift exceeds a configurable threshold. Explain your approach, including how you would implement reconciliation, detect drift, and handle rollback. Consider edge cases such as partial drift, temporary mismatches, and network issues.\"",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core GitOps skills (drift detection, reconciliation, rollback) with exact tech stack (ArgoCD, Helm, K8s manifests) from role. No red flags. Clarity: Crystal clear, well-scoped with specific requirements and edge cases; minor deduction for no explicit time/format constraint. Discriminative: Tests deep applied knowledge, trade-offs, and edge cases, strongly separating strong candidates who can reason through GitOps complexities from weak ones.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-467",
        "role_title": "Backend Engineer - Connection Pooling",
        "question": "\"Design and implement a Go-based connection pool manager that uses PgBouncer as a proxy to manage PostgreSQL connections. The manager should support dynamic pool sizing based on load, automatic failover to a backup PostgreSQL instance when primary fails, and monitor connection health using a health check endpoint. How would you structure the configuration, handle failover, and ensure connection health is maintained in a Kubernetes environment with horizontal pod autoscaling (HPA)?\"",
        "scores": {
          "relevance": 4,
          "clarity": 5,
          "discriminative": 7,
          "reasoning": "Relevance: Superficially mentions Go and PostgreSQL but heavily focuses on PgBouncer/Kubernetes/HPA not in role (red flag -2 tech not listed, generic infra design -3). Clarity: Bundled multi-part design question without scope/time constraints in screening context (red flag -2 multiple questions, -2 design w/o constraints). Discriminative: Tests applied trade-offs in pooling/failover but too broad for screening (>10min, advanced infra knowledge).",
          "composite": 5.333333333333333
        }
      },
      {
        "role_id": "be-468",
        "role_title": "Junior Backend Engineer - CI Pipelines",
        "question": "\"Design a GitHub Actions workflow that builds and tests a Python project using Docker. The workflow should include caching for Python dependencies, parallelize tests across multiple CPU cores, and run tests in a containerized environment. Provide a clear explanation of your choices for each step, including caching strategy and parallelization method.\"",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core CI pipeline skills (workflow syntax, caching, test parallelization) for GitHub Actions in Python/Docker context (-1 for minor Docker addition not explicit). Clarity: Well-scoped with specific requirements and explanation request, unambiguous for screening. Discriminative: Requires applied knowledge of caching strategies, parallelization trade-offs, and design choices, strongly separates candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-469",
        "role_title": "Senior Backend Engineer - Chaos Engineering",
        "question": "*Design a failure injection experiment using Chaos Mesh to simulate a node failure in a Kubernetes cluster, and implement a recovery validation mechanism that ensures the application remains available and consistent after the failure. Include details on how you would configure the experiment, monitor the impact, and validate recovery using metrics and logs.*",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets Chaos Mesh, Kubernetes node failure, blast radius control, and recovery validation\u2014core to chaos engineering role (no red flags). Clarity: Crystal clear, well-scoped for screening with specific components, minor deduction for lacking explicit time/format constraints. Discriminative: Tests applied design, configuration, monitoring, trade-offs in Chaos Mesh/K8s, and validation logic\u2014strong separation of strong vs weak candidates.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-470",
        "role_title": "Backend Engineer - GraphQL Federation",
        "question": "*Design a schema stitching strategy for a federated GraphQL gateway that connects a user subgraph and a product subgraph. The user subgraph defines a `User` type with fields `id`, `name`, and `email`, while the product subgraph defines a `Product` type with fields `id`, `name`, and `field`. The user subgraph also contains a field `products` of type `[Product!]!` which references the product subgraph. Implement a solution that correctly resolves the `products` field on a `User` object when queried via the gateway. Provide a clear explanation of how the query planning and entity resolution work in this scenario, including how the gateway handles the cross-subgraph resolution.*",
        "scores": {
          "relevance": 9,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core federated GraphQL skills (schema stitching, query planning, entity resolution) with exact role tech stack; minor deduction for not mentioning supergraph explicitly (-1). Clarity: Well-scoped with specific schemas and clear intent, but lacks explicit constraints on format/response depth (-1) and bundles design/explanation (-1). Discriminative: Excellent at probing trade-offs in stitching, planning, and resolution, distinguishing deep Apollo Federation knowledge from superficial understanding.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-471",
        "role_title": "Staff Engineer - Zero Trust Architecture",
        "question": "*Design a solution using SPIFFE and mTLS to secure inter-service communication in a Kubernetes cluster where services need to be authenticated and authorized using identity-based policies. Include a flow from service startup to actual communication. Explain how SPIFFE ensures zero trust, and describe how you would integrate with HashiCorp Vault for identity management and policy enforcement.*",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core role skills (zero trust, mTLS, policy enforcement) with exact tech (SPIFFE, Kubernetes) from description; no red flags. Clarity: Well-scoped with specific flow and integration; minor deduction as no explicit time/format constraint. Discriminative: Tests deep applied knowledge, trade-offs, and architecture; strong separation between superficial and expert responses; not trivial recall.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-472",
        "role_title": "Backend Engineer - Vector Search",
        "question": "Design a system to handle semantic search with hybrid retrieval using Pinecone and PostgreSQL. The system should support embedding generation, index management, and retrieval with a fast, scalable, and reliable pipeline.",
        "scores": {
          "relevance": 8,
          "clarity": 5,
          "discriminative": 7,
          "reasoning": "Relevance: Directly tests core skills (embeddings, index mgmt, hybrid search) and matches role, but assumes Pinecone/pgSQL not explicitly listed (-2 for unlisted techs). Clarity: Clear intent but 'design a system' lacks scope/constraints like scale/size (-2 red flag, vague depth). Discriminative: Probes applied knowledge/tradeoffs in pipeline design, separates good from weak, but broad scope risks >10min answers (-2).",
          "composite": 6.666666666666667
        }
      },
      {
        "role_id": "be-473",
        "role_title": "Senior Backend Engineer - Refund Processing",
        "question": "Design a system to process partial refunds for a payment platform, ensuring ledger reconciliation and audit trails. The system must handle asynchronous provider callbacks and support rollback on failure. How would you structure the refund processing pipeline in a distributed, fault-tolerant environment using Java, Spring Boot, Kafka, and PostgreSQL? Include considerations for state management, error handling, and audit logging.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core role skills (partial refunds, ledger reconciliation, audit trails, provider callbacks) with exact tech stack (Java, Spring Boot, Kafka, PostgreSQL); minor -1 for not explicitly mentioning rollback but it's implied. Clarity: Well-scoped design question with clear constraints, tech specified, no bundling or vagueness. Discriminative: Probes deep applied knowledge of distributed systems, state management, fault-tolerance, and trade-offs in pipeline design using specified stack, strongly separates strong candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-474",
        "role_title": "Backend Engineer - Dashboards API",
        "question": "Write a Go function to render a Grafana dashboard JSON template with dynamic variables. The function should:\n- Accept a dashboard JSON as input\n- Accept a map of variable values (e.g., `map[string]string{\"instance\": \"db1\", \"region\": \"us-east\"}`)\n- Replace all variable references in the dashboard JSON (e.g., `{{instance}}` or `{{region}}`) with their values\n- Output the rendered JSON as a string\n- Handle edge cases like missing variables, invalid JSON, and malformed variable references",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests Go (core for APIs), JSON handling, template rendering, and variable substitution\u2014exact match for dashboard management role (-0 red flags). Clarity: Precise inputs/outputs, explicit edge cases, well-scoped for screening. Discriminative: Requires applied Go/JSON skills, error handling, edge cases with trade-offs (e.g., strict vs. graceful failure), distinguishes strong implementers.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-475",
        "role_title": "Junior Backend Engineer - Search Analytics",
        "question": "\"Imagine you're building a query logging system for a search engine. Users submit search queries, and we want to record the query, the user's session ID, and the timestamp. After processing, we want to store these logs in Elasticsearch. However, we also need to ensure that the logs are not duplicated due to network issues or retries.",
        "scores": {
          "relevance": 8,
          "clarity": 8,
          "discriminative": 8,
          "reasoning": "Relevance: Directly tests core query logging skill for search analytics role using Elasticsearch (inferred relevance); minor deduction for not mentioning click tracking/relevance metrics (-2 per red flag). Clarity: Well-scoped scenario with clear requirements, but lacks explicit constraints on solution format/depth (-2 per red flag). Discriminative: Good test of applied knowledge on deduplication (idempotency, unique keys), distinguishing candidates who understand trade-offs vs basic logging.",
          "composite": 8.0
        }
      },
      {
        "role_id": "be-476",
        "role_title": "Backend Engineer - Live Updates",
        "question": "Design a scalable real-time dashboard update system using Socket.io and Redis. The system must support room-based selective broadcasting, automatic reconnection, and handle concurrent connections efficiently. Provide a high-level architecture and explain how you would implement the following:",
        "scores": {
          "relevance": 9,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core skills (live updates, room management, selective broadcasting, reconnection) and matches tech (Socket.io implied, Redis for scaling); minor ding as Redis not explicitly in role but fits perfectly. Clarity: Clear and well-scoped for high-level design, but lacks explicit constraints like scale metrics (-1 red flag avoided). Discriminative: Excellent at probing architecture, trade-offs, and implementation depth for real-time systems, separating strong candidates.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-477",
        "role_title": "Senior Backend Engineer - Data Catalog",
        "question": "\"Design a system to automatically extract metadata from a data pipeline using Apache Atlas, and ensure lineage is properly tracked through the pipeline. Describe how you would integrate Kafka for real-time event streaming, Airflow for orchestration, and PostgreSQL for storing metadata. Include specific examples of how you would define entities, relationships, and attributes in Apache Atlas, and how you would trigger metadata updates when data is transformed or moved through the pipeline.\"",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (metadata extraction, lineage tracking) with relevant tools (Atlas for catalog/lineage, Kafka/Airflow/PostgreSQL fit data pipelines/governance); minor ding as role doesn't explicitly list these integrations (-1). Clarity: Well-scoped design question with specific components, examples requested, and clear triggers; no bundling or vagueness. Discriminative: Requires applied knowledge of integrations, Atlas entity modeling, and real-time triggers, testing trade-offs and depth to separate strong candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-478",
        "role_title": "Backend Engineer - Cost Attribution",
        "question": "Explain how you would design a system to track and attribute cloud costs to individual teams or projects, using resource tagging and a database. Consider the following constraints:\n- Each cloud resource (e.g., EC2 instance, S3 bucket) has a set of tags (like `team`, `project`, `environment`)\n- Costs are billed at the monthly level, and you need to generate monthly chargeback reports\n- The system must support real-time cost tracking and monthly reporting\n- You must store cost data efficiently and avoid overwriting or losing data\n- You must handle edge cases like missing tags or inconsistent tagging\n- You should use Go, PostgreSQL, and Prometheus for monitoring",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core role skills (cost attribution, tagging, chargeback reports) with exact tech stack (Go, PostgreSQL, Prometheus); minor ding for assuming Prometheus is listed. Clarity: Well-scoped with specific constraints, unambiguous intent for screening. Discriminative: Probes deep design trade-offs (real-time vs monthly, edge cases, efficiency), separates strong architects from basic coders.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-479",
        "role_title": "Principal Engineer - Data Architecture",
        "question": "> \"You're designing a data architecture for a company that processes 10M+ daily transactions and runs complex analytical queries on aggregates. The transactional system is currently a PostgreSQL database with high write throughput. You need to build a scalable, cost-efficient data warehouse for analytical queries.  \n>  \n> Describe how you would design the architecture, including choices for data ingestion, storage, and query processing. Justify each decision with clear trade-offs. Consider performance, cost, scalability, and data consistency.\"",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core data architecture strategy, tech selection (builds on PostgreSQL), and analytics/transactional workloads from role; no red flags. Clarity: Crystal clear scenario with specifics (10M+ txns, PostgreSQL), well-scoped; minor deduction for no explicit time limit. Discriminative: Requires applied knowledge of trade-offs across ingestion/storage/query (e.g., CDC vs batch, Snowflake vs BigQuery, columnar stores), strongly separates experts from juniors; not too broad for screening.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-480",
        "role_title": "Junior Backend Engineer - Deployment Automation",
        "question": "\"Write a simple Ansible playbook that deploys a Python application using Docker to AWS EC2. The playbook should:\n- Launch an EC2 instance (using a specified AMI and instance type)\n- Install Docker on the instance\n- Build and run a Docker container from a local Dockerfile\n- Ensure the container starts automatically on boot\"",
        "scores": {
          "relevance": 4,
          "clarity": 8,
          "discriminative": 7,
          "reasoning": "Relevance: Low - Ansible is unmentioned in role (focus: deployment scripts, config mgmt, rollbacks, env setup); tests advanced Docker/AWS/EC2 not core to entry-level; generic devops (-3), misses role focus (-2), minor tech only (-2). Clarity: Clear intent and well-scoped, but lacks minor constraints like AMI ID (-1 red flag implicit, still high). Discriminative: Good applied knowledge of playbook writing with multi-step logic, separates those who can vs can't; not trivial recall.",
          "composite": 6.333333333333333
        }
      },
      {
        "role_id": "be-481",
        "role_title": "Backend Engineer - Canary Deployments",
        "question": "Design a canary deployment pipeline using Istio and ArgoCD that automatically rolls back a service if the canary traffic metrics exceed a predefined threshold (e.g., error rate > 1% or latency > 500ms) over a 5-minute window. Include details on how traffic shifting, metric monitoring, and rollback automation are implemented.",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core focus on canary deployments, traffic shifting, metrics, and rollback; directly tests Istio/ArgoCD implied by infrastructure needs. Clarity: Crystal clear with specific thresholds, window, and implementation details requested; minor deduction for no explicit time/format constraints. Discriminative: Tests deep applied knowledge of integration, trade-offs in monitoring/automation, distinguishes experts from juniors.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-482",
        "role_title": "Senior Backend Engineer - API Monetization",
        "question": "\"Design a system to track API usage and enforce quotas for different pricing tiers. Describe how you would structure the data model in PostgreSQL, implement quota enforcement in Go, and integrate with Stripe for billing. Include how you would handle edge cases like quota overages and billing disputes.\"",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Directly tests core role skills (API usage tracking, quotas, pricing tiers, billing) with exact tech stack (PostgreSQL, Go, Stripe). Crystal clear and well-scoped for screening, though minor deduction for lacking time/format constraints. Excellent trade-off probing via edge cases distinguishes strong candidates.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-483",
        "role_title": "Backend Engineer - WAF Integration",
        "question": "\"Design a Go service that integrates with ModSecurity to detect and log suspicious HTTP requests, including rate limiting and bot detection. The service should use Redis for rate limiting, PostgreSQL for persistent logging, and Nginx as the proxy. Explain how you would structure the service, handle the ModSecurity events, and ensure accurate bot detection using attack pattern analysis.\"",
        "scores": {
          "relevance": 8,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core WAF integration, rate limiting, bot detection, and attack patterns with Go/Redis/PostgreSQL; minor ding for ModSecurity/Nginx not in role desc (-2). Clarity: Well-scoped design question with specific tech, but lacks time/format constraints (-1, not fully 9-10). Discriminative: Strong on trade-offs in service structure, event handling, and bot analysis; distinguishes deep applied knowledge.",
          "composite": 8.333333333333334
        }
      },
      {
        "role_id": "be-484",
        "role_title": "Staff Engineer - Model Registry",
        "question": "\"Design a model registry system using MLflow and S3 that supports model versioning, automated deployment to Kubernetes, and governance through access control. Walk through your design decisions, including how you would structure the data model, how you'd manage model versions, how you'd automate deployment, and how you'd implement access control. Be specific about how these components integrate with MLflow, SML, and Kubernetes.\"",
        "scores": {
          "relevance": 8,
          "clarity": 7,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (model registry, versioning, deployment, governance) and key tech (MLflow, S3, Kubernetes), but penalize -2 for undefined 'SML' not in role desc. Clarity: Clear intent and specific components, but vague on scope/depth and 'SML' ambiguity (-1 jargon). Discriminative: Excellent for probing trade-offs, integration, and deep design thinking to separate strong architects.",
          "composite": 8.0
        }
      },
      {
        "role_id": "be-485",
        "role_title": "Junior Backend Engineer - Payment Webhooks",
        "question": "*You're processing a payment webhook from Stripe. The event contains the `id` and `data.object.payment_intent.id`. You need to ensure the webhook is processed only once even if it's delivered multiple times. How would you implement idempotency at the webhook receiver level using Python and FastAPI?*",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core idempotency skill for payment webhooks role using exact tech (Python/FastAPI); minor -1 for not mentioning signature verification. Clarity: Crystal clear scenario, specific fields, well-scoped for screening. Discriminative: Requires applied knowledge of idempotency (e.g., DB checks, caching), trade-offs like race conditions, distinguishes strong entry-level candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-486",
        "role_title": "Backend Engineer - Span Processing",
        "question": "Design a tail sampling strategy for OpenTelemetry spans in a high-throughput Kafka-based tracing pipeline. The system must handle 100K+ spans per second, with 10% of spans being high-value (e.g., user login, payment processing), and 90% being low-value (e.g., internal service calls). The pipeline processes spans via a Go service that consumes from Kafka, applies sampling logic, and writes to ClickHouse. Tail sampling must ensure that high-value spans are retained with 95% accuracy, while low-value spans are dropped with a configurable threshold. Implement a sampling decision logic that avoids data loss for critical operations and uses minimal CPU overhead. Describe your approach, including how you would validate its effectiveness.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core role skills (tail sampling, span processing, Kafka/Go/ClickHouse pipeline) with precise metrics; minor ding for not mentioning attribute enrichment/storage optimization (-1). Clarity: Crystal clear, well-scoped with throughput, ratios, accuracy targets, and tech stack; no ambiguity or missing constraints. Discriminative: Requires deep applied knowledge of tail sampling trade-offs, low-CPU implementation, validation in high-throughput context; distinguishes experts via design decisions and effectiveness metrics.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-487",
        "role_title": "Senior Backend Engineer - Search Ranking",
        "question": "Design a system to evaluate the performance of a learning-to-rank (LTR) model in a production search environment. The system should:",
        "scores": {
          "relevance": 7,
          "clarity": 4,
          "discriminative": 6,
          "reasoning": "Relevance: Tests core LTR evaluation skills relevant to role (pipelines, production), but generic/misses specific tech stack and role emphases like feature engineering or A/B testing (-3 generic). Clarity: Clear high-level intent but 'design a system' lacks scope/constraints like components, metrics, scale, or time (-2 red flag), multiple interpretations possible. Discriminative: Tests applied knowledge of eval systems (metrics, offline/online, A/B) with some trade-offs, but too open-ended (>10min, vague depth).",
          "composite": 5.666666666666667
        }
      },
      {
        "role_id": "be-488",
        "role_title": "Backend Engineer - Gaming Backend",
        "question": "Design a real-time state synchronization mechanism for a multiplayer game where clients connect via WebSocket and need to keep their game state in sync. The state includes player positions, health, and score. Describe how you would structure the state, handle network latency, and ensure consistency across clients. Specifically, consider the impact of network latency and how you would prevent desynchronization or \"stale\" states.",
        "scores": {
          "relevance": 9,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (real-time state sync, WebSocket implied for multiplayer backend) with exact state elements mentioned; minor deduct for not mentioning matchmaking/anti-cheat (-1). Clarity: Well-scoped and unambiguous, but lacks explicit constraints like scale or perf targets (-1 from red flag on design scope). Discriminative: Excellent at probing trade-offs (latency, consistency models), deep understanding of sync techniques (e.g., snapshots, interpolation, reconciliation); distinguishes strong candidates.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-489",
        "role_title": "Backend Engineer - Data Quality",
        "question": "\"Design a data quality monitoring system using dbt, Great Expectations, and Airflow to detect freshness, volume anomalies, and schema drift in a Snowflake warehouse. Walk through your approach, including how you\u2019d structure the data quality checks, define metrics, and schedule the checks in Airflow. Explain how you\u2019d detect schema drift and handle volume anomalies using statistical methods.\"",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core needs (freshness, volume anomalies, schema drift) and specifies exact tools (dbt, Great Expectations, Airflow, Snowflake). Clarity: Well-scoped with specific components, though minor deduction for lacking explicit time/format constraints. Discriminative: Tests applied design, trade-offs, and statistical methods, strongly separating candidates with deep understanding.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-490",
        "role_title": "Senior Backend Engineer - Environment Management",
        "question": "\"Design and implement a simple Go service that manages a preview environment for a microservice. The service should:",
        "scores": {
          "relevance": 4,
          "clarity": 3,
          "discriminative": 5,
          "reasoning": "Relevance: Superficially mentions Go (assuming in role) but tests generic service design, not core namespace isolation/quotas/TTL; generic (-3), misses specifics. Clarity: Incomplete/broken (cuts off), multiple bundled reqs (-2), no constraints (-2). Discriminative: Vague design task tests basics but poor separation due to ambiguity and breadth (>10min, -2).",
          "composite": 4.0
        }
      },
      {
        "role_id": "be-491",
        "role_title": "Junior Backend Engineer - Read Replicas",
        "question": "You're implementing a read replica routing system using Python and PostgreSQL with SQLAlchemy. Your system needs to route read queries to the appropriate replica based on a consistent, deterministic rule. The primary database is always at `primary.db.example.com:5432`. Read replicas are at `replica1.db.example.com:5432`, `replica2.db.example.com:5432`, and `replica3.db.example.com:5432`.",
        "scores": {
          "relevance": 9,
          "clarity": 6,
          "discriminative": 7,
          "reasoning": "Relevance: Directly tests core read replica routing skills with PostgreSQL/SQLAlchemy (9), minor ding for assuming exact tech stack not explicitly listed (-1). Clarity: Clear setup but incomplete-missing specific task, constraints, or expected output (6). Discriminative: Good setup for testing routing logic and replication considerations, but lacks prompt for trade-offs or lag handling to strongly differentiate (7).",
          "composite": 7.333333333333333
        }
      },
      {
        "role_id": "be-492",
        "role_title": "Backend Engineer - CD Pipelines",
        "question": "\"Design a GitOps-based continuous deployment pipeline using ArgoCD and GitHub Actions that supports multi-environment promotion with approval gates and rollback automation. Explain how you would structure the pipeline, the role of each tool, and how you ensure safety and reliability.\"",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core skills (GitOps, multi-env promotion, approvals, rollbacks) and specifies exact tools; no red flags. Clarity: Crystal clear and well-scoped for screening, minor deduction for lacking explicit time/format constraints. Discriminative: Tests deep applied knowledge, trade-offs, and architecture decisions to separate strong candidates.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-493",
        "role_title": "Staff Engineer - Reliability Standards",
        "question": "\"Design a production readiness review (PRR) checklist for a new Go service deployed to Kubernetes, with monitoring and alerting via Prometheus and PagerDuty. The checklist should include at least one reliability-focused item (e.g., circuit breakers, timeouts), one observability item (e.g., metrics, logs), and one incident management item (e.g., escalation path). Explain your rationale for including each category.\"",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role responsibilities (PRR design, reliability standards) with specific tech stack (Go, K8s, Prometheus, PagerDuty) matching role focus; minor ding for assuming those exact tools not explicitly listed. Clarity: Crystal clear with explicit requirements, categories, and rationale request; well-scoped for screening. Discriminative: Requires applied design thinking, trade-offs in reliability/observability/incident mgmt, and justification, strongly separating candidates with SRE depth.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-494",
        "role_title": "Backend Engineer - API Mocking",
        "question": "\"Design a WireMock stub configuration in TypeScript that handles a GET request to `/api/v1/users/{id}` with the following requirements:\n- Match the path and query parameters (e.g., `?active=true`)\n- Match the request headers, specifically `Authorization: Bearer <token>` and `X-Client-ID: 123`\n- Return a JSON response with user data for a specific user ID (e.g., `id: 42`)\n- Include a 200 status code and a `Content-Type: application/json` header\n- Handle a 404 if the user ID is not found (use a 404 stub with a custom error message)\n- Ensure the stub is reusable and can be easily configured with dynamic values (e.g., via environment variables or configuration files)\"",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests WireMock configuration in TypeScript for API mocking, request matching, and response templating\u2014core to role (-1 minor for assuming WireMock specifically). Clarity: Well-scoped with precise requirements, no ambiguity, single focused task. Discriminative: Requires applied knowledge of matchers, stubs, conditional logic (404), and reusability\u2014distinguishes deep understanding from superficial knowledge.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-495",
        "role_title": "Senior Backend Engineer - Audit Logging",
        "question": "\"Design a Go service that receives audit events from Kafka and writes them to ClickHouse with immutable storage and a time-based retention policy. The service should ensure data integrity and prevent tampering. Explain how you would implement tamper-proof storage and handle retention. Include considerations for high availability, scalability, and performance.\"",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core role skills (tamper-proof storage, retention) using relevant tech (Go, Kafka, ClickHouse); minor ding as role doesn't explicitly list Kafka/ClickHouse (-1). Clarity: Well-scoped design question with clear requirements and considerations, no bundling or vagueness. Discriminative: Probes deep trade-offs in integrity, HA, scalability, and performance, strongly separating strong candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-496",
        "role_title": "Junior Backend Engineer - Model Serving",
        "question": "\"Design and implement a FastAPI-based model serving endpoint that loads a pre-trained scikit-learn model (e.g., Random Forest Classifier) from disk and serves predictions via a REST API. The endpoint should support batch inference on a list of input data (a list of dictionaries), handle model loading and caching via Redis for performance, and include basic error handling and logging.\"",
        "scores": {
          "relevance": 4,
          "clarity": 6,
          "discriminative": 8,
          "reasoning": "Relevance: Low - generic FastAPI/ML serving but adds unmentioned Redis (red flag -2), ignores role's core entry-level focus on model loading/batching/monitoring; superficially mentions tech stack. Clarity: Understandable but vague on specifics like Redis usage details, input format, model path (missing constraints). Discriminative: Good - tests applied implementation with batching/caching/error handling, separates those who can code vs theory.",
          "composite": 6.0
        }
      },
      {
        "role_id": "be-497",
        "role_title": "Backend Engineer - Payout Systems",
        "question": "Design a system to process payout batches for a marketplace platform. Each batch contains multiple user payouts, and payouts must be validated against bank account details before being processed. The system must support batch processing, retry mechanisms, and reconciliation of processed payouts. Describe your architecture, including how you would handle bank account validation, batch processing, and reconciliation. Use Go, PostgreSQL, Kafka, Redis, and gRPC for your design.",
        "scores": {
          "relevance": 7,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core payout batch processing, validation, and reconciliation from role, but mandates specific tech stack (Go, Kafka, etc.) not listed in role description (-2). Clarity: Well-scoped system design with clear components and tech specified, though lacks time/format constraints (-1). Discriminative: Excellent at probing architecture trade-offs, retries, and integration of validation/reconciliation, strongly separates strong candidates.",
          "composite": 8.0
        }
      },
      {
        "role_id": "be-498",
        "role_title": "Senior Backend Engineer - Profiling Infrastructure",
        "question": "\"Explain how you would design a continuous profiling system for a Go application running in Kubernetes, using Pyroscope for data collection, ClickHouse for storage, and Grafana for visualization. Include how you would handle baseline comparison, detect anomalies, and optimize storage for long-term retention.\"",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core role skills (continuous profiling, flame graphs implied, baseline comparison, storage optimization) and specifies relevant tech (Go/K8s aligns with infra, tools match needs); minor ding for not explicitly mentioning flame graphs (-1). Clarity: Crystal clear, well-scoped design with specific components and features, no ambiguity or missing constraints. Discriminative: Tests deep applied knowledge, trade-offs in design/anomaly detection/storage, strong separation of strong vs weak candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-499",
        "role_title": "Principal Engineer - Search Platform",
        "question": "Design a resilient, scalable search platform for an enterprise with 10M+ daily active users and 50M+ daily search queries. The platform must support real-time indexing and querying of unstructured data (e.g., product catalogs, user-generated content). It must handle data consistency, availability, and fault tolerance across distributed systems. Your solution should include high-level architectural decisions, data flow, fault tolerance, and recovery strategies. Consider the trade-offs between performance, cost, and operational complexity. Explain how you would ensure data consistency and availability under network partitions and node failures. Describe how you would monitor and respond to failures.",
        "scores": {
          "relevance": 4,
          "clarity": 5,
          "discriminative": 7,
          "reasoning": "Relevance: Superficially matches search platform strategy but generic distributed systems design; no role-specific tech stack, enterprise adoption focus, or team/ops strategy (-3 generic, -2 no listed tech). Clarity: Understandable but classic 'design a system' without time/format constraints (-2); multiple aspects bundled (-2). Discriminative: Good on trade-offs and deep topics like CAP/consistency under partitions, separates strong architects, but too broad (>10min).",
          "composite": 5.333333333333333
        }
      },
      {
        "role_id": "be-500",
        "role_title": "Staff Engineer - Streaming Architecture",
        "question": "You're designing a real-time data pipeline for a financial trading platform that must deliver market data to thousands of connected clients with zero data loss and sub-100ms latency. The pipeline consumes market data from Kafka, processes it in Rust, and delivers it via WebSocket. Design the event schema, delivery guarantees, and fan-out pattern for this system. Explain how you would ensure zero data loss and sub-100ms latency.",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (event schemas, delivery guarantees, fan-out) using Kafka (streaming infra) and Rust (processing), perfectly aligned. Clarity: Crystal clear intent and scope for screening, though lacks explicit time/format constraints (-1). Discriminative: Requires deep applied knowledge of trade-offs in schemas, guarantees (e.g., at-least-once), fan-out (e.g., pub-sub), and perf/loss mitigations, strongly separates strong architects from weak ones.",
          "composite": 9.333333333333334
        }
      }
    ]
  }
}