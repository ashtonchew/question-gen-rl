{
  "rl": {
    "model": "rl",
    "num_samples": 100,
    "avg_relevance": 8.94,
    "avg_clarity": 8.6,
    "avg_discriminative": 8.87,
    "avg_composite": 8.803333333333336,
    "results": [
      {
        "role_id": "be-401",
        "role_title": "Backend Engineer - Training Infrastructure",
        "question": "\"Imagine you're tasked with optimizing the training job scheduling in a Kubernetes cluster using Ray. Currently, the system experiences long queues and underutilized GPUs. How would you design a scheduling strategy that improves GPU utilization while minimizing job wait times, and what specific Ray or Kubernetes features would you leverage to implement this?\"",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core role skills (job scheduling, GPU allocation) with Ray (ML orchestration) and Kubernetes; minor ding as Ray isn't explicitly listed but fits distributed ML infra (-1). Clarity: Well-scoped design question with clear problem statement and specific tech focus, no bundling or ambiguity. Discriminative: Probes trade-offs in scheduling (utilization vs. wait times), applied knowledge of Ray/K8s features, distinguishes deep understanding from superficial knowledge.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-402",
        "role_title": "Backend Engineer - Model Deployment",
        "question": "*How would you design a model deployment pipeline using KServe and Kubernetes that supports automatic rollback to a previous version when a new model version fails health checks?*",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core model deployment, automation, rollback - matches role; minor ding as KServe/K8s not explicitly listed but highly relevant (-1). Clarity: Precise scope, unambiguous, well-formed for screening. Discriminative: Requires applied knowledge of pipelines, health checks, trade-offs in KServe/K8s, strong separation of strong vs weak candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-403",
        "role_title": "Backend Engineer - Inference Optimization",
        "question": "\"Describe how you would optimize a PyTorch model for inference using TensorRT, including the steps you'd take to implement quantization (e.g., FP16 or INT8) and how you'd verify that the optimized model maintains both performance and accuracy. Provide a high-level explanation of how you\u2019d ensure GPU memory usage is minimized during inference.\"",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core role skills (quantization, inference optimization) with PyTorch/TensorRT; minor ding as TensorRT not explicitly listed (-1). Clarity: Well-scoped, unambiguous, high-level focus with clear components. Discriminative: Probes applied knowledge, trade-offs (perf/accuracy/memory), and verification\u2014strong separation of strong candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-404",
        "role_title": "Senior Backend Engineer - Feature Platform",
        "question": "\"Design a feature computation pipeline using Feast and Spark that ingests raw data from a Kafka stream, computes features using Spark, and stores them in Feast. The pipeline should handle incremental updates and support feature discovery. Explain how you would structure the features (e.g., user-level vs. entity-level), how you\u2019d manage feature dependencies, and how you\u2019d ensure consistency and correctness during incremental updates.\"",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core focus on feature computation, online serving (Feast), and discovery; uses exact tech (Feast, Spark) from context. No red flags. Clarity: Crystal clear, well-scoped with specific components and aspects; minor deduction for lacking explicit time/format constraints. Discriminative: Tests deep applied knowledge, trade-offs (e.g., incremental consistency, dependencies, entity structuring), strongly separates strong candidates.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-405",
        "role_title": "Senior Backend Engineer - Model Monitoring",
        "question": "\"Describe how you would design a system to detect concept drift in a real-time production ML model using Python, Prometheus, and Grafana. Include at least one specific metric you would track, how you\u2019d compute it, and how you\u2019d set up alerts in Prometheus and Grafana to notify when drift is detected.\"",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Directly targets core role responsibilities (drift detection, performance tracking, alerting) with exact tech stack implied for production ML monitoring (Python for implementation, Prometheus/Grafana standard for metrics/alerting). Crystal clear with specific requirements (one metric, computation, alerts), well-scoped for screening. Tests applied design knowledge and trade-offs (e.g., metric choice, real-time computation), strongly differentiates candidates with production ML experience. Minor deduction for no explicit time constraint, but excellent overall.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-406",
        "role_title": "Senior Backend Engineer - LLM Serving",
        "question": "*Design a batching strategy for a vLLM-based LLM serving system that handles both concurrent and sequential requests, with the goal of maximizing throughput while minimizing latency for interactive users. Explain how you would balance batching across different request types (e.g., chat, search, tool calls), and how you would handle dynamic load and request patterns. Provide a concrete example of how you would adjust batching size based on real-time metrics.*",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core role responsibilities (batching optimization in LLM serving, specifically vLLM), no red flags. Clarity: Well-scoped design question with clear goals, request types, and concrete example request; minor deduction for lacking explicit time constraint. Discriminative: Tests deep applied knowledge of trade-offs (throughput vs latency), dynamic adaptation, and real-time metrics, strongly separating strong candidates.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-407",
        "role_title": "Staff Backend Engineer - ML Infrastructure",
        "question": "\"Explain how you would design a distributed training system using Ray to scale up ML model training across multiple GPUs, while ensuring fault tolerance and efficient resource utilization. Include specific considerations for data parallelism, model checkpointing, and handling node failures.\"",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core ML infrastructure skills (distributed training, scaling, fault tolerance, optimization) for lead role; Ray is relevant for modern ML infra though not explicitly listed (-1). Clarity: Precise, well-scoped with specific considerations, no ambiguity. Discriminative: Requires deep applied knowledge of trade-offs in parallelism, checkpointing, and failure handling, strongly separates experts.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-408",
        "role_title": "Junior Backend Developer - Health Endpoints",
        "question": "\"Design a Go health check endpoint that returns a 200 OK status when the service is ready to serve traffic and a 503 Service Unavailable status when it's not. The endpoint should be configurable via environment variables for the health check path and timeout. Also, self-verify the health status by checking that the database connection is alive (using a simple ping query). If the database connection fails, return 503. If the database connection succeeds, return 200. The endpoint must be accessible via a Kubernetes liveness probe.\"",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role (Go health checks, liveness/readiness, DB ping). Clarity: Precise spec, minor ambiguity on exact env var names/env parsing but well-scoped. Discriminative: Tests endpoint impl, config, DB integration, K8s awareness\u2014strong separation of applied Go skills vs rote knowledge.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-409",
        "role_title": "Junior Backend Engineer - File Upload",
        "question": "How would you design a FastAPI endpoint to handle file uploads with support for large files (e.g., >100MB) using S3, including proper validation and error handling?",
        "scores": {
          "relevance": 9,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (file uploads, multipart/large files, validation, S3 storage) and exact tech (FastAPI); minor ding as entry-level might expect simpler scope (-1). Clarity: Clear intent and well-scoped for screening, but lacks explicit constraints like time or response format (-1, -1 red flag). Discriminative: Excellent at probing applied knowledge, trade-offs (e.g., streaming, multipart, S3 presign), separates those with practical understanding from theorists.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-410",
        "role_title": "Backend Engineer - Bulk Operations",
        "question": "Design and implement a Go API endpoint that accepts a batch of user IDs to be deleted from a PostgreSQL database. The endpoint should support:\n1. Idempotency via a unique request ID (e.g., UUID).\n2. Partial success handling \u2014 if some user IDs fail to delete, the response must indicate which ones failed.\n3. Asynchronous processing using Kafka for offloading the deletion operation.\n4. Use Redis to store the request status (e.g., \"pending\", \"success\", \"failed\") and allow for status checks via a /status/{request_id} endpoint.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Relevance: Directly tests core role skills (bulk ops, partial success, idempotency) with Go, PostgreSQL, Kafka, Redis perfectly aligned; minor ding for not explicitly mentioning 'batch processing' but covers it. Clarity: Crystal clear, well-scoped requirements with specific tech and endpoints, no ambiguity. Discriminative: Excellent test of trade-offs (async, partial success, idempotency), integration skills, error handling\u2014strong separation of strong vs weak candidates.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-411",
        "role_title": "Backend Engineer - Admin APIs",
        "question": "*Design and implement a FastAPI endpoint that allows an admin to retrieve a list of all users with their roles and last login timestamp. The endpoint must:*\n- Require admin authentication (using a JWT token with the `admin:read` scope)\n- Return a paginated response with 20 items per page\n- Include audit logging of each request (timestamp, user ID, IP address, and request method)\n- Be secured with rate limiting (10 requests per minute per user)\n- Use PostgreSQL to store user data and Redis to store audit log entries (with TTL of 1 hour)",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core admin API skills, JWT auth, audit logging, and role-specific tech (FastAPI, PostgreSQL, Redis) from description (-0 red flags). Clarity: Crystal clear with precise requirements and constraints, well-scoped for screening. Discriminative: Requires integrating auth, pagination, logging, rate limiting, and DB/caching with trade-offs (e.g., Redis TTL), strongly separates skilled candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-412",
        "role_title": "Backend Engineer - Analytics APIs",
        "question": "\"Design a Go-based API endpoint to serve time-series data from ClickHouse. The endpoint should support querying data by date range, with optional filtering on a categorical dimension (e.g., 'region'). The results should be cached using Redis with a TTL of 5 minutes. Implement the caching logic to avoid redundant queries to ClickHouse, and ensure the cache key is structured to support efficient key reuse across different query parameters. Provide a brief explanation of your design choices, including how you handle cache misses and cache evictions.\"",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (time-series, aggregation queries, caching) with Go, ClickHouse, Redis perfectly matching analytics APIs (-0 red flags). Clarity: Crystal clear, well-scoped with specific constraints, no ambiguity. Discriminative: Requires applied knowledge of caching trade-offs, key design, Go/ClickHouse/Redis integration, and explanations, strongly separates candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-413",
        "role_title": "Senior Backend Engineer - API Gateway Logic",
        "question": "How would you design a Lua script in Envoy to transform a request's header, such as `X-Custom-User-ID`, into a dynamically computed value using a service call to a backend microservice (e.g., a user service), and ensure the transformation is idempotent and handles retries in case of failure? Explain your approach, including error handling, retry logic, and how you'd ensure the transformation is idempotent.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Relevance: Directly tests core API gateway extension skills (Lua scripting in Envoy for request transformation), matching role's focus on custom logic, routing, and plugins (-0 red flags). Clarity: Crystal clear, well-scoped with specific example, requirements for idempotency/retries, no ambiguities or bundles. Discriminative: Excellent at separating candidates via trade-offs in async service calls, retries, idempotency (e.g., caching), and error handling in Envoy's constrained Lua env\u2014requires deep applied knowledge.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-414",
        "role_title": "Senior Backend Engineer - Long-Polling",
        "question": "Design a long-polling endpoint in Go that handles a client's request for a status update of an async operation. The endpoint should:\n1. Accept a `requestId` parameter in the query string.\n2. Wait for a response from a background service (e.g., a worker) that publishes updates to a Redis channel.\n3. If no update is available within a configurable timeout (e.g., 30 seconds), the connection should remain open and the client should receive a `200 OK` response with a `Content-Type: application/json` header and the message `{\"status\": \"pending\"}`.\n4. If a message is received from Redis, the response should include the message content with a `Content-Type: application/json` header and a `Connection: close` header to close the connection.\n5. The endpoint must handle connection errors (e.g., client disconnects, network issues) gracefully and return a `503 Service Unavailable` response if the connection is lost.\n6. The timeout should be configurable via a struct field (e.g., `Timeout time.Duration`).\n7. The endpoint should be implemented as a `http.HandlerFunc`.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (long-polling APIs, connection mgmt, timeout handling, Redis for async ops) in Go; no red flags. Clarity: Precise specs, unambiguous requirements, well-scoped with constraints; minor nit on response 5 but clear overall. Discriminative: Requires applied knowledge of Go http, Redis pub/sub, timeouts, error handling, and trade-offs (e.g., graceful disconnects), strongly separates candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-415",
        "role_title": "Senior Backend Engineer - Multipart APIs",
        "question": "\"Design a system to handle resumable uploads for large files using FastAPI, S3, and Celery. Specifically, how would you manage file chunks, store their metadata, and ensure that a user can resume an interrupted upload from where they left off? Include considerations for concurrency, data consistency, and error handling.\"",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core focus on resumable uploads, chunk management, and resumption using exact tech (FastAPI, S3 implied, Celery fits async). No red flags. Clarity: Crystal clear intent and scope for screening, minor deduction for lacking strict time/format constraints. Discriminative: Tests deep applied knowledge, trade-offs (concurrency/consistency), no major red flags though slightly broad.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-416",
        "role_title": "Staff Backend Engineer - External APIs",
        "question": "\"Design a Go service that exposes a health check endpoint (`/health`) and a webhook endpoint (`/webhook`) using OpenAPI, Kong, and Kubernetes. The health check must return a 200 status code if the service is ready, and a 503 if it's not. The webhook endpoint must validate the request signature using a shared secret, and only accept POST requests. Describe how you would structure the service, implement the health check, implement the webhook, and deploy it in Kubernetes with Kong as the API gateway. Include how you would secure the webhook using a shared secret and ensure the service is resilient to failures.\"",
        "scores": {
          "relevance": 3,
          "clarity": 7,
          "discriminative": 7,
          "reasoning": "Relevance: Superficially API-related but mandates unmentioned tech (Go, OpenAPI, Kong, K8s; -2 each for extras, -3 generic), ignores core API strategy/monetization/partner skills. Clarity: Clear intent and well-scoped, but bundles multiple elements (design+implement+deploy; -2). Discriminative: Tests applied infra knowledge with trade-offs, but too broad (>10min; -2).",
          "composite": 5.666666666666667
        }
      },
      {
        "role_id": "be-417",
        "role_title": "Junior Backend Developer - Event Sourcing",
        "question": "\"Describe how you would design a simple event handler in a Kafka-based event sourcing system using Java, where the event is a user signup event and the aggregate state is stored in PostgreSQL. Include how you would handle event replay and ensure state consistency during system restarts.\"",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core event sourcing skills (event handlers, aggregate state) for the role; Kafka and PostgreSQL are reasonable extensions despite not being explicitly listed (-1). Clarity: Well-scoped with specific event, tech, and requirements like replay/consistency. Discriminative: Requires applied knowledge of event handling, replay, and consistency trade-offs, distinguishing strong candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-418",
        "role_title": "Junior Backend Engineer - Service Clients",
        "question": "\"Design a gRPC client in Go that handles connection failures and retries with exponential backoff. Include a configuration for retry attempts and backoff duration. Implement the retry logic in a way that respects the gRPC deadline and avoids infinite loops. Provide a minimal working example.\"",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (retries, error handling) with gRPC/Go, though Go not explicitly listed (-1). Clarity: Crystal clear, well-scoped with specific requirements and constraints. Discriminative: Requires applied knowledge of retries, backoff, deadlines, and implementation, distinguishing strong entry-level candidates via trade-offs and working code.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-419",
        "role_title": "Backend Engineer - Retry Logic",
        "question": "*Design a retry mechanism for a distributed system using Go, Redis, and PostgreSQL. The system must handle exponential backoff with jitter, respect a retry budget (e.g., max 3 retries), and avoid infinite loops. Explain how you would store retry state, manage backoff, and ensure the system does't retry indefinitely. Include considerations for distributed environments and failure modes. Answer in 2\u20135 minutes.*",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core retry infrastructure skills (backoff, jitter, budgets) with role-specific distributed focus; uses relevant tech (Go, Redis, PostgreSQL for state); minor ding for not explicitly listing all in role desc (-1). Clarity: Crystal clear, well-scoped with time limit, specific requirements, no bundling or ambiguity. Discriminative: Requires applied design knowledge, trade-offs (storage, distributed consistency, failure modes), strong separation of deep vs shallow thinkers; time-boxed avoids over-broadth.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-420",
        "role_title": "Backend Engineer - Graceful Shutdown",
        "question": "How would you implement a graceful shutdown for a Go gRPC server that is running in a Kubernetes environment, ensuring that in-flight requests are completed before terminating the process, while handling termination signals from Kubernetes (like SIGTERM) and avoiding data loss during shutdown?",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core focus on graceful shutdown, connection draining, in-flight requests, and signals in Go/gRPC/K8s context. Clarity: Crystal clear and well-scoped for screening, minor deduction for lacking explicit time/format constraints. Discriminative: Tests deep applied knowledge of signals, gRPC lifecycle, K8s integration, and trade-offs like drain timeouts, strongly separates candidates.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-421",
        "role_title": "Senior Backend Engineer - Security Automation",
        "question": "\"Explain how you would design a security automation pipeline to automatically detect, report, and remediate outdated or insecure secrets in AWS environments using Python, HashiCorp Vault, and Kubernetes. Include details on how you\u2019d integrate with AWS services, how secrets would be rotated, and how you\u2019d ensure compliance with common security standards like NIST 800-53.\"",
        "scores": {
          "relevance": 9,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core role skills (secrets rotation, AWS, automation pipelines) with exact tech like Python/Vault; minor ding for not mentioning vulnerability scanning/compliance checks explicitly (-1). Clarity: Clear intent and scope, but lacks explicit constraints like time complexity or scale (-2). Discriminative: Excellent at testing applied design, integrations, trade-offs, and standards knowledge to separate strong security automation experts.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-422",
        "role_title": "Junior Backend Engineer - API Testing",
        "question": "Write a pytest test for a FastAPI endpoint that returns a list of users. The endpoint should return a 200 OK status and a JSON response with a list of user objects. Each user object should have the fields `id`, `name`, and `name`, and the response should include a `total` field indicating the number of users. Your test should use pytest fixtures to set up a test database with one user, and use `httpx` to make a request to the endpoint. The test should verify that the response status code is 200, the response contains the expected fields, and the `total` field matches the number of users in the database.",
        "scores": {
          "relevance": 7,
          "clarity": 6,
          "discriminative": 8,
          "reasoning": "Relevance: Tests pytest fixtures, API testing with FastAPI/httpx directly relevant to role, but minor mismatch on role's emphasis on contract testing/automation patterns and duplicate 'name' field (-1 generic API test). No red flags. Clarity: Mostly clear but vague on DB setup details, fixture scope, exact expected response structure (duplicate 'name', total field ambiguity), missing constraints (-2 for missing constraints, -1 ambiguous jargon). Discriminative: Strong applied knowledge of fixtures, HTTP testing, assertions with trade-offs in DB mocking/verification.",
          "composite": 7.0
        }
      },
      {
        "role_id": "be-423",
        "role_title": "Staff Engineer - Incident Response Platform",
        "question": "\"Describe your experience with building or operating incident response systems. Specifically, how have you designed or implemented alerting logic, runbook workflows, or post-mortem processes using Go and Kubernetes? Please share a real example where you improved reliability through on-call optimization or SLO tracking.\"",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Directly targets core role skills (incident response, alerting, runbooks, post-mortems, SLOs, on-call) with specified tech (Go, Kubernetes). Clear structure with specific focus areas and real example request. Strong differentiation via experience depth and trade-offs in reliability improvements; minor -1 relevance for assuming unlisted Go/K8s but aligns with tooling context.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-424",
        "role_title": "Backend Engineer - Message Serialization",
        "question": "*Design a message serialization pipeline for a distributed system using Kafka and Avro with schema evolution support. The system must handle backward and forward compatibility, and include schema validation at runtime. Describe how you would structure the schema registry integration, message handling, and error recovery in case of schema mismatches. Be specific about how you\u2019d ensure schema evolution is safe and how you\u2019d validate messages at runtime.*",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core focus on message serialization, schema evolution, backward/forward compatibility, and validation using exact tech (Kafka/Avro). No red flags. Clarity: Crystal clear and well-scoped for screening; minor deduction as 'design a pipeline' could imply broader system but constraints are explicit. Discriminative: Tests deep applied knowledge of trade-offs in schema evolution, registry integration, validation, and error recovery\u2014strong separation of strong vs weak candidates.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-425",
        "role_title": "Senior Backend Engineer - Graph Analytics",
        "question": "Design and optimize a graph analytics pipeline to compute personalized recommendations using Neo4j and Apache Spark. The pipeline should handle a billion-edge social network graph and must support real-time updates and batch processing. Provide a high-level architecture, explain how you would optimize query performance, and describe how you would integrate real-time user interactions (e.g., likes, follows) into the recommendation engine. Specifically, how would you ensure the recommendations remain accurate and up-to-date while minimizing latency and resource usage?",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's graph analytics, billion-edge scale, social networks, and recommendations; uses role-relevant Neo4j/Spark. Clarity: Well-scoped high-level design with specific requirements, minor vagueness on exact constraints. Discriminative: Tests deep applied knowledge of optimization, trade-offs in real-time/batch, latency/resources, strongly separates strong candidates.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-426",
        "role_title": "Junior Backend Engineer - DevOps Scripting",
        "question": "Write a GitHub Actions workflow that deploys a Python application to a production environment using Docker. The workflow should:\n- Use a Python 3.11 Docker image as the base\n- Install dependencies from a requirements.txt file\n- Build and push the Docker image to a private Docker registry (e.g., AWS ECR)\n- Deploy the image to a production Kubernetes cluster using `kubectl`\n- Include proper environment variables (e.g., DB_URL, API_KEY) from secrets\n- Add a step to verify the deployment is successful by checking the service is reachable",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core DevOps automation skills (GitHub Actions CI/CD, Docker, Kubernetes deployment) for entry-level role; minor deduction as ECR is an example, not strictly required (-1). Clarity: Precise requirements with specific tech, constraints, and scope; no bundling or ambiguity. Discriminative: Requires integrating multiple tools with best practices (secrets, verification), testing applied knowledge and trade-offs like security/staging, distinguishing strong candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-427",
        "role_title": "Backend Engineer - Feature Store",
        "question": "\"Design a feature that tracks user session duration in a real-time ML pipeline using Feast. The session is defined as a continuous period of user activity (e.g., page views or clicks) lasting more than 5 minutes. You must handle real-time feature computation, versioning, and serving using Feast. Explain how you would structure the feature, including the data sources, feature service configuration, and how you'd ensure data consistency and correctness across different user sessions. Be specific about how you'd handle edge cases like session boundaries, overlapping sessions, and user inactivity.\"",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core role skills (feature computation, versioning, real-time serving) using Feast, matching role description perfectly (no red flags). Clarity: Well-scoped with specific definition, requirements, and edge cases; minor deduction for lacking explicit time/format constraints. Discriminative: Requires applied knowledge of Feast, real-time pipelines, trade-offs in session logic/edge cases, strongly separating strong candidates.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-428",
        "role_title": "Senior Backend Engineer - Payment Orchestration",
        "question": "*Design a system to route payment transactions between multiple providers (e.g., Stripe, PayPal, Adyen) using Go, PostgreSQL, and Kafka. The system must support failover during provider outages and track success/fail rate per provider. Describe how you would structure the service, including data models, message flow, and failover logic. Be specific about how you would handle provider outages and how you would store and monitor success/fail rates.*",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core needs (routing, failover, success tracking) with exact tech stack (Go, PostgreSQL, Kafka). Clarity: Crystal clear and well-scoped for screening, though minor deduction for no time/format constraints. Discriminative: Tests deep applied knowledge, trade-offs in failover/message flow/data models, strongly separates strong candidates.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-429",
        "role_title": "Backend Engineer - Log Aggregation",
        "question": "Explain how you would design a log aggregation pipeline using Go, Kafka, and ClickHouse to handle 10 million events per second, including data ingestion, parsing, enrichment, and retention policies. Specifically, describe how you would ensure data consistency and handle high throughput with minimal latency.",
        "scores": {
          "relevance": 10,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Directly matches role's core focus on log aggregation pipelines at massive scale, using relevant tech (Go, Kafka, ClickHouse align perfectly with ingestion/parsing/enrichment/retention). No red flags.\nClarity: Well-formed and unambiguous, specifies key components and concerns, but lacks explicit constraints like time budget or implementation depth (-2 for 'design a system' style).\nDiscriminative: Excellent at probing trade-offs in consistency, throughput, latency using specific stack; separates architects from implementers (not pure recall or trivial).",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-430",
        "role_title": "Junior Backend Engineer - Search Indexing",
        "question": "Imagine you're building a search index for a product catalog with 100,000 products. Each product has a name, description, and price. You want to index these products into Elasticsearch using bulk indexing.",
        "scores": {
          "relevance": 7,
          "clarity": 6,
          "discriminative": 5,
          "reasoning": "Relevance: Tests bulk indexing in Elasticsearch, a core skill for the role, but superficially mentions the stack without specifics on mapping design or relevance tuning (-1 generic setup). Clarity: Scenario clear but vague on expected answer (e.g., code? steps? mapping? trade-offs?), missing constraints (-2). Discriminative: Basic setup tests minimal knowledge but lacks depth for trade-offs or applied understanding (weak separation).",
          "composite": 6.0
        }
      },
      {
        "role_id": "be-431",
        "role_title": "Staff Engineer - Real-time Collaboration",
        "question": "Design a system to handle real-time collaborative editing of a document where two users are editing the same paragraph simultaneously. The system must resolve conflicts when both users make changes to the same paragraph at the same time. Describe how you would implement conflict resolution using CRDTs, including the specific CRDT type you would use, how it would be applied to the document structure, and how you would ensure that both users see a consistent and correct version of the document after the conflict is resolved.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (real-time collaboration, conflict resolution) using CRDTs, perfectly matching OT/CRDT focus; minor ding as CRDTs not explicitly named but implied. Clarity: Precise scope to paragraph editing with CRDTs, specific asks (type, application, consistency), well-constrained for screening. Discriminative: Probes deep applied knowledge of CRDT selection (e.g., LSEQ/Woot), document integration, and consistency guarantees, distinguishing experts via trade-offs.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-432",
        "role_title": "Backend Engineer - Data Validation",
        "question": "\"Design a Great Expectations suite to validate a daily data pipeline that ingests CSV files from S3 into a PostgreSQL table. The CSV files are generated by a legacy system and vary in schema. Your goal is to ensure the data is clean and consistent before it is loaded.",
        "scores": {
          "relevance": 9,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests data validation frameworks (Great Expectations), schema validation for varying schemas, and pipeline quality\u2014core to role; minor ding for assuming Great Expectations familiarity (-1). Clarity: Clear scenario and goal, but lacks explicit constraints on suite scope or expectations detail (-2). Discriminative: Requires applied knowledge of Great Expectations for schema checks, statistical validation, and pipeline handling; tests trade-offs in varying schemas, strong separation.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-433",
        "role_title": "Senior Backend Engineer - Platform APIs",
        "question": "\"Design a gRPC service for a service catalog that allows developers to list and search available services, with filtering by category and tags. The service must support pagination and rate limiting. Describe how you would structure the service, including the gRPC service definition, the Go implementation, and how you would implement rate limiting. Also, describe how you would ensure the service remains performant under load and how you would validate data consistency between the catalog and the actual services it describes.\"",
        "scores": {
          "relevance": 9,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (service catalog APIs, developer tooling) with gRPC and Go (common for internal platforms); minor ding as role doesn't explicitly list gRPC/Go but fits perfectly (-1). Clarity: Well-scoped with specific features, but bundles multiple aspects (design, impl, perf, consistency) without time/format constraints (-2). Discriminative: Excellent at probing trade-offs in gRPC/Go design, rate limiting, perf, and consistency\u2014strong separation of strong vs weak candidates.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-434",
        "role_title": "Principal Engineer - Database Strategy",
        "question": "\"Design a database strategy for a multi-region deployment using PostgreSQL and CockroachDB, with the goal of achieving 99.99% availability and sub-100ms latency for critical transactions. Consider trade-offs in consistency, availability, and durability, and explain how you would balance these using a hybrid approach. Include specific architectural decisions (e.g., region placement, replication model, failover mechanism) and data movement strategies (e.g., when to sync, how to handle conflicts).\"",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Directly targets core role responsibilities (multi-region DB strategy, tech selection). Crystal clear with precise goals/constraints; minor vagueness on 'hybrid approach' but well-scoped. Tests deep trade-offs, architecture, and applied expertise, strongly separating strong candidates.",
          "composite": 9.666666666666666
        }
      },
      {
        "role_id": "be-435",
        "role_title": "Junior Backend Engineer - Queue Consumers",
        "question": "Explain how you would design a consumer for a RabbitMQ queue that handles messages with idempotency, message acknowledgment, and dead-letter handling. Include how you'd handle the case where a message is processed successfully but the acknowledgment is not sent due to a network failure. What steps would you take to ensure the message is not processed again?",
        "scores": {
          "relevance": 10,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core skills (idempotency, ack patterns, dead letter handling) for RabbitMQ message consumers, no red flags. Clarity: Clear and well-scoped for screening but lacks explicit constraints like code format or depth (-2). Discriminative: Tests deep applied knowledge, trade-offs (e.g., network failure recovery), and ensures idempotency reasoning, strongly separates candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-436",
        "role_title": "Backend Engineer - API Documentation",
        "question": "Can you walk me through how you'd design a system that automatically generates OpenAPI specs for a Node.js/TypeScript backend with PostgreSQL, including how you'd handle versioning, schema validation, and example generation?",
        "scores": {
          "relevance": 9,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core role skills (auto-generated specs, versioning, example generation) with exact tech stack (Node.js/TS, PostgreSQL implied via backend); minor ding for adding schema validation. Clarity: Clear intent and scope, but lacks explicit constraints like time/format (-1) or full system design limits (-1). Discriminative: Tests applied design with trade-offs (versioning, validation, examples), strongly separates strong candidates who can reason architecturally.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-437",
        "role_title": "Senior Backend Engineer - Secrets Management",
        "question": "\"Design a secure and automated solution to rotate AWS KMS keys used by a Kubernetes cluster to encrypt secrets at rest in PostgreSQL. Your solution must handle rotation without downtime, ensure all access to secrets is auditable, and maintain compliance with least privilege. Describe how you would implement this with HashiCorp Vault as the central secrets management layer, and how you would integrate it with Kubernetes and AWS KMS. Include specific details on how you would manage key rotation, access policies, audit logging, and how you would verify that no secrets are leaked during rotation.\"",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (secrets rotation, encryption-at-rest, access policies, audit logging) with exact tech (AWS KMS, Kubernetes, PostgreSQL implied, Vault added as central layer without penalty since role mentions HashiCorp implicitly via infra; minor -1 for not explicitly listing PostgreSQL but spot-on otherwise). Clarity: Crystal clear, well-scoped with specific requirements (no downtime, auditable, least privilege), no ambiguity or bundling. Discriminative: Tests deep applied knowledge, trade-offs (zero-downtime rotation, leak prevention), integration specifics\u2014strong separation of strong vs weak candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-438",
        "role_title": "Staff Engineer - ML Serving Infrastructure",
        "question": "*You're designing a real-time inference system using Triton Inference Server with a model that has a high latency baseline of 100ms. You've observed that during peak load, the average latency increases to 300ms, and the 99th percentile latency exceeds 500ms. What specific configuration changes in Triton would you implement to reduce both the average and 99th percentile latencies, and how would you validate these changes?*",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core ML serving skills (latency optimization, real-time inference) with Triton, matching role focus; minor ding as Triton not explicitly listed (-1). Clarity: Crystal clear scenario, specific metrics, well-scoped for configs and validation. Discriminative: Requires deep applied knowledge of Triton configs (e.g., batching, scheduling, instance groups), trade-offs, and testing methodology to separate strong candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-439",
        "role_title": "Backend Engineer - Billing Integration",
        "question": "How would you design a system to handle proration for a subscription upgrade/downgrade in a billing system using Stripe and PostgreSQL?",
        "scores": {
          "relevance": 9,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (proration, subscriptions) and mentions Stripe/PostgreSQL from context (-0 red flags). Clarity: Clear intent and well-scoped for screening, but lacks explicit constraints like time complexity or edge cases (-2 for missing constraints). Discriminative: Probes applied knowledge, trade-offs (e.g., precision, concurrency, idempotency), distinguishes strong candidates who understand billing nuances.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-440",
        "role_title": "Junior Backend Engineer - Metrics Collection",
        "question": "\"Explain how you would instrument a simple Go HTTP handler to expose a Prometheus metric using a counter for request count and a gauge for request duration. Include how you would expose these metrics via a HTTP endpoint and how you would set up a Prometheus scrape job to collect them.\"",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Directly tests core instrumentation skills (counter/gauge) with Prometheus in Go for entry-level role; clear, well-scoped with specific patterns and endpoint setup; tests applied knowledge and trade-offs (e.g., histogram vs gauge for duration), distinguishing candidates who understand metrics exposure from those who don't. Minor ding for assuming Go without role spec, but highly aligned.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-441",
        "role_title": "Senior Backend Engineer - Autocomplete Systems",
        "question": "Design a scalable and low-latency autocomplete system that supports both real-time personalization (e.g., based on user session) and prefix matching across a large dataset (e.g., 1M+ entries). How would you structure the data flow, indexing strategy, and caching layer to achieve sub-50ms latency at 100K QPS? Be specific about how you'd handle personalization, and explain trade-offs in scalability, latency, and accuracy.",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (autocomplete, prefix matching, personalization, sub-50ms@100K QPS) with exact metrics; no red flags. Clarity: Crystal clear, specific scope, well-formed, though minor deduction for lacking explicit time constraint. Discriminative: Excellent at probing trade-offs in data flow/indexing/caching/personalization, separating strong systems thinkers from weak ones; appropriately scoped for screening.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-442",
        "role_title": "Backend Engineer - Event Streaming",
        "question": "\"Design a system to fan out real-time updates from a PostgreSQL database to multiple subscribers via NATS and WebSocket. The system must support filtering (e.g., only send updates for 'order_status' = 'shipped') and replay capability. Explain how you would structure the event pipeline, including data flow, message format, and how filtering and replay are implemented.\"",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core needs (event streaming, fan-out, filtering, replay) using relevant tech (PostgreSQL, NATS, WebSocket). Clarity: Crystal clear with specific requirements and examples, minor ding for lacking time/format constraints. Discriminative: Requires applied architecture knowledge, trade-offs in pipelines/filtering/replay, strongly separates strong candidates.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-443",
        "role_title": "Senior Backend Engineer - Data Lake Architecture",
        "question": "*You're designing a data lake pipeline that ingests daily transactional data from S3 into an Iceberg table using Spark. The data is structured with a partition key of `year` and `month`, and a schema that will evolve over time. You want to ensure the pipeline supports schema evolution, efficient partition pruning, and time travel queries. How would you design the Spark job to handle these requirements, and what specific configuration and code changes would you make to enable schema evolution and time travel?*",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (Iceberg tables, schema evolution, partition pruning, time travel) using exact tech (Spark, S3). No red flags.\nClarity: Well-scoped with specific context (daily ingest, partition keys, requirements); minor deduction for lacking explicit time/format constraints.\nDiscriminative: Requires applied knowledge of Spark/Iceberg configs (e.g., write.merge.schema, snapshot management), trade-offs in partitioning/schema handling; distinguishes strong big data engineers.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-444",
        "role_title": "Backend Engineer - Service Templates",
        "question": "\"Imagine you're building a service template using Cookiecutter to scaffold a new Go service. The template should include a `go.mod` file with a proper module path, a `main.go` with a minimal working example, and a `Dockerfile` that builds a secure container image. Additionally, the template should include a GitHub Actions workflow that runs tests and lints the code before allowing a PR to be merged. How would you structure the template to ensure that the generated service follows modern Go practices, includes basic security hardening, and provides a clear developer experience?\"",
        "scores": {
          "relevance": 9,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (service scaffolding with Cookiecutter, Go, CI/CD via GitHub Actions, security defaults, observability-adjacent via best practices); minor ding for assuming Cookiecutter without role spec but fits scaffolding perfectly (-1). Clarity: Well-scoped with specific files/components, clear intent, but lacks explicit constraints on answer format/depth (-1 for 8). Discriminative: Requires applied knowledge of Go modules/Docker security/CI workflows/trade-offs in templating, strongly separates candidates with practical experience.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-445",
        "role_title": "Junior Backend Engineer - Database Migrations",
        "question": "How would you write an Alembic migration that adds a new column to an existing table in PostgreSQL, ensuring that the migration is safe and can be rolled back?",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Directly tests Alembic migrations (core to role) with PostgreSQL context, zero-downtime safety, and rollback (explicit role skills). Crystal clear scope for entry-level, no ambiguity or bundling. Requires applied knowledge of upgrade/downgrade functions and safe practices (e.g., defaults, backfill), distinguishing candidates who understand beyond basic syntax.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-446",
        "role_title": "Staff Engineer - Distributed Tracing",
        "question": "How would you design a sampling strategy for distributed traces in a high-throughput Kafka-based microservices architecture where you need to maintain trace visibility without overwhelming storage and processing resources? Include specific considerations for traffic patterns, service topology, and telemetry volume.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core distributed tracing sampling (key role skill), Kafka/microservices match high-throughput distributed context; minor deduction as role doesn't explicitly list Kafka (-1). Clarity: Crystal clear, well-scoped with specific considerations, no ambiguity or missing constraints. Discriminative: Probes deep applied knowledge of trade-offs (visibility vs resources), traffic/service factors; distinguishes strong architects via reasoning on patterns/topology.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-447",
        "role_title": "Backend Engineer - Load Shedding",
        "question": "\"Design a load shedding mechanism in Go that uses Redis to track request priority and enforce adaptive throttling based on system backpressure signals. The mechanism should support priority-based queuing, allow dynamic adjustment of throttling thresholds, and respond to backpressure signals from Prometheus metrics. When backpressure is detected, prioritize lower-priority requests over high-priority ones, and gradually reduce throughput for high-priority requests. Explain your approach, including how you would structure the Go code, manage state, and integrate with Prometheus for real-time monitoring.\"",
        "scores": {
          "relevance": 9,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (load shedding, priority queues, adaptive throttling, backpressure) and specifies Go/Redis/Prometheus from stack (-1 for minor inaccuracy in prioritization logic, which is likely a typo). Clarity: Clear and well-scoped but lacks explicit time/format constraints for screening (-2). Discriminative: Excellent at testing applied design, trade-offs, integration, and reasoning; distinguishes strong candidates via depth of explanation.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-448",
        "role_title": "Senior Backend Engineer - Webhook Infrastructure",
        "question": "\"Design a webhook delivery system in Go using Kafka for message queuing and PostgreSQL/Redis for persistence and state tracking. Your design must ensure guaranteed delivery with exponential backoff retry logic, and include signature verification to prevent spoofing. Include a clear explanation of how the system ensures message delivery, handles failures, and tracks delivery status. Also, describe how you would monitor and troubleshoot delivery failures.\"",
        "scores": {
          "relevance": 9,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core webhook delivery skills (guaranteed delivery, retries, signatures, analytics) with relevant tech (Go, Kafka, PostgreSQL/Redis matching queuing/persistence needs); minor ding for specificity beyond role's high-level stack (-1). Clarity: Clear intent and scope but lacks explicit constraints like time or diagram format (-2). Discriminative: Excellent at probing trade-offs, failure handling, and deep system design, strongly separating strong candidates.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-449",
        "role_title": "Junior Backend Engineer - Auth Integration",
        "question": "\"Imagine you're implementing a GitHub OAuth2 login flow for a web app. The OAuth2 provider will redirect the user back to your app with a code. Your app will exchange that code for an access token and refresh token. The access token has a 10-minute expiry, and the refresh token has a 1-year expiry. You want to store the tokens securely in your database.",
        "scores": {
          "relevance": 9,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests OAuth2 flows, token handling, refresh logic, and secure storage\u2014core to role; minor ding for GitHub specificity not in desc (-1). Clarity: Clear scenario and flow, but lacks explicit constraints on storage method or response format (-1, minor jargon assumption). Discriminative: Excellent for probing trade-offs in token expiry, refresh implementation, security (e.g., hashing, HTTPS), distinguishing practical knowledge from theory.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-450",
        "role_title": "Principal Engineer - ML Platform Strategy",
        "question": "\"Design a scalable, governed ML platform using Kubeflow, MLflow, and Ray. Your design must include:  \n1. A clear architecture diagram (text-based) showing data flow from ingestion to model deployment.  \n2. How you would implement versioning and lineage for models and experiments using MLflow.  \n3. How you would ensure reproducibility and auditability of experiments and deployments.  \n4. How you would manage model serving and scaling using Ray Serve, and what metrics you would monitor.  \n5. How you would integrate governance policies (e.g., data quality, model drift, access control) into the platform.  \n6. How you would onboard new ML engineers to this platform, including training and documentation.\"",
        "scores": {
          "relevance": 9,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core ML platform strategy, governance, and tooling (Kubeflow/MLflow/Ray align perfectly with role); minor ding for assuming specific unlisted tools (-1). Clarity: Well-structured but bundles 6 sub-questions and lacks time/constraints (-2). Discriminative: Excellent at probing deep applied knowledge, trade-offs, and holistic thinking for strong separation.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-451",
        "role_title": "Backend Engineer - Subscription Billing",
        "question": "*How would you handle proration when a customer changes plans during a billing cycle? For example, if a customer moves from a $10/month plan to a $20/month plan halfway through the month, how would you calculate the correct invoice amount, and what data would you need to store in your system to support this?*",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Directly tests core role skill (proration for plan changes) with precise example matching job needs; crystal clear with good scope, minor deduction for unspecified edge cases; strong separation via trade-offs in calculation logic and data modeling.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-452",
        "role_title": "Senior Backend Engineer - Faceted Search",
        "question": "What are the challenges of implementing accurate facet counts in Elasticsearch when dealing with large datasets and dynamic filters, and how would you design a solution to ensure correctness under load and with real-time filtering?",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Directly targets core role skills (faceted search, dynamic filtering, aggregation queries, facet accuracy at scale) with no red flags. Crystal clear, well-scoped for screening (challenges + design solution). Tests deep understanding of Elasticsearch trade-offs, approximations vs. accuracy, sampling, caching, and optimizations, strongly distinguishing experts from juniors.",
          "composite": 9.666666666666666
        }
      },
      {
        "role_id": "be-453",
        "role_title": "Backend Engineer - Notification Delivery",
        "question": "How would you design a system to deliver push notifications to multiple devices in real-time using Go, Redis, and FCM/APNS, ensuring reliability, scalability, and delivery tracking? Please include considerations for device registration, batching, failure handling, and monitoring.",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (device registration, batching, delivery tracking) and exact tech stack (Go, Redis, FCM/APNS). Clarity: Crystal clear with specific components listed, though slightly broad 'design a system' without time constraints. Discriminative: Probes deep applied knowledge, trade-offs in reliability/scalability/failures, distinguishing strong architects from weak ones; well-scoped for screening.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-454",
        "role_title": "Junior Backend Engineer - ETL Scripts",
        "question": "Write a Python script using pandas and PostgreSQL to extract data from a PostgreSQL table, transform it (e.g., filter, aggregate, rename), and load it into another table. Include error handling for common issues like connection failures, missing data, and invalid data types. The script should be reusable and include comments explaining key decisions.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core ETL skills (extract, transform, load) with pandas and PostgreSQL, matching role's data transformation and error handling focus; minor ding as PostgreSQL not explicitly listed but fits ETL context (-1). Clarity: Well-scoped with specific requirements, examples, and reusability; no bundling or ambiguity. Discriminative: Requires applied knowledge of ETL pipeline, error handling trade-offs, and clean code design, strongly separating capable entry-level engineers.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-455",
        "role_title": "Staff Engineer - Developer Portal",
        "question": "\"Design a plugin architecture for a developer portal using Backstage, where plugins can register their services and documentation. The plugin should allow service discovery, dynamic documentation rendering, and support for multiple service types (e.g., REST, GraphQL, gRPC). Describe how you would structure the plugin, define the registration mechanism, and ensure that documentation is dynamically rendered based on the service type. Include considerations for extensibility, error handling, and onboarding for new developers.\"",
        "scores": {
          "relevance": 9,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core plugin architecture and service discovery for Backstage developer portal (-1 for assuming Backstage in role desc without explicit mention). Clarity: Clear and well-scoped, but lacks explicit time/constraints for screening (-2). Discriminative: Excellent at probing trade-offs, extensibility, and applied design knowledge to separate strong architects from average devs.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-456",
        "role_title": "Backend Engineer - Query Optimization",
        "question": "Explain how you would detect and diagnose a slow query in PostgreSQL using `pganalyze`, and walk through a step-by-step process to identify the problematic query, analyze its execution plan, and recommend an index to improve performance. Include how you would validate the impact of the index after applying it.",
        "scores": {
          "relevance": 4,
          "clarity": 8,
          "discriminative": 7,
          "reasoning": "Relevance: Superficially mentions PostgreSQL (relevant) but mandates unlisted tool 'pganalyze' (-2), testing generic programming/query skills rather than core role focus like building custom tooling (-3 for generic). Clarity: Well-formed and unambiguous with clear step-by-step scope. Discriminative: Tests applied knowledge of query diagnosis and optimization process with trade-offs, but tool specificity reduces broad separation.",
          "composite": 6.333333333333333
        }
      },
      {
        "role_id": "be-457",
        "role_title": "Senior Backend Engineer - SLO Automation",
        "question": "\"Describe how you would design and implement a system that monitors the burn rate of an SLO and triggers an alert when the burn rate exceeds a configurable threshold. Include the data flow, the logic for calculating burn rate, and how you would ensure the system is reliable and avoids false positives.\"",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core SLO burn rate alerting and reliability, matching role's SLO management and incident focus; no red flags. Clarity: Well-scoped design question with specific elements (data flow, logic, reliability); minor deduction for lacking explicit time/format constraints. Discriminative: Tests applied system design, trade-offs (false positives, reliability), and deep understanding, strongly separating candidates.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-458",
        "role_title": "Backend Engineer - API Gateway Plugins",
        "question": "You are tasked with creating a Kong plugin that validates JWT tokens in incoming API requests. The plugin must:\n1. Extract the JWT token from the `Authorization` header (format: `Bearer <token>`).\n2. Verify the token\u2019s signature using a public RSA key stored in a Redis instance.\n3. If the token is valid, set a `x-authenticated-user` header in the response.\n4. If the Auth fails, return a 401 Unauthorized response.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core API gateway plugin development for Kong (matches role's custom auth focus), uses JWT/RS256 common in auth plugins; minor ding as Redis isn't explicitly in role but fits transformation context (-1). Clarity: Precise requirements, unambiguous steps, well-scoped for screening. Discriminative: Requires applied knowledge of Kong plugin lifecycle, JWT parsing/verification, Redis integration, error handling, and header manipulation\u2014tests trade-offs in async verification and strong separation of skilled candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-459",
        "role_title": "Junior Backend Engineer - Container Security",
        "question": "\"Explain how Trivy scans a Docker image for vulnerabilities, and describe one way you would configure Trivy to prevent scanning a base image from being used in production.\"",
        "scores": {
          "relevance": 4,
          "clarity": 7,
          "discriminative": 6,
          "reasoning": "Relevance: Superficially related to container scanning but fixates on specific unmentioned tool (Trivy) instead of core skills like vulnerability detection; generic explanation + red flag for non-listed tech (-2). Clarity: Clear intent and well-formed, but lacks scoping constraints on explanation depth. Discriminative: Tests some applied knowledge beyond recall but weak separation as it allows superficial answers; partially recall-based.",
          "composite": 5.666666666666667
        }
      },
      {
        "role_id": "be-460",
        "role_title": "Backend Engineer - Experiment Tracking",
        "question": "\"Design a Python script using MLflow to track an ML experiment that includes logging metrics, model artifacts, and a comparison of two different models. The script should use PostgreSQL as the backend storage and S3 for artifact storage. The script must include proper error handling for failed metric logging and failed artifact uploads. Include comments explaining your design decisions.\"",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests MLflow for experiment tracking, metrics, artifacts, and model comparison\u2014core to role; PostgreSQL/S3 are reasonable extensions (-1 for not explicitly listed). Clarity: Well-scoped design task with specific tech, constraints, error handling, and comments. Discriminative: Requires applied MLflow knowledge, integration, error handling, and design rationale\u2014tests trade-offs and depth effectively.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-461",
        "role_title": "Senior Backend Engineer - Fraud Detection",
        "question": "How would you design a real-time fraud detection pipeline that ingests transaction events from Kafka, applies a rule-based scoring system (e.g., high transaction value, rapid sequence of transactions), and integrates with a machine learning model to generate a fraud score? Include how you would handle model scoring latency, ensure the system is scalable, and manage the trade-off between false positives and false negatives for a payment system with high transaction volume?",
        "scores": {
          "relevance": 9,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core fraud detection pipeline skills (rules, ML integration, real-time scoring) and Kafka (relevant for event ingestion), matching role; minor ding for not mentioning manual review queues. Clarity: Well-scoped with specific components and concerns, but lacks explicit constraints like time budget or throughput targets. Discriminative: Excellent at probing trade-offs (latency, scalability, FP/FN), architecture decisions, and deep understanding, strongly separating candidates.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-462",
        "role_title": "Backend Engineer - Alert Routing",
        "question": "\"Alertmanager is configured to route alerts based on a label, such as `team=backend`, to a specific PagerDuty service. However, you observe that alerts from the `backend` team are being routed to two different PagerDuty services: one for production and one for a staging environment. Why might this be happening, and how would you fix it?\"",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core alert routing and grouping skills for the role (-0 red flags). Clarity: Crystal clear scenario with specific observation and unambiguous intent, well-scoped for screening. Discriminative: Probes deep Alertmanager understanding (labels/matchers vs match_re), trade-offs in config, and practical fixes, strongly separates candidates with real experience.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-463",
        "role_title": "Junior Backend Engineer - Full-text Search",
        "question": "How would you implement a full-text search feature in a Python Flask/FastAPI application using PostgreSQL and Redis for caching? Include specific details on how you\u2019d structure the query to handle ranking, and explain how Redis would improve performance.",
        "scores": {
          "relevance": 8,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests PostgreSQL full-text search (core role skill) with tsvector/ranking implied in query structure, but deducts for unmentioned Flask/FastAPI/Python/Redis (-2 red flag). Clarity: Crystal clear, well-scoped with specific details requested. Discriminative: Excellent at testing applied knowledge, trade-offs in ranking/caching, and integration\u2014strong separation of candidates.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-464",
        "role_title": "Staff Engineer - Presence Systems",
        "question": "Design a system to track real-time presence and typing indicators for a large-scale messaging application with 10M+ users. The system must handle 100K+ concurrent WebSocket connections and support low-latency (<100ms) updates for presence and typing states. How would you design the data model, connection management, and heartbeat protocol to meet these requirements? Include how you would handle connection churn, scalability, and fault tolerance.",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core focus on presence/typing indicators, heartbeats, connection management, state sync at scale, with exact tech elements and no red flags. Clarity: Crystal clear with precise constraints (10M users, 100K WS, <100ms), well-scoped; minor deduct for lacking explicit time/format but excellent for screening. Discriminative: Tests deep applied knowledge, trade-offs in data modeling, protocols, churn/scalability/fault tolerance, strongly separates strong architects from weak ones; not too broad for screening.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-465",
        "role_title": "Backend Engineer - CDC Pipelines",
        "question": "\"How would you design a CDC pipeline using Debezium and Kafka to replicate data from a PostgreSQL database to Snowflake, including handling schema changes, initial snapshot, and ensuring exactly-once delivery?\"",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core requirements (CDC pipelines, schema changes, snapshots, exactly-once) with exact tech stack implied for role. Clarity: Crystal clear and well-scoped for screening, minor deduction as no explicit time/format constraints. Discriminative: Tests deep applied knowledge, trade-offs (e.g., Debezium config, Kafka idempotency, Snowflake upserts), strongly separates candidates.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-466",
        "role_title": "Senior Backend Engineer - GitOps Tooling",
        "question": "\"Describe how you would implement a drift detection mechanism in a GitOps pipeline using ArgoCD and Helm, including how you would detect changes in the deployed state that are not reflected in the Git repository, and how you would trigger an alert or remediation action when such drift is detected.\"",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Directly tests core GitOps skills (drift detection) using role-relevant tools (ArgoCD, Helm); clear, well-scoped with specific implementation, detection, and action-triggering aspects; probes applied knowledge and trade-offs for strong candidate separation. Minor ding for assuming ArgoCD/Helm without role confirmation, but highly aligned.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-467",
        "role_title": "Backend Engineer - Connection Pooling",
        "question": "*\"Imagine you're implementing a connection pooling layer using PgBouncer in a Kubernetes environment. A service running in a pod needs to connect to a PostgreSQL database. The application uses a connection timeout of 30 seconds and a maximum connection pool size of 100. When the application starts, it sees a 5-second delay before establishing the first connection. After the first connection, subsequent connections are fast (under 1 second).",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 8,
          "reasoning": "Relevance: Directly tests PgBouncer connection pooling, pool sizing, Kubernetes context, and startup delays\u2014core to role's focus on connection pooling infrastructure (-1 for no explicit transaction/failover but strongly aligned). Clarity: Crystal clear scenario with specific constraints and symptoms, unambiguous for screening. Discriminative: Requires understanding of pooling mechanics, cold start delays, and PgBouncer config in K8s to diagnose\u2014tests applied knowledge and trade-offs effectively.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-468",
        "role_title": "Junior Backend Engineer - CI Pipelines",
        "question": "Write a GitHub Actions workflow that:\n- Runs on push to main and pull request\n- Uses a Python 3.11 container\n- Installs dependencies via pip\n- Runs tests using pytest\n- Caches pip dependencies using a cache key based on the requirements.txt file\n- Runs tests in parallel using the `parallel` strategy in GitHub Actions",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core CI pipeline skills (workflow syntax, caching, parallelization) for entry-level role; no red flags. Clarity: Crystal clear with specific requirements; minor deduction as no explicit time constraint but well-scoped. Discriminative: Requires applied knowledge of GitHub Actions features with trade-offs in caching/parallelism, strongly separates candidates able to implement vs recall basics.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-469",
        "role_title": "Senior Backend Engineer - Chaos Engineering",
        "question": "How would you design a Chaos Mesh experiment to simulate a pod failure in a Kubernetes cluster, and ensure that the system can recover gracefully within a defined recovery window? Specifically, describe how you would define the experiment, configure the recovery validation, and validate that the system remains stable after recovery.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests Chaos Mesh (chaos engineering tool), pod failure simulation, recovery validation - core to role's focus on failure injection, blast radius, and recovery (minor deduction as Chaos Mesh not explicitly named but perfectly fits Kubernetes chaos infra). No red flags. Clarity: Precise, well-scoped with specific components (experiment definition, config, validation); no ambiguity or bundling. Discriminative: Requires applied design knowledge, trade-offs in config/stability checks - distinguishes deep chaos eng expertise from basics.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-470",
        "role_title": "Backend Engineer - GraphQL Federation",
        "question": "\"Design and implement a schema stitching solution using Apollo Federation to compose two subgraphs: a `User` subgraph and a `Post` subgraph. The `User` subgraph should expose `id`, `name`, and `email`, while the `User` subgraph should expose `id`, `title`, and `content`. Both subgraphs must be able to resolve a `User`-`Post` relationship where a user can have multiple posts. The `User` subgraph must resolve the posts via a `posts` field that returns a list of `Post` objects. The `Post` subgraph must be able to resolve a `user` field that returns a `User` object. Implement the schema stitching logic in a gateway that seamlessly combines the two subgraphs and handle query planning for nested queries such as `user { posts { title } }`. Also, ensure that the data model is normalized and that all queries return consistent results across the subgraphs.\"",
        "scores": {
          "relevance": 9,
          "clarity": 7,
          "discriminative": 8,
          "reasoning": "Relevance: Directly tests Apollo Federation, schema stitching, entity resolution, and query planning - core to the role (-1 for minor typo but still spot-on). Clarity: Clear intent and specifics, but typo ('User' subgraph for Post fields) creates minor ambiguity; lacks explicit code format constraints (-2 bundled depth, -1 jargon context). Discriminative: Strong on applied federation knowledge, trade-offs in resolution, distinguishes experts (-2 too implementation-heavy for screening, >10min risk).",
          "composite": 8.0
        }
      },
      {
        "role_id": "be-471",
        "role_title": "Staff Engineer - Zero Trust Architecture",
        "question": "How would you design a secure, automated mTLS setup using SPIFFE and Kubernetes for a microservices architecture where services need to verify each other's identity at rest and in transit? Please describe the identity issuance process, how services authenticate to each other, and how you would ensure the trust anchor is correctly managed and rotated in production.",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core role requirements (mTLS automation, zero trust for microservices) using exact tech (SPIFFE, Kubernetes); no red flags. Clarity: Precise, well-scoped to key processes without bundling or ambiguity; minor deduction for unspecified depth but excellent for screening. Discriminative: Probes applied knowledge, trade-offs in production (rotation, trust anchor), distinguishing strong architects from weak ones; not trivial recall.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-472",
        "role_title": "Backend Engineer - Vector Search",
        "question": "Can you walk me through how you would design a system to support hybrid search using Pinecone and PostgreSQL, including how you'd handle embedding generation, index management, and retrieval? Specifically, how would you ensure consistency between the vector and text search results when returning responses to users?",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (hybrid search, embeddings, index mgmt) with exact tech (Pinecone, PostgreSQL); no red flags. Clarity: Well-scoped with specific components and consistency focus, minor deduction for lacking time/format constraints. Discriminative: Probes applied design knowledge, trade-offs (e.g., consistency strategies), and integration depth for strong candidate separation; not trivial recall.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-473",
        "role_title": "Senior Backend Engineer - Refund Processing",
        "question": "How would you design a system to handle partial refunds in a payment processing system, ensuring ledger accuracy, handling provider callbacks, and maintaining audit trails? Specifically, describe how you\u2019d manage state transitions for partial refunds (e.g., from initial charge to partial refund, then full refund), and how you\u2019d ensure that ledger reconciliation occurs correctly in the presence of asynchronous provider callbacks?",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core requirements (partial refunds, ledger reconciliation, provider callbacks, audit trails) with no red flags. Clarity: Crystal clear and well-scoped for screening, directly addresses key aspects; minor deduction for no explicit time/format constraints. Discriminative: Tests deep applied knowledge, state management, async handling, and trade-offs, strongly separating strong candidates.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-474",
        "role_title": "Backend Engineer - Dashboards API",
        "question": "*You're building a dashboard template API in Go that supports variable substitution in dashboard JSON. Users can define variables like `{{.env.PROD}}` which are resolved at runtime by the system. The system stores dashboards in PostgreSQL and caches results in Redis. How would you design the variable resolution process to ensure that during high load, variable resolution is fast, consistent, and avoids race conditions when multiple concurrent requests are processing the same dashboard?*",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (dashboard APIs, template rendering, variable substitution) with exact tech stack (Go, PostgreSQL, Redis); minor deduction for not specifying sharing. Clarity: Crystal clear, well-scoped to variable resolution under high load with explicit constraints. Discriminative: Excellent at testing applied knowledge of caching, concurrency (race conditions), consistency, and performance trade-offs in real-world observability API context.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-475",
        "role_title": "Junior Backend Engineer - Search Analytics",
        "question": "How would you design a system to log and analyze search queries for a web application, and what would be the key metrics you\u2019d track to evaluate search performance?",
        "scores": {
          "relevance": 7,
          "clarity": 6,
          "discriminative": 8,
          "reasoning": "Relevance: Directly matches role's query logging, analytics, and metrics focus but generic (no specific tech stack), -3 for generic. Clarity: Clear intent but vague scope/constraints for screening ('design a system'), -2 for no constraints, -2 for multiple bundled questions. Discriminative: Strong on trade-offs/design thinking and applied metrics knowledge, good separation.",
          "composite": 7.0
        }
      },
      {
        "role_id": "be-476",
        "role_title": "Backend Engineer - Live Updates",
        "question": "\"Design a system to handle live updates for a dashboard where users can join rooms to receive real-time data. Users should only receive updates for rooms they are in, and the system must support reconnection logic. Describe how you would structure your Socket.io and Redis implementation to ensure efficient, scalable, and reliable delivery of updates. Include considerations for handling reconnection, data consistency, and performance.\"",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Directly targets core role needs (live updates, rooms, selective broadcasting, reconnection) with exact tech stack (Socket.io, Redis). Crystal clear scope for screening, tests applied architecture/trade-offs (scalability, consistency, performance). Minor ding for assuming Socket.io/Redis familiarity without role confirmation, but aligns perfectly otherwise. Strong separation of strong vs weak candidates via design reasoning.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-477",
        "role_title": "Senior Backend Engineer - Data Catalog",
        "question": "*You're designing a metadata ingestion pipeline for Apache Atlas using Python. The pipeline reads data from a Kafka topic containing schema changes (e.g., `{\"type\": \"record\", \"name\": \"user\", \"fields\": [...]}`), extracts the schema definition, and stores it in a PostgreSQL database for later use by Atlas. However, the schema data is often malformed or incomplete (e.g., missing required fields, malformed JSON).*",
        "scores": {
          "relevance": 4,
          "clarity": 7,
          "discriminative": 8,
          "reasoning": "Relevance: Superficially mentions metadata extraction but focuses on generic Kafka/Python/PostgreSQL pipeline with Apache Atlas (not listed in role); generic data processing (-3), tech not in role (-2). Clarity: Clear scenario and intent, but lacks constraints on solution scope/format (-1). Discriminative: Strong on error handling/trade-offs in pipelines, differentiates applied skills well.",
          "composite": 6.333333333333333
        }
      },
      {
        "role_id": "be-478",
        "role_title": "Backend Engineer - Cost Attribution",
        "question": "\"Design a system in Go that tracks cloud resource usage and assigns costs to teams based on resource tagging. The system should collect metrics from Prometheus, store them in PostgreSQL, and generate daily chargeback reports. Describe how you would structure the data model, handle tag-based cost allocation, and ensure data consistency across distributed components. Use specific examples to illustrate your design decisions.\"",
        "scores": {
          "relevance": 7,
          "clarity": 6,
          "discriminative": 8,
          "reasoning": "Relevance: Tests core cost attribution, tagging, and reporting but assumes unmentioned Go/Prometheus/PostgreSQL (-2 for tech not in role). Clarity: Clear intent but 'design a system' lacks constraints like scale/time (-2). Discriminative: Strong on trade-offs, data modeling, and distributed consistency, good separation.",
          "composite": 7.0
        }
      },
      {
        "role_id": "be-479",
        "role_title": "Principal Engineer - Data Architecture",
        "question": "\"Design a data architecture for a high-traffic e-commerce platform that supports both real-time transactional workloads (OLTP) and batch analytics (OLAP) with low-latency reporting. The platform processes 10M+ transactions per day and serves 10M+ unique users. Consider the following constraints:\n- Transactional data must be immediately available for user-facing services (e.g., order placement, inventory updates).\n- Analytics queries must be fast, scalable, and support complex aggregations (e.g., sales trends, customer segmentation).\n- Data must be retained for 180 days for compliance and auditing.\n- The system must be deployed on Kubernetes with automated scaling and monitoring.\n- You must minimize data duplication and ensure data consistency across systems.",
        "scores": {
          "relevance": 8,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core data architecture strategy for OLTP/OLAP, tech selection (Kubernetes), and team guidance; minor deduction for no explicit role tech stack details (-1, generic feel -1). Clarity: Crystal clear with specific constraints, scale, and scope; well-formed for screening. Discriminative: Requires trade-offs in hybrid architecture (e.g., CDC, lakehouse), consistency, scaling\u2014strong separation of deep vs shallow knowledge.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-480",
        "role_title": "Junior Backend Engineer - Deployment Automation",
        "question": "\"Explain how you would automate the deployment of a Python application to AWS EC2 using Ansible and Docker. Include how you would handle environment-specific configurations (e.g., development vs production), manage rollbacks, and ensure security.\"",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Directly tests core role skills (deployment automation, config management, rollbacks, environment setup) with relevant tech (Ansible, Docker, AWS EC2 for Python app). Crystal clear, well-scoped with specific elements (envs, rollbacks, security). Strong separation via trade-offs in automation, security, and rollback strategies. Minor ding for assuming Ansible/Docker familiarity in entry-level, but aligns with learning path.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-481",
        "role_title": "Backend Engineer - Canary Deployments",
        "question": "\"Design a canary deployment pipeline using ArgoCD and Istio that automatically rolls back a canary if the error rate exceeds a threshold during a 5-minute evaluation window. Include how you\u2019d detect the error rate, how you\u2019d trigger the rollback, and how you\u2019d ensure the rollback happens within 1 minute of detection.\"",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Directly tests core role skills (canary deployments, automated rollback, traffic shifting, metrics) with ArgoCD/Istio implied in infrastructure. Crystal clear with precise constraints (5-min window, 1-min rollback). Strong separation via trade-offs in detection, triggering, and Istio/ArgoCD integration; minor deduction for no explicit format/depth guidance.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-482",
        "role_title": "Senior Backend Engineer - API Monetization",
        "question": "Design a system to track API usage and enforce quotas for different pricing tiers. Consider the following requirements:\n- Users can subscribe to a tier (e.g., Free, Pro, Enterprise) with different quotas (e.g., 1k, 10k, 100k requests/day)\n- Each request must be validated against the user\u2019s active tier's quota\n- Quotas must be enforced in real-time, with a 100ms latency ceiling\n- The system must handle high concurrency (10k+ requests/sec)\n- You must integrate with Stripe for billing and subscription management\n- Use Go, PostgreSQL, and Redis for storage and caching",
        "scores": {
          "relevance": 8,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (usage tracking, quotas, tiers, billing) and exact tech stack (Go, PostgreSQL, Redis, Stripe); minor deduction for generic 'design a system' feel (-1, close to red flag). Clarity: Well-scoped with specific requirements, latency, concurrency, and tech; no bundling or ambiguity. Discriminative: Requires trade-offs in real-time enforcement, high concurrency, caching (Redis), and integration, strongly separating strong architects from weak ones.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-483",
        "role_title": "Backend Engineer - WAF Integration",
        "question": "\"Describe how you would design and implement a Go service to detect and block HTTP requests that match known attack patterns using ModSecurity, with a focus on rate limiting and bot detection. Include how you would integrate this with Nginx and Redis for performance and scalability.\"",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core WAF/attack detection, rate limiting, bot detection with Go/Nginx/Redis (aligned to role; -1 for unmentioned ModSecurity). Clarity: Precise scope, unambiguous, well-formed for screening. Discriminative: Requires applied design knowledge, trade-offs in integration/scalability, strong separation of strong vs weak candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-484",
        "role_title": "Staff Engineer - Model Registry",
        "question": "\"Design a model registry workflow for a production ML system using MLflow and Kubernetes, with a focus on versioning, deployment automation, and governance. Specifically, how would you ensure that only approved models are deployed to production, and how would you track and enforce model lineage and provenance?\"",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Directly targets core role skills (model registry, versioning, deployment, governance) with exact tech (MLflow, Kubernetes). Crystal clear with specific focus areas, though minor deduction for lacking time/format constraints. Tests applied design thinking, trade-offs in approval/lineage enforcement, strongly separates strong ML architects from others. No red flags.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-485",
        "role_title": "Junior Backend Engineer - Payment Webhooks",
        "question": "\"Design a system to process incoming payment webhooks from a payment provider. The webhook payload contains a payment event (e.g., 'payment.succeeded') with a transaction ID and amount. Your system must verify the webhook signature to ensure it's from the real provider, handle idempotency to avoid processing the same event twice, and store the event for later use. Describe your approach using Python, FastAPI, PostgreSQL, and Redis.\"",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core skills (signature verification, idempotency, event processing) and entry-level webhook focus using relevant stack. Clarity: Crystal clear with specific requirements and tech stack, minor ding for no time/format constraints. Discriminative: Tests applied system design with critical trade-offs (verification, idempotency), strongly separates candidates able to reason through integration vs rote knowledge.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-486",
        "role_title": "Backend Engineer - Span Processing",
        "question": "\"How would you design a tail sampling strategy for spans in a high-throughput Kafka-based tracing pipeline using Go and OpenTelemetry, ensuring that the storage cost remains within a defined budget while maintaining observability of key error and performance metrics?\"",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Directly targets core role skills (tail sampling, pipeline, storage optimization) with exact tech stack (Kafka, Go, OpenTelemetry). Crystal clear scope for screening with budget/observability constraints. Tests deep trade-offs in design, throughput, and metrics\u2014strong separation of strong vs weak candidates. Minor ding for lacking explicit time constraint but exceptionally well-formed.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-487",
        "role_title": "Senior Backend Engineer - Search Ranking",
        "question": "\"Design a feature pipeline for a learning-to-rank model using Elasticsearch and Redis, where features are extracted from search results and stored in Redis for efficient model training. Explain how you would handle feature extraction, caching, and model training updates. Specifically, how would you ensure that the features are up-to-date and consistent across training iterations?\"",
        "scores": {
          "relevance": 8,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core learning-to-rank pipeline skills (feature engineering, model training) but deducts 2 for specifying unmentioned tech (Elasticsearch, Redis). Clarity: Well-scoped with specific components, minor vagueness on constraints deducts 2. Discriminative: Excellent at probing trade-offs in extraction, caching, updates, and consistency, distinguishing strong ML engineers.",
          "composite": 8.333333333333334
        }
      },
      {
        "role_id": "be-488",
        "role_title": "Backend Engineer - Gaming Backend",
        "question": "\"Design a system to synchronize game state between clients in a multiplayer game using Go and WebSockets. Clients connect via WebSockets and send state updates (e.g., position, health) every 100ms. The server must ensure all clients see the same state, handle client disconnects, and reject invalid state updates (e.g., health > 100). Describe how you'd implement state synchronization, handle network latency and disconnections, and validate state updates. Use Protocol Buffers to structure your data.\"",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core real-time multiplayer skills (state sync, validation akin to anti-cheat) with Go/WebSockets from role; minor ding as matchmaking not covered (-1). Clarity: Well-scoped design question with specific constraints (100ms, examples, tech); no bundling or vagueness. Discriminative: Probes deep applied knowledge of trade-offs (latency, disconnects, validation), distinguishes strong candidates able to discuss authoritative server, interpolation, etc.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-489",
        "role_title": "Backend Engineer - Data Quality",
        "question": "Explain how you would implement a freshness check for a Snowflake table using dbt and Great Expectations, including how you would define the check, where it would run, and how you would alert on failure. Use a concrete example with a sample table and relevant metrics.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core data quality monitoring (freshness checks) using exact tools (dbt, Great Expectations) and warehouse (Snowflake) from role; minor ding as role mentions pipelines broadly (-1). Clarity: Crystal clear, concrete example specified, well-scoped with define/run/alert structure. Discriminative: Requires applied knowledge of tool integration, trade-offs in metrics/alerting, distinguishes strong data engineers from superficial knowledge.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-490",
        "role_title": "Senior Backend Engineer - Environment Management",
        "question": "\"Design a system that allows for safe preview deployments in Kubernetes using namespaces, resource quotas, and TTL cleanup. Your solution should support isolation between environments (e.g. staging, production), enforce limits on resources, and automatically clean up preview environments after a specified time. Explain how you would structure the system using Go, Kubernetes, Terraform, and ArgoCD, including how you would handle namespace lifecycle, resource quotas, and TTL cleanup. Be specific about how you would implement the TTL cleanup mechanism and how you would ensure that only authorized users can create preview environments.\"",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core requirements (namespace isolation, quotas, TTL cleanup) and specifies exact tech stack (Go, K8s, Terraform, ArgoCD). Clarity: Crystal clear and well-scoped for screening, with specific focus areas; minor deduction for lacking time/format constraints. Discriminative: Tests deep applied knowledge, trade-offs in lifecycle/TTL/auth, strongly separates strong candidates able to reason through integrations from weak ones.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-491",
        "role_title": "Junior Backend Engineer - Read Replicas",
        "question": "Explain how you would implement a simple read replica routing system in Python using PostgreSQL and SQLAlchemy, including how you\u2019d handle replication lag detection and basic failover logic.",
        "scores": {
          "relevance": 9,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Directly targets core role skills (read replica routing, replication lag, failover) in exact tech stack (Python, PostgreSQL, SQLAlchemy). Clear and well-scoped for entry-level, but lacks explicit constraints like time complexity (-1 clarity). Strong trade-offs in lag detection/failover distinguish solid understanding.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-492",
        "role_title": "Backend Engineer - CD Pipelines",
        "question": "\"Design a GitOps-based deployment pipeline for a microservices application using ArgoCD and GitHub Actions. The pipeline should support multi-environment promotion (dev \u2192 staging \u2192 prod), include approval gates at each promotion step, and automate rollbacks in case of deployment failure. Provide a high-level architecture diagram in text and describe how each component interacts with the other.\"",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core GitOps, ArgoCD, multi-env promotion, approvals, and rollbacks from role (-1 for minor GitHub Actions assumption, though workflow-aligned). Clarity: Well-scoped high-level design with explicit requirements and text diagram format. Discriminative: Requires applied knowledge of tool integration, trade-offs in GitOps flows, and failure handling to distinguish strong candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-493",
        "role_title": "Staff Engineer - Reliability Standards",
        "question": "\"Describe a time when you designed or implemented a reliability standard (e.g., max uptime, error rate, latency SLI) that was later challenged or revised. What specific feedback or data led to the revision? How did you adjust the standard, and what was the impact on team behavior or system performance?\"",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core SRE skills (reliability standards, SLIs) central to role; behavioral format targets real experience (-1 for no explicit tech stack tie-in). Clarity: Unambiguous STAR-style prompt with clear structure and examples. Discriminative: Probes deep understanding via challenges, data-driven revisions, adjustments, and impact; separates strong candidates with trade-off reasoning from superficial ones.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-494",
        "role_title": "Backend Engineer - API Mocking",
        "question": "\"Design a WireMock configuration to mock a `GET /users/{id}` endpoint that returns a user object with a dynamic `created_at` timestamp, and validate that the response includes a `created_at` field that is not older than 1 hour. How would you achieve this in TypeScript using WireMock's `request` and `response` features, including handling dynamic content and validation?\"",
        "scores": {
          "relevance": 7,
          "clarity": 8,
          "discriminative": 8,
          "reasoning": "Relevance: Tests core API mocking (WireMock), dynamic responses, and validation directly relevant to role, but deduct -2 for WireMock specificity/type constraints not in role description and -1 for generic design feel. Clarity: Clear intent and scope, but minor ambiguity on exact TypeScript integration with WireMock (-1 for missing depth constraints). Discriminative: Requires applied knowledge of dynamic templating and validation, good separation via trade-offs, not pure recall.",
          "composite": 7.666666666666667
        }
      },
      {
        "role_id": "be-495",
        "role_title": "Senior Backend Engineer - Audit Logging",
        "question": "How would you design a tamper-proof audit logging system in Go that ensures immutable storage and secure querying, while handling high write throughput and long-term retention (e.g., 10+ years) using Kafka and ClickHouse as the primary components?",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core role skills (tamper-proof storage, querying, retention) with Go and specified components Kafka/ClickHouse; minor deduction as role doesn't explicitly list them (-1). Clarity: Crystal clear, well-scoped with specific tech, throughput/retention constraints, no ambiguity. Discriminative: Tests deep applied knowledge, trade-offs (immutability vs throughput, Kafka+ClickHouse integration), distinguishes strong architects from weak ones.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-496",
        "role_title": "Junior Backend Engineer - Model Serving",
        "question": "\"Design a FastAPI endpoint to serve a pre-loaded ML model using PyTorch. The endpoint should accept a JSON request with a 'data' field containing a list of input samples, batch process them, and return a list of predictions. Use Redis to cache the results of the last 10 predictions, so that if the same input is requested again, it is returned from cache instead of reprocessing. Implement error handling for invalid inputs and model loading failures.\"",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core skills (model serving API, batching) and introduces Redis caching/monitoring concepts aligned with role; minor ding as Redis not explicitly listed (-1). Clarity: Crystal clear with precise requirements, inputs/outputs, no ambiguity or bundling. Discriminative: Requires applied knowledge of FastAPI/PyTorch integration, batching, caching trade-offs, error handling\u2014strong separation of strong vs weak candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-497",
        "role_title": "Backend Engineer - Payout Systems",
        "question": "How would you design a system to process a batch of payout requests in a distributed environment using Go, PostgreSQL, Kafka, and Redis? Please consider failure handling, idempotency, and reconciliation.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Relevance: Directly tests core payout batch processing, reconciliation, and exact tech stack (Go, PostgreSQL, Kafka, Redis); minor deduction as role emphasizes bank validation not explicitly called out (-1). Clarity: Well-scoped distributed system design with specific considerations, no bundling or vagueness. Discriminative: Probes deep trade-offs in failure handling, idempotency, and reconciliation in distributed systems, strongly separates strong candidates.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-498",
        "role_title": "Senior Backend Engineer - Profiling Infrastructure",
        "question": "Explain how you would design a system to detect and alert on performance regressions in a Go microservice running on Kubernetes, using Pyroscope for continuous profiling and ClickHouse for storage. Include specific details on how you\u2019d define baselines, how you\u2019d compare profiles over time, and how you\u2019d trigger alerts when regression thresholds are exceeded.",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core focus on continuous profiling, flame graphs (Pyroscope), baselines, comparisons, and storage (ClickHouse); no red flags. Clarity: Crystal clear and well-scoped for screening, minor deduction only for unspecified time/format constraints. Discriminative: Tests deep applied knowledge of integration, trade-offs in baselines/comparisons/alerting, strongly separates capable candidates.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-499",
        "role_title": "Principal Engineer - Search Platform",
        "question": "How would you design a scalable and fault-tolerant search platform for a global enterprise with 10M+ daily active users, where search queries must be processed within 100ms P99 latency? What specific architectural decisions would you make around indexing, query routing, and data freshness, and how would you ensure that the platform remains performant and reliable under high load and regional outages?",
        "scores": {
          "relevance": 7,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Tests core search platform strategy, architecture, and scalability directly matching role's focus on strategy/tech decisions/ops excellence, but generic (no specific tech stack) and enterprise-applicable (-3 for generic). Clarity: Well-scoped with user scale/latency targets, clear components asked, minor lack of time/format constraints. Discriminative: Excellent at probing trade-offs in indexing/routing/freshness/reliability, distinguishes deep architectural thinkers from superficial ones.",
          "composite": 8.0
        }
      },
      {
        "role_id": "be-500",
        "role_title": "Staff Engineer - Streaming Architecture",
        "question": "\"Design a real-time dashboard that displays user activity metrics (e.g., clicks, form submissions) from a high-volume web application. The dashboard must support low-latency updates, handle concurrent user updates, and ensure data consistency under heavy load. How would you architect this using Kafka, Redis, and WebSocket? Describe the event schema, message flow, and data consistency guarantees. Explain how you would ensure the dashboard remains responsive even during peak traffic.\"",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (event schemas, delivery guarantees, fan-out) with Kafka implied; minor ding for specifying Redis/WebSocket not in role desc (-1, no other red flags). Clarity: Well-scoped design question with specific tech, clear components asked, no ambiguity or bundling. Discriminative: Probes trade-offs in real-time streaming (consistency, latency, load), message flow, and guarantees, strongly separating architects from juniors.",
          "composite": 9.0
        }
      }
    ]
  },
  "grok-4-1": {
    "model": "grok-4-1",
    "num_samples": 100,
    "avg_relevance": 9.16,
    "avg_clarity": 9.06,
    "avg_discriminative": 9.02,
    "avg_composite": 9.080000000000002,
    "results": [
      {
        "role_id": "be-406",
        "role_title": "Senior Backend Engineer - LLM Serving",
        "question": "**Question:**  \nIn vLLM, explain how continuous batching works for LLM inference and describe one optimization you've implemented (or would implement) for KV cache management in a production serving setup using Python and vLLM. (2-3 sentences)",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Directly tests core role skills (batching optimization, KV cache management) using vLLM, which aligns perfectly with LLM serving infrastructure; clear scoping (2-3 sentences, explain + one optimization) avoids ambiguity; strong separation via applied knowledge requiring trade-offs/experience, not just recall, while personal implementation tests depth.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-403",
        "role_title": "Backend Engineer - Inference Optimization",
        "question": "**Question:**  \nYou're optimizing a latency-sensitive transformer model for inference using TensorRT on a single GPU. The model experiences high tail latency due to variable input sizes. Describe a step-by-step approach (including code snippets if relevant) to implement dynamic batching and INT8 quantization in TensorRT to reduce p99 latency by at least 50%, while ensuring GPU memory usage stays under 80%. What trade-offs would you monitor?",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Relevance: Directly targets core role skills (quantization, batching, GPU memory) with TensorRT for transformer inference; minor ding as TensorRT not explicitly listed but fits optimization focus (-1). Clarity: Precise scope, constraints, and expectations; no bundling or vagueness. Discriminative: Requires deep applied knowledge, trade-offs, and practical steps, strongly separating experts from pretenders.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-404",
        "role_title": "Senior Backend Engineer - Feature Platform",
        "question": "**Question:**  \nImagine you're building a feature platform using Feast and need to compute a user's average transaction amount over the last 7 days (with a 1-day lookback window) for real-time ML serving via Redis. Walk me through how you'd define the feature view in Feast, including the entities, aggregation, and online serving setup in Python. What key considerations would you have for latency and data freshness?",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests Feast (core to feature platforms), feature computation, online serving (Redis), and discovery\u2014perfect match for role (-0 red flags). Clarity: Precise scope, unambiguous, well-formed with specific params, no bundling or vagueness. Discriminative: Requires applied Feast knowledge (feature view definition, entities, agg, serving), plus trade-offs (latency/freshness), distinguishing strong ML platform engineers from weak ones.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-408",
        "role_title": "Junior Backend Developer - Health Endpoints",
        "question": "**Question:**  \nIn Go, implement a simple HTTP handler for a `/healthz` liveness probe endpoint that returns a 200 OK status if the application is healthy (e.g., a database connection is available) or a 500 status if not. Assume a global `dbHealthy` bool variable tracks the DB status (set it to `true` initially). Use the `net/http` package and keep it under 20 lines.\n\n*(Follow-up: How would you expose this endpoint's response time as a Prometheus metric?)*",
        "scores": {
          "relevance": 9,
          "clarity": 10,
          "discriminative": 9,
          "reasoning": "Directly tests core role skills (health checks/liveness probes in Go) with exact endpoint and assumptions. Crystal clear with line limit and stdlib constraint. Applied coding + follow-up on metrics tests trade-offs (strong separation). Minor ding on relevance for assuming global var (not idiomatic), but spot-on for entry-level role focus.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-401",
        "role_title": "Backend Engineer - Training Infrastructure",
        "question": "**Question:**  \nIn a Kubernetes cluster with multiple GPU nodes, you're using Ray to launch a distributed PyTorch training job for a large language model. The job needs 4 GPUs across 2 nodes but fails with a scheduling error. Walk me through 3 practical steps you'd take to debug and resolve GPU allocation issues in this setup (using Python/Ray/K8s commands or configs). \n\n*(Expected: Quick mentions like `ray status`, `kubectl describe pod`, checking RayCluster CRD GPU requests/limits, node taints/labels, or `nvidia-smi` via debug pod.)*",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core skills in distributed ML infra (K8s, Ray, PyTorch, GPU allocation/scheduling) matching role; no red flags (-1 for minor tech like nvidia-smi but central to GPU issues). Clarity: Precise scenario, specific ask for 3 steps with command examples, well-scoped; expected answer clarifies without ambiguity. Discriminative: Requires applied debugging knowledge and trade-offs in real Ray/K8s GPU setup, distinguishes practitioners from theorists; not recall or trivial.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-402",
        "role_title": "Backend Engineer - Model Deployment",
        "question": "**Question:**  \nYou're deploying a new version of a machine learning model to production using KServe on Kubernetes. Describe a step-by-step Python-based approach (using KServe SDK or kubectl) to perform a canary deployment with 10% traffic initially, monitor for errors, and automate a full rollout or rollback if latency exceeds 200ms or error rate >5%. Include key code snippets or commands.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core model deployment, Kubernetes, canary strategies, monitoring, and automation (-1 for no explicit KServe mention in role, but perfect fit otherwise). Clarity: Crystal clear, well-scoped steps with specific metrics, no bundling or ambiguity. Discriminative: Requires applied knowledge of KServe/K8s, Python automation, monitoring integration, and trade-offs in rollout/rollback, strongly separates skilled candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-410",
        "role_title": "Backend Engineer - Bulk Operations",
        "question": "**Question:**  \nYou're building a Go API endpoint that accepts a bulk request to update up to 1,000 user records in PostgreSQL (e.g., via a JSON array of user IDs and updates). The operation must be idempotent, support partial success (e.g., some updates succeed, others fail due to constraints), and use Kafka for async processing of successes/failures.\n\nDescribe your high-level approach in Go, including:  \n1. How you'd ensure idempotency.  \n2. How you'd handle batching/partial failures in PostgreSQL.  \n3. The Kafka integration flow.  \n\nSketch a simple code outline or pseudocode for the handler if it helps explain. (Answer in 2-5 minutes.)",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core role needs (bulk ops, idempotency, partial success) with exact tech stack (Go, PostgreSQL, Kafka, JSON arrays); minor -1 for assuming Kafka scale not explicit. Clarity: Precise scope (up to 1k, specific subpoints, 2-5 min limit, pseudocode optional). Discriminative: Requires applied trade-offs (idempotency patterns, batch SQL strategies, Kafka flows), separates strong candidates via reasoning depth vs rote answers.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-409",
        "role_title": "Junior Backend Engineer - File Upload",
        "question": "**Question:**\n\nIn a FastAPI endpoint for file uploads, how would you implement basic validation to ensure only image files (under 5MB) are accepted? Provide a short code snippet showing the endpoint, including file reading, size/content-type checks, and a clear error response if invalid.\n\n*(Expected: Use `UploadFile`, check `content_type` starts with 'image/', read bytes to verify size < 5*1024*1024, return 400 error if invalid. Tests practical FastAPI file handling at junior level.)*",
        "scores": {
          "relevance": 9,
          "clarity": 10,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core file upload validation and FastAPI (matches role's multipart uploads, validation focus); minor deduction as role doesn't explicitly list FastAPI (-1). Clarity: Crystal clear with specific requirements, expected checks, and scoped for short snippet. Discriminative: Requires practical applied knowledge of UploadFile, content_type/size checks, error handling\u2014distinguishes juniors who can code it from those who can't; not mere recall.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-411",
        "role_title": "Backend Engineer - Admin APIs",
        "question": "**Question:**  \nYou're building a FastAPI endpoint for an admin to delete a user account, which requires privileged access and must log the action for audit purposes. The endpoint uses PostgreSQL for user data and Redis for session caching.  \n\nOutline the key steps and code structure (in Python/FastAPI) to:  \n1. Authenticate and authorize the admin (e.g., via JWT or API key).  \n2. Perform the deletion atomically.  \n3. Log the audit trail to PostgreSQL.  \n4. Invalidate any related Redis cache.  \n\nProvide a high-level code snippet focusing on dependencies, security checks, and error handling. (No need for full implementation.)",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (admin APIs, privileged access, audit logging) with exact tech stack (FastAPI, PostgreSQL, Redis); minor ding for assuming JWT/API key. Clarity: Crystal clear, well-scoped with numbered steps, high-level snippet request, no ambiguity. Discriminative: Requires applied knowledge of auth, transactions, auditing, caching invalidation, and error handling trade-offs, strongly separates strong candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-414",
        "role_title": "Senior Backend Engineer - Long-Polling",
        "question": "**Question:**  \nIn a Go-based long-polling API for async operations (using Redis for state storage and PostgreSQL for persistence), describe how you'd implement connection management and timeout handling for a `/poll/{operationId}` endpoint. Include key Go code snippets for the handler, a timeout mechanism (e.g., context with deadline), and a fallback strategy if the connection hangs. What Redis operations would you use to check operation status?",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core focus on long-polling APIs, connection management, timeout handling, fallback strategies, with exact tech stack (Go, Redis, PostgreSQL). No red flags. Clarity: Crystal clear, well-scoped for screening with specific endpoint and examples; minor deduction for lacking explicit time/format constraints. Discriminative: Tests applied deep knowledge, trade-offs (timeouts, fallbacks), code snippets, and Redis ops, strongly separating strong candidates; not trivial recall or too broad.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-405",
        "role_title": "Senior Backend Engineer - Model Monitoring",
        "question": "**Question:**  \nIn a production ML system, you've detected data drift in one of your models using a statistical test like Kolmogorov-Smirnov. Walk me through how you'd implement an automated alerting pipeline for this using Python, Prometheus, and Grafana. What key metrics would you expose, and how would you configure the alert to avoid false positives? (Feel free to sketch pseudocode or a high-level architecture.)",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly targets core role responsibilities (drift detection, alerting automation) with exact tech stack implied for production ML monitoring. Clarity: Crystal clear, well-scoped with specific tools and asks for architecture/pseudocode; minor deduction for no strict time constraint. Discriminative: Requires applied knowledge of drift handling, metrics, alerting config, and false positive mitigation\u2014tests trade-offs and integration deeply, strong separation.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-416",
        "role_title": "Staff Backend Engineer - External APIs",
        "question": "**Question:**  \nAs a Staff Backend Engineer leading our external API strategy, imagine we're designing a monetized API for partners using Go, OpenAPI specs, and Kong as the gateway on Kubernetes. Walk me through your approach to implementing rate limiting that supports multiple partner tiers (e.g., free, pro, enterprise) while ensuring fair usage and scalability. What key OpenAPI annotations, Kong plugins, and Go middleware patterns would you use, and why?",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core role responsibilities (API strategy, partner enablement, monetization) with exact tech stack (Go, OpenAPI, Kong, Kubernetes); no red flags. Clarity: Crystal clear scenario, well-scoped to rate limiting with specific tech; minor deduction for lacking explicit time constraint. Discriminative: Tests applied knowledge, trade-offs (fairness, scalability, tiers), and integration of multiple tools, strongly separating strong candidates who can reason through patterns from weak ones.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-415",
        "role_title": "Senior Backend Engineer - Multipart APIs",
        "question": "**Question:**  \nIn FastAPI, describe how you would implement a resumable multipart upload endpoint for large files to S3, including how to handle chunked requests, track upload progress/state (e.g., via a database or Redis), and resume interrupted uploads. Provide a high-level code structure with key dependencies like `boto3` and mention any relevant S3 multipart upload APIs you'd use (e.g., `create_multipart_upload`). What challenges might arise with concurrent chunk uploads, and how would you address them?",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core focus on multipart APIs, resumable uploads, chunk management, and S3 integration with exact tech stack (FastAPI, boto3). No red flags. Clarity: Crystal clear, well-scoped for screening with specific components requested; minor deduction for no explicit time constraint. Discriminative: Tests deep applied knowledge, trade-offs (concurrency challenges), and architecture decisions, strongly separating strong candidates.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-413",
        "role_title": "Senior Backend Engineer - API Gateway Logic",
        "question": "**Question:**  \nIn Envoy, you're building a custom Lua filter for an API gateway to transform incoming requests by adding a custom `x-user-id` header based on a JWT token in the `Authorization` header. The JWT is signed with RS256 and contains a `sub` claim.\n\nProvide a concise Lua script snippet (10-20 lines) for the `onRequest` phase that:  \n1. Extracts and validates the JWT (basic signature check not required, just decode).  \n2. Pulls the `sub` claim.  \n3. Adds it as `x-user-id` header if valid, or returns a 401 response if invalid/missing.  \n\nWhat headers would you inspect or set, and how do you handle errors? (Answer in code + 1-2 sentences explanation.)",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests Envoy Lua filters, JWT handling, and request transformation\u2014core to API gateway extensions (-1 minor for not explicitly listing Envoy/Lua/JWT but perfectly matches role). Clarity: Crystal clear scope, line limit, specific requirements, and format. Discriminative: Requires applied Lua/Envoy/JWT knowledge with error handling and trade-offs, distinguishing strong candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-407",
        "role_title": "Staff Backend Engineer - ML Infrastructure",
        "question": "**Question:**  \nYou're leading the migration of our distributed ML training jobs from Kubernetes-native setups to Ray on Kubernetes for better scalability. Walk me through your 3-step plan to ensure a smooth rollout, including how you'd handle cost optimization and one key failure mode you'd monitor for.\n\n*(Expected answer outline: 1) Pilot with shadow traffic/canary deploys; 2) Tune Ray cluster autoscaling + spot instances for cost; 3) Monitor for node failures via Ray dashboard/ Prometheus. Probes leadership in ML infra platforms.)*",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core ML infra leadership skills (Ray/K8s migration, scaling, cost opt), matches role perfectly (-0 red flags). Clarity: Unambiguous 3-step structure with specific inclusions, well-scoped for screening. Discriminative: Requires applied trade-offs (pilot strategy, cost tuning, failure monitoring), separates leaders with Ray/ML infra depth from generalists.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-417",
        "role_title": "Junior Backend Developer - Event Sourcing",
        "question": "**Question:**\n\nIn an event-sourced system using Kafka and Java, you have an `Order` aggregate. An `OrderCreated` event is published to a Kafka topic. \n\nWrite a short Java code snippet (5-10 lines) for a simple **event handler** that loads the current aggregate state from a repository, applies the `OrderCreated` event to build the initial state (e.g., set order ID and status to \"PENDING\"), and saves the updated state back to PostgreSQL.\n\n*(No need for full Kafka consumer setup or DB connection details\u2014just focus on the handler logic.)*\n\n**Expected Answer Outline (for recruiter reference):**  \n```java\npublic class OrderEventHandler {\n    private OrderRepository repo;\n\n    public void handle(OrderCreated event) {\n        Order order = repo.load(event.getOrderId()).orElse(new Order()); // Load or init\n        order.apply(event); // e.g., order.setId(event.getOrderId()); order.setStatus(\"PENDING\");\n        repo.save(order);\n    }\n}\n```\n*(Tests: event application to aggregate, basic repo interaction, event sourcing flow\u2014junior level, no complex projections/distribution needed.)*",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly targets event sourcing core (event handlers, aggregate state, repo interaction) for entry-level role; uses listed tech (Java, Kafka, PostgreSQL) without straying. Clarity: Crystal clear scope (5-10 lines, handler logic only), unambiguous instructions. Discriminative: Requires applied understanding of event sourcing flow and basic Java patterns, distinguishes juniors who grasp aggregates from those stuck on CRUD; not trivial recall. Minor subtract avoided as single focused task.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-412",
        "role_title": "Backend Engineer - Analytics APIs",
        "question": "**Question:**  \nYou're building a Go-based analytics API endpoint that aggregates daily active users (DAU) from a ClickHouse table over the past 30 days, grouped by date. The query is expensive and runs frequently for dashboard refreshes.  \n\n1. Sketch a simple Go handler function outline (using Gin or net/http) that executes this aggregation query.  \n2. Describe **two caching strategies** you'd implement with Redis to optimize it (e.g., cache keys, TTLs, invalidation), and when you'd use each.  \n\n*(Expect 2-5 min: pseudocode + brief explanation)*",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests Go API development, ClickHouse aggregation queries, time-series DAU, and caching\u2014core to role (-1 minor for not explicitly listing Gin/Redis, but highly aligned). Clarity: Precise scope, parts clearly separated, time-constrained, unambiguous. Discriminative: Requires applied Go pseudocode + trade-off reasoning on caching strategies (keys/TTL/invalidation), separates strong candidates via depth and practicality.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-420",
        "role_title": "Backend Engineer - Graceful Shutdown",
        "question": "**Question:**  \nIn a Go gRPC server running on Kubernetes, explain how you would implement graceful shutdown to handle SIGTERM signals, drain existing connections, and complete in-flight requests without dropping them. Outline the key code changes (e.g., context management, HTTP server shutdown) and any relevant Kubernetes config (e.g., terminationGracePeriodSeconds).",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core focus on graceful shutdown, connection draining, in-flight requests, and signals in exact tech stack (Go, gRPC, Kubernetes). No red flags. Clarity: Crystal clear, well-scoped with specific elements (SIGTERM, code changes, K8s config), minor deduction for lacking explicit time constraint. Discriminative: Tests deep applied knowledge, trade-offs in context/signal handling, and cross-domain integration, strongly separates strong candidates.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-418",
        "role_title": "Junior Backend Engineer - Service Clients",
        "question": "**Question:**  \nIn Go, when implementing a gRPC service client, how would you add a simple retry mechanism for transient errors (e.g., network timeouts)? Provide a short code snippet showing the key parts using the standard gRPC retry options or a basic loop, and explain why you'd choose transient errors only.\n\n*(Expected answer: Use gRPC's built-in `grpc.WithBackoff` and `grpc.RetryPolicy` with `grpc.RetryPolicy{RetryableStatusCodes: []codes.Code{codes.ResourceExhausted, codes.Unavailable}}`, or a simple loop with `time.Sleep` backoff. Focus on not retrying idempotency-sensitive errors like `AlreadyExists`. Tests practical error handling knowledge for juniors.)*",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 8,
          "reasoning": "Relevance: Directly tests core error handling/retry skills for service clients in Go/gRPC per role (-1 minor for entry-level specificity). Clarity: Precise scope, expected depth clear, unambiguous. Discriminative: Requires applied knowledge of gRPC APIs, transient error reasoning, and trade-offs (idempotency), separates juniors with practical skills from trivia recall.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-419",
        "role_title": "Backend Engineer - Retry Logic",
        "question": "**Question:**\n\nYou're implementing retry logic in Go for a Kafka producer that sends critical events to a topic, backed by Redis for tracking retry budgets (max 5 retries per event ID) and PostgreSQL for final failure logging. Describe how you'd structure the retry function, including exponential backoff with jitter. Provide a concise Go code snippet (~20 lines) showing the key logic (e.g., budget check, backoff calc, sleep). What Go libraries would you use, and why?\n\n*(Expected: full jitter impl like `time.Duration(rand.Intn(max-jitter))`, redis.Get/Set for budget, time.Sleep; stdlib `math/rand` or `retry-go`; mid-level shows error classification, budget decrement.)*",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's retry infrastructure focus (backoff, jitter, budgets) with exact tech stack (Go, Kafka, Redis, PostgreSQL). No red flags. Clarity: Crystal clear, well-scoped with line limit, expected depth, and example hints; minor jargon but contextualized. Discriminative: Requires applied knowledge of backoff/jitter math, budget logic, error handling, and Go/Redis patterns; distinguishes mid/junior via code quality and trade-offs without being too broad.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-424",
        "role_title": "Backend Engineer - Message Serialization",
        "question": "**Question:**  \nYou're using Avro with Kafka and Schema Registry to serialize messages for a distributed system. A new version of your Avro schema adds an optional string field `userRole` (default: null) after an existing required field `userId`. Explain whether this change is backward and/or forward compatible, and why. If not fully compatible, what minimal adjustment would make it so?",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core schema evolution, backward/forward compatibility, and validation using Avro (implied in role's serialization infra). No red flags. Clarity: Crystal clear, well-scoped scenario with specific schema change; minor deduction as Avro/Kafka not explicitly in role desc but perfectly aligned. Discriminative: Requires deep understanding of compatibility rules, trade-offs (e.g., reader vs writer schemas), and practical fixes, strongly separating experts from juniors.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-422",
        "role_title": "Junior Backend Engineer - API Testing",
        "question": "**Question:**\n\nYou're writing pytest tests for a FastAPI endpoint that retrieves user data from a PostgreSQL database. The endpoint is defined as:\n\n```python\n@app.get(\"/users/{user_id}\")\ndef get_user(user_id: int, db: Session = Depends(get_db)):\n    return db.query(User).filter(User.id == user_id).first()\n```\n\nWrite a simple pytest test function to verify that the endpoint returns a 404 status code when the user ID doesn't exist in the database. Assume a test client and a test database fixture (`test_db`) are available.\n\n(Expect a concise response showing `TestClient`, a mock non-existent user, `assert response.status_code == 404`, and basic setup.)",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 8,
          "reasoning": "Relevance: Directly tests pytest + FastAPI + PostgreSQL (core stack for API tests role), with fixture usage; minor ding as contract testing not explicit (-1). Clarity: Crystal clear with code example, explicit assumptions, expected response format. Discriminative: Tests applied testing skills (setup, client usage, assertions) distinguishing entry-level competence; not trivial recall.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-423",
        "role_title": "Staff Engineer - Incident Response Platform",
        "question": "**Question:**  \nYou're building an incident response platform in Go that integrates with PagerDuty and Slack. Describe how you would design and implement a reliable retry mechanism for sending Slack notifications triggered by PagerDuty escalations, including handling duplicates, rate limits, and failures. What Go libraries or patterns would you use, and how would you ensure it doesn't exacerbate an ongoing incident?",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core incident response tooling skills with Go, PagerDuty, Slack integration, and reliability\u2014matches role focus perfectly (-0 red flags). Clarity: Well-scoped design question with specific constraints (duplicates, rate limits, failures, non-exacerbation); unambiguous for screening. Discriminative: Probes applied knowledge of retries, Go patterns (e.g., idempotency, backoff), trade-offs in failure handling\u2014strong separation of strong vs weak candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-421",
        "role_title": "Senior Backend Engineer - Security Automation",
        "question": "**Question:**  \nYou're automating secrets rotation for a Kubernetes cluster using HashiCorp Vault and Python. Walk me through how you'd implement a Python script to rotate a database password stored in Vault, update the corresponding Kubernetes Secret, and roll out the change to a Deployment without downtime. What key Vault APIs or auth methods would you use, and how would you handle errors like lease expiration?",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Relevance: Directly tests core secrets rotation in K8s/Vault/Python (-1 minor for not mentioning scanning/compliance). Clarity: Precise, well-scoped steps with specific tech. Discriminative: Requires deep applied knowledge of APIs, auth, K8s rolling updates, error handling, and trade-offs like zero-downtime.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-426",
        "role_title": "Junior Backend Engineer - DevOps Scripting",
        "question": "**Question:**\n\nYou're setting up a simple CI/CD pipeline in GitHub Actions to build and test a Python application. The workflow should:\n\n1. Trigger on push to `main`.\n2. Run on `ubuntu-latest`.\n3. Install Python 3.11 and dependencies from `requirements.txt`.\n4. Run `pytest` tests.\n\nWrite the YAML for `.github/workflows/ci.yml`. (No Docker needed.)\n\n*Expected time: 3-5 minutes. Look for correct structure, `uses:`, `run:`, and basic steps.*",
        "scores": {
          "relevance": 9,
          "clarity": 10,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests GitHub Actions YAML for CI/CD pipelines, core to DevOps automation role (-1 for not mentioning deployments despite role scope). Clarity: Crystal clear with precise requirements, triggers, runner, steps, and time estimate. Discriminative: Requires applied YAML knowledge of structure, 'uses', 'run', and Python setup, distinguishing those who can write working workflows from those who can't; 3-5 min scope perfect.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-427",
        "role_title": "Backend Engineer - Feature Store",
        "question": "**Question:**  \nIn Feast, describe the key steps to define a simple feature view for real-time serving of a user activity count feature (e.g., computed from Kafka events and stored in Redis). Include the main configuration elements you'd specify in the feature view YAML, and explain how you'd retrieve it online for inference in a Python service.\n\n*(Expected answer outline: FeatureView with entities (e.g., user_id), stream source (Kafka), online store (Redis), TTL, and features with value_type. Retrieval via feature_store.get_online_features() with entity rows.)*",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests Feast for feature store infrastructure, core to ML pipelines with versioning, real-time serving, Kafka, and Redis\u2014no red flags. Clarity: Precise, well-scoped with YAML elements and retrieval; minor vagueness on exact YAML syntax prevents 10. Discriminative: Requires applied Feast knowledge, configuration details, and API usage with trade-offs (e.g., TTL, stores), distinguishing strong candidates from weak ones.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-425",
        "role_title": "Senior Backend Engineer - Graph Analytics",
        "question": "**Question:**  \nYou're optimizing a Neo4j graph traversal query for a recommendation engine on a social network with 1 billion edges (users \u2192 follows \u2192 interests). The current Cypher query for \"find top-10 interest recommendations for a user\" is taking 30+ seconds due to high cardinality.  \n\n```\nMATCH (u:User {id: 123})-[:FOLLOWS*1..3]->(other:User)-[:LIKES]->(i:Interest)\nWITH i, count(DISTINCT other) as score\nRETURN i.name, score\nORDER BY score DESC\nLIMIT 10\n```\n\nWhat are 2-3 specific optimizations you'd apply (e.g., indexes, query rewrites, or config changes), and why would they improve performance at this scale?",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests Neo4j/Cypher optimization for graph traversal at billion-edge scale, core to role (-1 minor for assuming Neo4j not explicitly listed). Clarity: Crystal clear with specific query, scenario, and scoped ask for 2-3 optimizations. Discriminative: Requires applied knowledge of graph query tuning, indexes, cardinality issues, and scale trade-offs, separating strong candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-430",
        "role_title": "Junior Backend Engineer - Search Indexing",
        "question": "**Question:**  \nYou're building a Python script to bulk index 10,000 user profiles from PostgreSQL into Elasticsearch. The profiles have fields like `name`, `email`, and `bio`.  \n\n1. Write a simple Elasticsearch mapping for these fields (JSON).  \n2. Provide a Python code snippet using the `elasticsearch` library to connect, prepare a bulk action for 3 sample docs, and execute the indexing.  \n\n(Expect ~100-200 words of code/response; focus on correctness over optimization.)",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 8,
          "reasoning": "Relevance: Directly tests core role skills (bulk indexing, mapping design) with exact tech stack (Python, PostgreSQL, Elasticsearch); minor ding for not covering relevance tuning (-1). Clarity: Crystal clear with specific fields, sample size, library, and scoped expectations. Discriminative: Requires applied knowledge of mappings and bulk API, distinguishes those who can code correctly from theorists; not elite trade-offs but strong for entry-level.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-429",
        "role_title": "Backend Engineer - Log Aggregation",
        "question": "**Question:**  \nYou're building a log aggregation pipeline in Go using Vector for collection and ClickHouse for storage, processing millions of events per second on Kubernetes. Describe a practical approach to implement dynamic retention policies (e.g., delete logs older than 7 days for high-volume sources but 30 days for critical ones). Include key Go code snippets for the retention logic and any ClickHouse/Vector configurations you'd use. (2-5 min)",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core role skills (log pipelines, retention policies at scale) with exact tech stack (Go, Vector, ClickHouse, K8s); minor generic penalty avoided (-0). Clarity: Well-scoped (2-5 min, practical approach, specific elements), unambiguous with clear expectations. Discriminative: Requires applied knowledge of integration, trade-offs (dynamic policies at scale), and code/config, strongly separating experienced candidates; not mere recall.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-428",
        "role_title": "Senior Backend Engineer - Payment Orchestration",
        "question": "**Question:**  \nIn our payment orchestration system built with Go, Kafka, and PostgreSQL, we route transactions to multiple providers based on success rates, costs, and failover rules. Describe how you would implement a failover mechanism in Go that switches providers mid-transaction if the primary provider times out or fails (e.g., after 2 seconds), while ensuring the transaction state is reliably tracked across retries. Include key code snippets for the routing logic and any relevant Kafka/PostgreSQL interactions.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core payment orchestration skills (failover, routing, success tracking) with exact tech stack (Go, Kafka, PostgreSQL); minor deduction as role doesn't explicitly specify 'mid-transaction' switch. Clarity: Well-scoped with specific timeout example, clear expectations for code snippets and DB/Kafka interactions. Discriminative: Requires applied knowledge of Go concurrency, distributed state, retries, and trade-offs (idempotency, at-least-once), strongly separates strong candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-435",
        "role_title": "Junior Backend Engineer - Queue Consumers",
        "question": "**Question:**  \nYou're building a Python RabbitMQ consumer that processes order updates from a queue and saves them to PostgreSQL. The consumer might crash mid-processing, causing duplicates if restarted.  \n\nExplain in 2-3 sentences how you'd make it **idempotent** (safe to retry). What code pattern or DB technique would you use, and why?  \n\n*(Follow-up if time: Sketch a simple code snippet.)*",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests idempotency for message queue consumers (core role skill) with Python/RabbitMQ/PostgreSQL stack; minor deduction as role is entry-level but question fits learning focus (-1). Clarity: Crystal clear, scoped to 2-3 sentences with specific ask for pattern/technique/why; follow-up is optional. Discriminative: Requires applied understanding of trade-offs (e.g., DB unique constraints vs. app logic), distinguishes those grasping retry safety from rote memorizers.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-433",
        "role_title": "Senior Backend Engineer - Platform APIs",
        "question": "**Question:**  \nYou're designing a gRPC-based internal platform API in Go for a service catalog that allows developers to query and provision resources (e.g., databases or caches). The API needs to support pagination for large result sets and integrate with PostgreSQL for storage and Redis for caching.\n\nWalk us through how you'd structure the protobuf schema for the `ListServices` RPC, including pagination and caching strategy. What key gRPC and Go features would you use, and why? (e.g., streaming vs. unary, context handling).",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (API design, service catalog, provisioning) with exact tech stack (gRPC, Go, PostgreSQL, Redis); minor deduction as role doesn't explicitly list gRPC but fits internal platform APIs (-1). Clarity: Crystal clear, well-scoped to one RPC with specific constraints, no ambiguity. Discriminative: Probes deep applied knowledge of protobuf design, gRPC features (streaming vs unary), Go specifics, caching trade-offs, and context handling for strong candidate separation.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-434",
        "role_title": "Principal Engineer - Database Strategy",
        "question": "**Question:**  \nYou're leading the database strategy for a new multi-region application deployment using PostgreSQL as the primary database, with CockroachDB as a potential option for future scalability, Redis for caching, and Kafka for event streaming, all orchestrated on Kubernetes.  \n\nWalk me through your high-level approach to selecting between PostgreSQL (with replication) and CockroachDB for strong consistency across 3 regions, including 2-3 key trade-offs you'd evaluate and how you'd ensure operational resilience (e.g., failover, monitoring).  \n\n*(Expected response: 2-5 min verbal explanation demonstrating strategic trade-offs like latency vs. availability, cost, ops complexity, and practical Kubernetes integration.)*",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core database strategy, multi-region deployment, and tech selection (PostgreSQL, CockroachDB) from role; minor ding for unmentioned Redis/Kafka/K8s (-1). Clarity: Crystal clear, well-scoped with explicit trade-offs, resilience focus, and time expectation. Discriminative: Forces trade-off analysis (latency/availability/cost/ops) and applied Kubernetes knowledge, separating strategic thinkers from rote learners.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-432",
        "role_title": "Backend Engineer - Data Validation",
        "question": "**Question:**\n\nIn a Python-based data pipeline using Great Expectations and Airflow, you've set up an expectation suite that includes schema validation and a statistical check for row count drift (e.g., expecting row counts to stay within 10% of a baseline). During an Airflow DAG run, the task fails due to a Great Expectations validation error on the row count check.\n\nWalk me through **3 practical steps** you'd take to debug and resolve this issue, including specific Great Expectations or Airflow commands/tools you'd use, and explain why each step helps ensure pipeline quality.\n\n*(Expected response time: 2-4 minutes; follow-up: Ask for code snippet if they mention CLI commands.)*",
        "scores": {
          "relevance": 9,
          "clarity": 10,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (schema validation, statistical checks) with exact tech stack (Python, Great Expectations, Airflow); minor deduction as role mentions data lineage not covered (-1). Clarity: Crystal clear, well-scoped with 3 steps, specific tools, expected time, and follow-up. Discriminative: Requires applied debugging knowledge, trade-offs in pipeline quality, and specific commands; strong separation beyond recall.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-431",
        "role_title": "Staff Engineer - Real-time Collaboration",
        "question": "**Question:**  \nYou're designing a real-time collaborative text editor using WebSockets and CRDTs. A user applies a large text insertion (e.g., 10KB) while another user deletes a nearby character simultaneously. Walk me through how you'd ensure both operations are correctly merged without conflicts or data loss, including any trade-offs in your CRDT choice (e.g., Logoot vs. Yjs).",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Relevance: Directly tests core role skills (real-time collab, conflict resolution, operational transforms via CRDTs); minor ding as role mentions OT too but question focuses perfectly on CRDTs (-1). Clarity: Crystal clear scenario, well-scoped, specific examples, no ambiguity. Discriminative: Forces deep understanding of CRDT mechanics, concurrency, and trade-offs (Logoot vs Yjs), excellent separation of strong vs weak candidates.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-438",
        "role_title": "Staff Engineer - ML Serving Infrastructure",
        "question": "**Question:**  \nYou're designing a Triton Inference Server deployment on Kubernetes to serve a high-throughput computer vision model with sub-100ms p99 latency targets. The model uses dynamic batching. Walk me through your approach to configuring batching parameters (e.g., max_batch_size, preferred_batch_size) and any Kubernetes autoscaling integrations you'd use to handle traffic spikes, including one key Prometheus metric you'd monitor for optimization.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Directly targets core role skills (ML serving, batching, latency optimization) with Triton/K8s specifics highly relevant to infrastructure design; crystal clear with precise parameters, integrations, and metric scope; tests applied trade-offs in config and monitoring, distinguishing deep expertise from superficial knowledge. Minor ding for assuming Triton without role mention, but fits ML serving context perfectly.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-442",
        "role_title": "Backend Engineer - Event Streaming",
        "question": "**Question:**  \nIn our event streaming system built with Rust and NATS, we need to support fan-out of real-time updates to thousands of connected clients (e.g., via WebSocket). Describe a practical approach you'd take to efficiently fan-out a single high-volume event topic across NATS subjects, including how you'd handle filtering for client-specific subscriptions (e.g., by user ID or event type). What Rust crates or NATS features would you leverage, and how would you ensure scalability without overwhelming Redis or PostgreSQL?",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core role skills (event streaming, fan-out, filtering) with exact tech stack (Rust, NATS); minor ding for mentioning Redis/PostgreSQL not in role desc (-1). Clarity: Well-scoped, unambiguous, specific constraints provided. Discriminative: Requires applied knowledge of trade-offs, scalability, and specific tools, strongly separates strong candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-437",
        "role_title": "Senior Backend Engineer - Secrets Management",
        "question": "**Question:**  \nYou're implementing automated secret rotation for a service account used by a Go application running in Kubernetes, storing its credentials in HashiCorp Vault. The secret needs to be rotated every 24 hours, with the app fetching the latest version via Vault's API, and old versions revoked after successful update. Walk me through your approach in Go, including key code snippets for the rotation logic, handling Vault authentication, and ensuring zero-downtime during the update. What Vault features (e.g., policies, dynamic secrets) would you leverage, and how would you test this in a Kubernetes environment?",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's secrets management, rotation, Vault, with Go/K8s specifics (no red flags). Clarity: Crystal clear, well-scoped for screening, minor deduction for lacking explicit time/format constraints. Discriminative: Tests deep applied knowledge, trade-offs (zero-downtime), Vault features, and testing\u2014strong separation of strong vs weak candidates.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-440",
        "role_title": "Junior Backend Engineer - Metrics Collection",
        "question": "**Question:**  \nIn Go, using the Prometheus client library, write a short code snippet (5-10 lines) to create and increment a counter metric named `http_requests_total` that tracks the number of incoming HTTP requests, labeled by HTTP method (e.g., \"GET\", \"POST\"). Explain briefly (1-2 sentences) how you'd register and expose this metric for Prometheus scraping.\n\n*(Expected answer: Import `prometheus/client_golang/prometheus` and `prometheus/client_golang/prometheus/promhttp`; define var `httpRequests = prometheus.NewCounterVec(prometheus.CounterOpts{Name: \"http_requests_total\"}, []string{\"method\"});` register it with `prometheus.MustRegister(httpRequests);` then `httpRequests.WithLabelValues(\"GET\").Inc();` in handler; expose via `/metrics` handler with `promhttp.Handler().` Tests basic metric patterns, Go syntax, and observability setup.)*",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's focus on counter metrics/patterns in Go with Prometheus, core to entry-level observability; no red flags. Clarity: Crystal clear with line limit, specific labels, and scoped explanation; minor deduction for not specifying handler context explicitly. Discriminative: Requires applied knowledge of Prometheus client (not just recall), Go syntax, registration, and exposure, distinguishing candidates familiar with observability setup from novices.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-444",
        "role_title": "Backend Engineer - Service Templates",
        "question": "**Question:**  \nYou're setting up a Cookiecutter template for a new Go microservice that will deploy to Kubernetes via ArgoCD. Walk me through 3-5 key best practices you'd include in the template for security defaults and observability, and briefly explain how you'd implement one of them using Go or GitHub Actions.\n\n*(Expected response: Covers basics like non-root containers, network policies, structured logging with zap/zerolog, Prometheus metrics, GitHub Actions for scanning dependencies/secrets. Implementation example: Embedding OTEL exporter in Go main() or Actions lint/test workflow.)*",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (templates, security defaults, observability) with exact tech (Cookiecutter, Go, K8s/ArgoCD); minor ding for not mentioning CI/CD explicitly (-1). Clarity: Precise scope (3-5 practices, one implementation), no ambiguity or bundling. Discriminative: Requires applied knowledge of best practices + implementation, tests trade-offs/explanations to separate strong candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-443",
        "role_title": "Senior Backend Engineer - Data Lake Architecture",
        "question": "**Question:**  \nIn a data lake using Apache Iceberg on S3 with Spark for writes and Trino for queries, describe a practical approach to handle schema evolution (e.g., adding a new column) while ensuring safe partition pruning and time travel queries remain functional. What Iceberg-specific features would you leverage, and why? (Briefly outline 2-3 steps with trade-offs.)\n\n*(Expected answer time: 2-4 minutes. Tests senior-level practical experience with Iceberg schema evolution, integration across Spark/Trino/S3, and awareness of features like schema evolution modes, metadata tables, and branching for time travel.)*",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core skills (Iceberg table formats, schema evolution, partition pruning, time travel) with exact tech stack (S3, Spark, Trino). Clarity: Crystal clear, well-scoped with steps/trade-offs, expected time, and no bundling/ambiguity. Discriminative: Requires applied senior-level knowledge of Iceberg features/modes, trade-offs, and integrations, strongly separating experienced candidates; minor deduction as not ultra-deep edge-case probing.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-439",
        "role_title": "Backend Engineer - Billing Integration",
        "question": "**Question:**  \nIn a Python-based billing system using Stripe and PostgreSQL, describe how you would implement a customer upgrade from a $20/month plan to a $50/month plan mid-cycle (e.g., on day 10 of a 30-day month), ensuring proper proration of the unused portion of the old plan while immediately applying the new plan. Include key Stripe API calls and any relevant database considerations. (Answer in 3-5 sentences or bullet points.)\n\n*(Expected answer outline: Use `stripe.Subscription.modify()` with `proration_behavior='create_prorations'` and `items` to update the price ID; credit the unused $10 days of the old plan ($20 * 20/30 \u2248 $13.33) and charge the prorated new plan ($50 * 20/30 \u2248 $33.33); store subscription metadata and sync status/events in PostgreSQL via a Celery task triggered by Stripe webhooks for idempotency and audit trail.)*",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core focus on billing integrations, proration, Stripe, with Python/PostgreSQL directly from description (no red flags). Clarity: Crystal clear scenario, well-scoped with format/depth guidance; minor deduction for lacking exact proration formula. Discriminative: Tests deep applied knowledge of Stripe API, proration math, DB sync, and idempotency\u2014distinguishes strong candidates via trade-offs and integration thinking.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-441",
        "role_title": "Senior Backend Engineer - Autocomplete Systems",
        "question": "**Question:**  \nYou're building an autocomplete system using Elasticsearch that needs to support prefix matching with user personalization (e.g., boosting recent searches for the user) while targeting sub-50ms latency at 100K QPS. Traffic is sharded across 10 ES nodes.\n\nDescribe your approach to indexing and querying for prefix matches with personalization. What key ES features would you use (e.g., specific query types, mappings), and how would you optimize for latency at this scale? (Focus on 2-3 most critical optimizations.)",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core autocomplete skills, prefix matching, personalization, and latency/QPS from role desc using ES (relevant tech); minor ding as ES not explicitly listed (-1). Clarity: Well-scoped with specific constraints, focus on 2-3 opts, no ambiguity. Discriminative: Requires applied ES knowledge, trade-offs in indexing/querying/optimizations at scale, separates strong candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-445",
        "role_title": "Junior Backend Engineer - Database Migrations",
        "question": "**Question:**\n\nYou're using Alembic to manage database migrations for a PostgreSQL database with SQLAlchemy. A recent migration added a non-nullable `email` column to the `users` table without a default value, but some existing rows now fail validation because they lack email data.\n\nExplain in 2-3 steps how you would create a follow-up Alembic migration to fix this (backfill missing emails with a placeholder like `'missing@example.com'`, make the column nullable temporarily, then add a NOT NULL constraint). Include the key Alembic commands and any relevant SQLAlchemy/Alembic revision code snippets.\n\n*(Expected answer time: 2-4 minutes)*",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly targets Alembic migrations, PostgreSQL, SQLAlchemy, and zero-downtime/rollback patterns for entry-level role; no red flags. Clarity: Crystal clear with specific steps, expected output, and time scope; minor deduction for not specifying exact code format. Discriminative: Requires applied knowledge of Alembic revision creation, upgrade/downgrade logic, and safe schema changes, distinguishing those with hands-on experience from theorists.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-447",
        "role_title": "Backend Engineer - Load Shedding",
        "question": "**Question:**\n\nYou're implementing load shedding in a Go service running on Kubernetes with Envoy as the ingress proxy. Under high load, you need to gracefully degrade by dropping low-priority requests while preserving critical ones. Describe a practical approach using Go code snippets, including:\n\n1. How you'd integrate with Envoy's external authorization or backpressure signals (e.g., via headers or metrics).\n2. A simple adaptive throttling mechanism (e.g., using Redis for a priority queue or rate limiter).\n3. How you'd signal backpressure to upstream callers without crashing.\n\nKeep it concise\u2014focus on the key logic and trade-offs. (Answer in 2-5 minutes.)",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core load shedding, priority queues, throttling, and backpressure from role; mentions Go/K8s/Envoy (relevant stack), minor ding for Redis not in desc (-1). Clarity: Crystal clear, well-scoped with numbered parts, time limit, and focus on logic/trade-offs. Discriminative: Requires applied knowledge, integration, code snippets, and trade-offs; distinguishes deep understanding from superficial knowledge.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-450",
        "role_title": "Principal Engineer - ML Platform Strategy",
        "question": "**Question:**  \nYou're leading the ML platform strategy for our organization and need to recommend a tooling stack for productionizing ML workflows across 50+ teams. Our current setup includes Kubernetes for orchestration, and we're evaluating Kubeflow, MLflow, and Ray.  \n\nWalk me through your high-level architecture design: How would you integrate 1-2 of these tools with Kubernetes to support end-to-end ML lifecycles (experimentation, training, serving, monitoring)? What key governance considerations (e.g., access control, cost management) would you prioritize, and why?",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core ML platform strategy, tooling selection (Kubeflow/MLflow/Ray), governance, and Kubernetes integration from role (-1 for not specifying all role elements like team enablement). Clarity: Crystal clear, well-scoped with specific tools, lifecycle stages, and governance examples; no bundling or vagueness. Discriminative: Requires applied architecture design, integration trade-offs, and prioritized reasoning, strongly separating strategic thinkers from rote responders.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-448",
        "role_title": "Senior Backend Engineer - Webhook Infrastructure",
        "question": "**Question:**  \nIn Go, design a simple retry mechanism for delivering a webhook payload to an endpoint that might fail transiently (e.g., 5xx errors). Outline the key components (code structure, not full implementation), including exponential backoff, deduplication, and a max retry limit. How would you persist retry state across restarts using Redis or PostgreSQL? (Answer in 300 words or less.)\n\n*(Expected answer: Struct with payload ID, exponential backoff like `time.Sleep(base * 2^attempt)`, Redis/PostgreSQL for state like `SETEX payload_id {attempts,deadline}`, dead letter queue on max retries; tests delivery guarantees at senior level.)*",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (retry logic, guaranteed delivery) in Go, with persistence; minor deduction as role doesn't explicitly specify Go or Redis/PostgreSQL (-1). Clarity: Crystal clear, well-scoped with word limit, specific components listed, no ambiguity. Discriminative: Requires applied knowledge of backoff, deduplication, persistence trade-offs, distinguishes seniors via design choices and guarantees; not mere recall.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-452",
        "role_title": "Senior Backend Engineer - Faceted Search",
        "question": "**Question:**\n\nIn Elasticsearch, you're building a faceted search for an e-commerce product catalog with millions of items. Users want to filter by category, price range, and brand, while seeing accurate facet counts (e.g., \"10 laptops in stock under $1000\"). \n\nExplain how you'd structure the search query using `bool` filters and `composite` or `terms` aggregations to ensure facet counts exclude the applied filters and handle high cardinality efficiently. Include a brief code snippet in Python (using elasticsearch-py) and mention one key optimization for scale.\n\n*(Expected answer: Use `post_filter` for client-side filters or filtered aggs; `composite` agg for pagination/high-cardinality; optimizations like sampler agg or keyword subfields.)*",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's faceted search, dynamic filtering, aggregation queries, facet accuracy at scale with Elasticsearch; no red flags. Clarity: Crystal clear, well-scoped with specific requirements and example; minor deduction for not specifying exact response format/length. Discriminative: Tests deep applied knowledge of ES query structure, aggs trade-offs (composite vs terms), optimizations; expected answer shows strong separation; not pure recall.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-449",
        "role_title": "Junior Backend Engineer - Auth Integration",
        "question": "**Question:**  \nIn a Node.js/Express app using TypeScript and PostgreSQL, you're implementing an OAuth2 authorization code flow for Google login. After receiving the authorization code from Google and exchanging it for an access token and refresh token via their token endpoint, walk us through the key steps you'd take to securely store and manage these tokens for the user. Mention any relevant database schema considerations and security best practices (e.g., for refresh tokens). \n\n*(Expected answer outline: Validate code, exchange for tokens securely (HTTPS), hash refresh token before storing in DB with user ID/expiry, store access token in HTTP-only secure cookie or short-lived session, implement refresh logic with validation/rotation, use libraries like `passport-oauth2` or `oauth4webapi`.)*",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's OAuth focus, token handling, refresh flows, and exact tech stack (Node.js/Express/TS/PostgreSQL); no red flags. Clarity: Crystal clear, well-scoped steps with specific context, minor deduction only for implicit time constraint. Discriminative: Requires applied knowledge of secure token practices, trade-offs (hashing vs cookies, rotation), and DB design, strongly separates candidates familiar with OAuth security from those with superficial knowledge; not mere recall.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-436",
        "role_title": "Backend Engineer - API Documentation",
        "question": "**Question:**  \nYou're building an OpenAPI spec for a Node.js/Express API endpoint that retrieves user data from PostgreSQL. The endpoint is `GET /api/v1/users/{userId}` and returns a JSON object like `{ id: number, name: string, email: string }`.  \n\nWrite a minimal OpenAPI 3.0 YAML snippet for this endpoint, including:  \n- Path parameters  \n- A 200 response schema  \n- One example response  \n- Basic security (e.g., API key)  \n\n(Provide just the `paths` and `components` sections; no need for full spec.)  \n\n**Expected Time:** 3-5 minutes  \n**What it tests:** Practical OpenAPI authoring, TypeScript-aligned schema design, API versioning awareness.",
        "scores": {
          "relevance": 9,
          "clarity": 10,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core OpenAPI spec generation and API versioning (v1 in path), central to role's auto-generated specs and documentation tooling; minor ding as Node.js/PostgreSQL are illustrative not required (-1). Clarity: Crystal clear with precise scope (minimal YAML, specific sections, constraints), expected time, no ambiguity. Discriminative: Requires practical applied knowledge of OpenAPI 3.0 YAML syntax, schemas, examples, and security; distinguishes those who can author specs from theorists (strong separation, not recall).",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-451",
        "role_title": "Backend Engineer - Subscription Billing",
        "question": "**Question:**  \nIn a subscription billing system using PostgreSQL and Java/Spring Boot, how would you implement usage-based metering for a feature like API calls, including storing the usage data and checking limits during billing cycles? Briefly describe the schema design and a key method or query you'd use to compute billable usage (e.g., for proration across mid-cycle plan changes).",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Directly tests core role skills (usage metering, proration, billing cycles) with exact tech stack (PostgreSQL, Java/Spring Boot). Crystal clear, well-scoped with specific schema and method/query asks. Requires applied knowledge of design trade-offs, distinguishes strong candidates via practical implementation details. Minor ding for not specifying exact constraints, but excellent overall.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-446",
        "role_title": "Staff Engineer - Distributed Tracing",
        "question": "**Question:**  \nYou're designing a distributed tracing system for a high-scale service using OpenTelemetry and Jaeger as the backend. Traffic is at 10,000 requests/second, and storing 100% of traces is infeasible due to storage costs in ClickHouse.  \n\nWalk me through how you'd architect a sampling strategy: What type(s) of sampling would you use (e.g., head/tail, rate-limiting, probabilistic), how would you configure it in OpenTelemetry, and how would you tune it dynamically based on errors or SLOs? What trade-offs are you considering for trace completeness vs. cost?",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Relevance: Directly targets core role responsibilities (distributed tracing, sampling strategies, SLO correlation) with exact tech stack (OpenTelemetry, Jaeger, ClickHouse); minor ding for assuming specific backend not explicitly listed (-1). Clarity: Crystal clear, well-scoped scenario with concrete numbers, specific asks, no ambiguity. Discriminative: Forces deep applied knowledge of sampling types, config, dynamic tuning, and trade-offs, perfectly separating strong architects from weak ones.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-454",
        "role_title": "Junior Backend Engineer - ETL Scripts",
        "question": "**Question:**\n\nYou're writing a Python ETL script using pandas to load sales data from a CSV file into a PostgreSQL table. The CSV has 100k rows with a 'sale_date' column in string format ('2023-12-01'). \n\nWrite a short code snippet (5-10 lines) that:\n- Reads the CSV into a pandas DataFrame\n- Converts 'sale_date' to a proper datetime type\n- Handles any invalid dates by setting them to NaT (not a time)\n- Inserts the cleaned data into a PostgreSQL table named 'sales' (assume connection string is provided)\n\nInclude necessary imports and explain one potential error you might encounter during the insert.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core ETL skills (pandas data loading/transformation) and PostgreSQL for entry-level role; minor ding for assuming connection string (-1). Clarity: Precise scope (5-10 lines), clear steps, well-constrained. Discriminative: Requires applied pandas/datetime handling, error management, SQL integration, and trade-off explanation (e.g., connection errors), separating competent from weak candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-456",
        "role_title": "Backend Engineer - Query Optimization",
        "question": "**Question:**  \nYou're analyzing a slow PostgreSQL query using `EXPLAIN ANALYZE` that shows a sequential scan on a large table (`users`) with 10M rows, filtering on `email` (varchar(255)). The query is `SELECT * FROM users WHERE email = 'user@example.com';`.  \n\n1. What index would you recommend adding, and why?  \n2. How would you verify if it improves performance?  \n\n*(Expected answer: B-tree index on `email` since it's selective and equality-based; re-run `EXPLAIN ANALYZE` to check for index scan, compare planning/execution time.)*",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Directly tests core role skills (slow query detection, index recommendations, plan analysis) with PostgreSQL example matching query optimization focus; no red flags. Crystal clear with specific scenario, two focused sub-questions (minor bundle deduct not applied as well-scoped). Strong separation via trade-offs (why B-tree/selective), verification method testing applied knowledge beyond recall.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-455",
        "role_title": "Staff Engineer - Developer Portal",
        "question": "**Question:**  \nIn Backstage, imagine you're designing a plugin architecture for a developer portal where teams need to onboard new services to the service catalog with minimal friction. Walk us through how you would structure a custom Backstage plugin (using TypeScript and React) to automate service entity creation from a GitHub PR webhook, including key code snippets for the backend handler and frontend scaffolder UI. What Backstage APIs or patterns would you leverage to ensure it integrates seamlessly with the existing catalog and supports extensibility for future teams?",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Relevance: Directly tests Backstage plugin architecture, service catalog onboarding, and extensibility\u2014core to the role (-1 for not explicitly mentioning team flows). Clarity: Well-scoped with specific tech (TypeScript/React), clear deliverables (structure, snippets, APIs), no bundling or vagueness. Discriminative: Requires deep applied knowledge of Backstage APIs/patterns, trade-offs in integration/extensibility, and code-level implementation\u2014strong separation of strong vs weak candidates.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-453",
        "role_title": "Backend Engineer - Notification Delivery",
        "question": "**Question:**  \nYou're implementing a Go service for delivering push notifications using FCM and APNS. Devices register their tokens in Redis (with TTL for expiration) and PostgreSQL (for persistence). Describe a simple batching strategy to efficiently send notifications to 10,000 registered devices in real-time, including how you'd handle token expiration checks and delivery tracking. What Go data structures and Redis operations would you use?\n\n*(Expected answer outline: Use Redis SCAN or Lua script for batched token fetching with TTL checks; Go channels/slices for concurrent goroutine batches; PostgreSQL for idempotent delivery status updates via UPSERT. Limit batch size e.g., 100-500 per goroutine to respect FCM/APNS rate limits.)*",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core requirements (push notifications, device registration, batching, delivery tracking) with exact tech stack (Go, Redis, PostgreSQL, FCM/APNS). Clarity: Crystal clear, well-scoped for screening, specific tech details provided. Discriminative: Tests applied knowledge of batching, concurrency (Go channels/goroutines), Redis ops, rate limits, and trade-offs like batch size; expected outline shows deep understanding needed. Minor clarity ding for no explicit time constraint, but excellent overall.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-458",
        "role_title": "Backend Engineer - API Gateway Plugins",
        "question": "**Question:**  \nIn Kong, you're developing a custom Lua plugin to implement JWT authentication. The plugin needs to verify the JWT token from the `Authorization` header, decode it using the `lua-resty-jwt` library, validate the `exp` claim against the current time, and either allow the request or return a 401 Unauthorized response.  \n\nWrite a concise code snippet for the `access` phase of the plugin that accomplishes this (assume `kong.request.get_header(\"Authorization\")` provides the header value starting with \"Bearer \"). Include error handling for missing/invalid tokens.  \n\n*(Expected: ~10-15 lines of Lua code demonstrating practical Kong plugin structure, header parsing, JWT library usage, and phase logic.)*",
        "scores": {
          "relevance": 10,
          "clarity": 10,
          "discriminative": 9,
          "reasoning": "Directly tests core Kong Lua plugin development for API gateway authentication (matches role's custom auth focus). Crystal clear with precise scope, assumptions, expected output, and error handling requirements. Strong separation via applied coding with library integration, validation logic, and Kong phase structure; minor deduction as top candidates might discuss edge cases beyond snippet.",
          "composite": 9.666666666666666
        }
      },
      {
        "role_id": "be-457",
        "role_title": "Senior Backend Engineer - SLO Automation",
        "question": "**Question:**  \nIn Go, implement a simple function to calculate the current SLO burn rate for a service, given a time window (e.g., 28 days), the total error budget (e.g., 1000 \"credits\"), the good request count, and the total request count over that window. The burn rate should be the rate at which error budget is consumed per request (e.g., 0.01 credits per bad request). Explain your assumptions and edge cases briefly.\n\n*(Expected answer: Something like `func BurnRate(window int64, budget, good, total uint64) float64 { errorBudgetConsumed := float64(total - good) / float64(budget); return errorBudgetConsumed / float64(window) }` \u2013 tests Go proficiency, SLO math, and senior-level reasoning on reliability concepts like error budgets.)*",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests Go (core for role), SLO burn rate/error budget math central to SLO infrastructure/burn rate alerting; minor deduct for not specifying exact role tech stack but fits perfectly (-1). Clarity: Crystal clear inputs/outputs, well-scoped function impl with explanation request. Discriminative: Requires Go syntax, correct SLO math trade-offs (per-request vs per-time), edge cases (zero requests, full budget); distinguishes seniors who catch bad expected formula from juniors.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-459",
        "role_title": "Junior Backend Engineer - Container Security",
        "question": "**Question:**  \nYou're setting up Trivy to scan a Docker image for vulnerabilities in a CI/CD pipeline. The scan reveals a high-severity vulnerability in a dependency. Walk me through 3 practical steps you'd take next to address it, including one command you'd run with Trivy to help investigate. (Keep your answer to 3-5 sentences.)",
        "scores": {
          "relevance": 9,
          "clarity": 10,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core container scanning skills with Trivy (key for vulnerability detection), perfectly matching entry-level role focus (-0 red flags). Clarity: Crystal clear, well-scoped with exact step count, sentence limit, and specific command request. Discriminative: Requires practical application and trade-offs (e.g., investigate vs. fix), distinguishing those with hands-on CI/CD scanning experience from theorists.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-463",
        "role_title": "Junior Backend Engineer - Full-text Search",
        "question": "**Question:**  \nYou're implementing a full-text search feature in PostgreSQL for a blog application using Python and FastAPI. You have a `posts` table with a `content` column (text). Write a SQL query to create a GIN-indexed `tsvector` column for full-text search, then show a basic `SELECT` query that searches for posts matching the query \"machine learning\" and ranks results by relevance (using `ts_rank`).  \n\n*Bonus (optional): How would you pass this search query from a FastAPI endpoint to execute it?*",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests PostgreSQL full-text search core skills (tsvector, GIN index, ts_rank) from role description, with relevant FastAPI context (-1 for minor Python mention not core to entry-level Postgres focus). Clarity: Crystal clear, well-scoped tasks with specific elements, bonus properly separated. Discriminative: Requires applied SQL knowledge and trade-offs (indexing, ranking), distinguishes candidates who understand FTS implementation from those with superficial knowledge.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-465",
        "role_title": "Backend Engineer - CDC Pipelines",
        "question": "**Question:**  \nIn a Debezium CDC pipeline streaming PostgreSQL changes to Kafka (with Snowflake as the downstream sink), describe how you would handle an unexpected schema change, such as a new required column being added to a table mid-stream. Walk through your step-by-step approach to ensure exactly-once delivery and minimal downtime (2-3 sentences max).",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Directly tests core CDC pipeline skills (schema changes, exactly-once delivery) matching role focus on PostgreSQL/Debezium/Kafka/Snowflake; crystal clear with specific scenario, constraints (2-3 sentences), and step-by-step scope; probes applied trade-offs for minimal downtime, strongly separating experienced engineers from juniors. Minor ding for assuming exact tech stack familiarity without role confirmation (-1 relevance).",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-464",
        "role_title": "Staff Engineer - Presence Systems",
        "question": "**Question:**  \nYou're designing a real-time presence system in Go using WebSockets to track user online status across 1M+ concurrent connections. Describe how you'd implement a heartbeat protocol for connection health checks, including client-side and server-side logic, handling missed heartbeats, and ensuring low-latency state updates (e.g., via Redis). What trade-offs would you consider for scalability in Kubernetes?",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (presence, heartbeats, connection management, state sync) with exact tech (Go, WebSockets, Redis) and scale (1M+), minor ding for Kubernetes not explicit in role (-1). Clarity: Well-scoped, unambiguous, specifies components without bundling multiples. Discriminative: Probes applied knowledge, trade-offs, and deep scalability understanding, strongly separates candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-462",
        "role_title": "Backend Engineer - Alert Routing",
        "question": "**Question:**  \nIn Alertmanager, you've received a batch of 10 alerts from a service outage (e.g., high CPU, high latency, error rate spikes across 5 pods). Describe your Go-based approach to implement smart grouping and deduplication before routing to PagerDuty. Include key grouping labels and a brief code snippet for the matching logic (no full implementation needed). How would you handle escalation if no one acknowledges within 5 minutes?",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (alert grouping, deduplication, escalation) with Alertmanager and Go, matching role focus; minor ding as PagerDuty is implied not explicit (-1). Clarity: Well-scoped, specific scenario, clear asks (approach, labels, snippet, escalation) without ambiguity. Discriminative: Requires applied knowledge of Alertmanager concepts, Go implementation trade-offs, and system design, distinguishing strong candidates via reasoning and code insight.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-460",
        "role_title": "Backend Engineer - Experiment Tracking",
        "question": "**Question:**  \nIn MLflow, you've just logged a run with parameters, metrics, and a model artifact to an S3-backed artifact store. Describe the exact Python code (or key MLflow API calls) you'd use to query and compare the average validation accuracy across the top 3 runs from that experiment, sorted by a custom metric like 'f1_score'. Assume you're using a PostgreSQL tracking backend.\n\n*(Expected answer: Use `mlflow.search_runs()` with `order_by=[\"metrics.f1_score DESC\"]`, `max_results=3`, filter by experiment ID if needed, then aggregate `metrics.val_accuracy` via pandas or list comprehension. Tests practical MLflow querying, metric handling, and Python fluency.)*",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests MLflow (core for experiment tracking), metric handling, artifact storage (S3), and comparison - matches role perfectly; minor ding as not all role aspects explicit (-1). Clarity: Precise, well-scoped API-focused question with clear assumptions; no bundling or ambiguity. Discriminative: Requires applied MLflow knowledge (search_runs params, filtering, aggregation), trade-offs in querying; distinguishes practical users from theorists.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-461",
        "role_title": "Senior Backend Engineer - Fraud Detection",
        "question": "**Question:**  \nYou're building a real-time fraud detection pipeline using Kafka for event streaming, Redis for caching, and scikit-learn models for scoring. Describe how you'd architect the system to score a payment transaction in under 100ms, including how you'd handle feature lookups from PostgreSQL, rule engine evaluation (e.g., velocity checks), and ML inference. What trade-offs would you make for latency vs. accuracy, and how would you use Redis here?\n\n*(Expected answer outline: Kafka consumer \u2192 Redis for hot features/low-latency lookups \u2192 simple rule engine in Python \u2192 lightweight scikit-learn inference (e.g., cached/ONNX) \u2192 score + queue to Postgres/manual review. Trade-offs: approximate features in Redis, async ML retraining, sampling for accuracy monitoring.)*",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's fraud detection pipelines, real-time scoring, rule engines, ML integration; uses exact tech stack implied (streaming, caching, ML, DB). No red flags.\nClarity: Crystal clear, well-scoped to <100ms latency with specific components; minor -1 for not specifying response format/depth.\nDiscriminative: Excellent test of applied architecture, trade-offs (latency/accuracy), and integration; distinguishes deep understanding from superficial knowledge. Not too broad for screening.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-466",
        "role_title": "Senior Backend Engineer - GitOps Tooling",
        "question": "**Question:**  \nIn ArgoCD, you've deployed an application via GitOps, but a manual `kubectl edit` by a developer has caused configuration drift. Walk me through how you'd detect this drift, confirm it, and design a simple Go-based reconciliation loop to automatically revert it to the desired Git state. What key ArgoCD APIs or CRDs would you use, and how would you handle potential race conditions?",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Directly tests core GitOps skills (drift detection, reconciliation loops) matching role's focus on declarative infra tooling. Crystal clear with specific scenario and asks for design, APIs, and trade-offs like race conditions, strongly distinguishing deep ArgoCD/Go expertise. Minor ding for assuming ArgoCD familiarity not explicitly listed, but it's core GitOps tooling.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-467",
        "role_title": "Backend Engineer - Connection Pooling",
        "question": "**Question:**  \nIn a Go application using PgBouncer for connection pooling to PostgreSQL in a Kubernetes environment, you've observed high connection latency during traffic spikes. Walk me through your step-by-step approach to diagnose and tune the PgBouncer pool sizing (e.g., default_pool_size, reserve_pool_size) while ensuring it handles failover without dropping connections. What key metrics would you monitor, and how would you validate the changes?",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Directly tests core role skills in connection pooling (PgBouncer pool sizing), scaling, failover, and transaction handling in Kubernetes; highly relevant to PostgreSQL/Go context. Crystal clear with specific parameters, step-by-step expectation, metrics, and validation; well-scoped. Strong differentiation via diagnostic reasoning, tuning trade-offs, and monitoring, distinguishing experienced engineers.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-468",
        "role_title": "Junior Backend Engineer - CI Pipelines",
        "question": "**Question:**  \nYou're setting up a GitHub Actions workflow in YAML to build and test a Python application using Docker. The workflow runs `pip install -r requirements.txt` which takes 2 minutes every time.  \n\nWrite a simple 4-step job that:  \n1. Builds a Docker image from a `Dockerfile` (assume it installs Python deps).  \n2. Caches the pip dependencies to speed it up.  \n3. Runs unit tests inside the container.  \n4. Uses matrix strategy to parallelize tests across Python 3.9 and 3.11.  \n\nProvide the full `jobs.<job_id>.steps` YAML snippet (no need for full workflow file).  \n\n*Follow-up (if time): What does the cache key look like, and why use it?*",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core CI pipeline skills (GitHub Actions YAML, caching, matrix parallelization, Docker) from role description (-0 red flags). Clarity: Precise scope (4 steps, YAML snippet only), unambiguous instructions, well-constrained. Discriminative: Requires applied YAML knowledge with trade-offs (caching keys, matrix), follow-up probes deeper understanding; strong separation for entry-level.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-469",
        "role_title": "Senior Backend Engineer - Chaos Engineering",
        "question": "**Question:**\n\nIn Chaos Mesh on Kubernetes, you're designing a failure injection experiment to test pod resilience in a critical service. Describe a scenario where you'd use a **NetworkChaos** to simulate 50% packet loss between pods in a specific namespace, including the key fields you'd set in the Chaos Mesh CRD YAML (e.g., mode, selector, and disruption rules). Why would you choose **One** mode over **All** here, and how would you limit the blast radius to avoid impacting production traffic? \n\n*(Expected: Concise YAML snippet or key fields, rationale on mode for controlled testing, namespace/selector for blast radius; tests practical Chaos Mesh + K8s knowledge in 2-5 mins.)*",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests Chaos Mesh and Kubernetes skills core to chaos engineering role (-1 for assuming Chaos Mesh as 'the' tool, not explicitly listed). Clarity: Precise, well-scoped with examples, avoids ambiguity. Discriminative: Requires applied YAML knowledge, trade-offs (One vs All), and blast radius control, separating practical experts from theorists in <5 mins.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-471",
        "role_title": "Staff Engineer - Zero Trust Architecture",
        "question": "**Question:**  \nIn a Kubernetes-based microservices environment using Envoy as a sidecar proxy, walk me through how you would automate mTLS between services using SPIFFE/SPIRE for identity, including key components you'd configure and a high-level step-by-step approach. What challenges might arise with certificate rotation, and how would you address them?",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core mTLS automation, SPIFFE identity (matches SPIRE), and microservices security in Kubernetes/Envoy context; minor -1 for not explicitly mentioning federation/policy but central to zero trust. Clarity: Precise, well-scoped with components/steps/challenges; no bundling or vagueness. Discriminative: Requires applied knowledge of integration, trade-offs in rotation challenges; distinguishes deep expertise from superficial recall.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-470",
        "role_title": "Backend Engineer - GraphQL Federation",
        "question": "**Question:**  \nIn Apollo Federation, explain how entity resolution works across subgraphs during query execution. Walk through a simple example with two subgraphs (e.g., `User` service exposing a `User` entity and `Order` service referencing it), including the key directives involved and what the gateway does step-by-step. What happens if entity resolution fails?\n\n*(Expected answer: Candidate describes `@key` directive on entity in owning subgraph, `@requires` or `@provides` if needed, reference resolution via `_entities` field on gateway, canonical entity definition, batching via `@ApolloFederationInclude`, and error handling like partial resolution or normalization failures. Tests practical understanding of federation flow.)*",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core Apollo Federation skills (schema stitching, entity resolution, query planning) from role description; no red flags. Clarity: Crystal clear with specific example scope, minor deduction for lacking explicit time/format constraint. Discriminative: Requires deep applied knowledge of directives, flow, trade-offs (batching, errors), strong separation of experts from surface-level knowers; not mere recall.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-472",
        "role_title": "Backend Engineer - Vector Search",
        "question": "**Question:**  \nYou're building a semantic search API using FastAPI, Pinecone for vector storage, and a model like Sentence Transformers for embeddings. Walk me through the key steps to implement a hybrid search endpoint that combines vector similarity (from Pinecone) with a keyword filter (using metadata), including how you'd generate and upsert embeddings for new documents. What Pinecone features would you use, and how would you handle rate limiting with Redis? \n\n*(Expected: 2-4 min verbal explanation covering embedding gen/upsert, Pinecone hybrid query, metadata filtering, Redis for caching/rate limits)*",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's vector search, embeddings, indexing, hybrid search; uses exact tech stack (FastAPI, Pinecone, embeddings). Clarity: Crystal clear steps and scope, minor deduction for no explicit time constraint (expected time noted but not in question). Discriminative: Tests integrated applied knowledge, trade-offs in hybrid search/upsert/rate limiting; strong separation without being too broad.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-477",
        "role_title": "Senior Backend Engineer - Data Catalog",
        "question": "**Question:**  \nYou've implemented a data pipeline using Apache Atlas for metadata management and Kafka for streaming events. Describe a practical approach to automatically extract and register lineage metadata (e.g., from Airflow DAGs processing data in PostgreSQL) into Atlas, including how you'd handle updates if a downstream job fails. What Kafka topic schema and Atlas entity types would you use?",
        "scores": {
          "relevance": 7,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Tests core lineage/metadata skills but assumes unlisted tools (Atlas, Kafka, Airflow; -2 red flag); Clarity: Crystal clear, well-scoped with specific scenario; Discriminative: Excellent on trade-offs, integration depth, schema design, and error handling for strong separation.",
          "composite": 8.333333333333334
        }
      },
      {
        "role_id": "be-479",
        "role_title": "Principal Engineer - Data Architecture",
        "question": "**Question:**  \nYou're leading the data architecture for a platform handling both high-throughput transactional workloads (OLTP) and real-time analytics (OLAP). We use PostgreSQL for transactions, ClickHouse for analytics, Kafka for streaming, Redis for caching, and Kubernetes for orchestration.  \n\nWalk me through your approach to architecting an efficient data pipeline that ingests events from Kafka, supports low-latency OLTP lookups in PostgreSQL, and feeds aggregated analytics into ClickHouse\u2014while ensuring scalability and fault tolerance. What key trade-offs would you consider, and how would you mentor a junior engineer on implementing it?  \n\n*(Expected: 2-5 min response; probes practical architecture strategy, tech stack integration, trade-offs like consistency vs. latency, and mentorship at principal level)*",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's data architecture strategy for analytics/transactional workloads, directly tests listed tech stack (PostgreSQL, ClickHouse, Kafka, Redis, K8s), no red flags. Clarity: Crystal clear, well-scoped with expected time and probes specified, minor deduction for no explicit constraints on format. Discriminative: Tests deep applied knowledge, trade-offs (e.g., consistency vs. latency), integration, scalability, and principal-level mentorship, strongly separates candidates.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-474",
        "role_title": "Backend Engineer - Dashboards API",
        "question": "**Question:**  \nIn our observability platform, we use Grafana dashboards with templating variables (e.g., `${instance}` or `${namespace}`) that are resolved at query time from Prometheus data sources. When building a Go API endpoint to render and share a dashboard snapshot (e.g., for public viewing), how would you handle variable substitution server-side? Describe the key steps, potential pitfalls with PostgreSQL/Redis storage, and a rough Go pseudocode snippet for the substitution logic. (2-4 min)",
        "scores": {
          "relevance": 9,
          "clarity": 10,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core role skills (Go APIs, dashboard templating, variable substitution, sharing) with exact tech (Grafana, Prometheus); minor ding for not explicitly mentioning PostgreSQL/Redis in role but fits observability platform (-1). Clarity: Crystal clear, well-scoped (2-4 min), specific context, precise ask. Discriminative: Tests applied knowledge, trade-offs (pitfalls with DB storage), and practical pseudocode; strong separation of deep vs superficial candidates.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-476",
        "role_title": "Backend Engineer - Live Updates",
        "question": "**Question:**\n\nIn our live updates system using Socket.io and Redis, we need to broadcast dashboard data updates only to users viewing a specific dashboard (e.g., dashboard ID \"abc123\"). Describe how you'd implement room management for this: \n\n1. How do you join/leave rooms when a user connects/disconnects?\n2. How do you trigger a selective broadcast (e.g., on data change)?\n3. How do you handle a user reconnecting mid-session to rejoin the correct room?\n\nProvide concise TypeScript/Node.js code snippets for the key parts (e.g., join logic, emit logic).",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests Socket.io rooms, Redis broadcasting, and reconnection\u2014core to role's live updates, dashboard-specific broadcasting, and reconnection logic (-0 red flags). Clarity: Well-scoped with 3 focused sub-questions, clear TypeScript/Node.js expectations, no bundling issues. Discriminative: Requires applied knowledge of real-time systems with trade-offs (reconnection handling), code snippets test practical skills, strong separation beyond recall.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-483",
        "role_title": "Backend Engineer - WAF Integration",
        "question": "**Question:**  \nIn a Go-based WAF integration using ModSecurity with Nginx, you need to implement custom rate limiting to block IPs exceeding 100 requests per minute. Describe a practical approach using Redis for the rate limiter, including key code snippets for the increment/check logic and eviction strategy. How would you integrate this with ModSecurity rules? (Answer in 2-5 minutes)",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core WAF rate limiting with Go, Redis, ModSecurity/Nginx from role; minor ding for assuming Go/ModSecurity not explicit (-1). Clarity: Precise scope, time limit, clear components requested. Discriminative: Requires applied knowledge of rate limiting (sliding window/token bucket trade-offs), Redis ops, and ModSecurity integration, distinguishing strong candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-475",
        "role_title": "Junior Backend Engineer - Search Analytics",
        "question": "**Question:**  \nYou're analyzing slow search queries in Kibana using Elasticsearch logs for our search analytics platform. A sample log shows a query taking 5+ seconds with this snippet:  \n\n```\n{\n  \"query\": {\n    \"bool\": {\n      \"must\": [\n        { \"match\": { \"title\": \"python tutorial\" } },\n        { \"range\": { \"timestamp\": { \"gte\": \"now-7d\" } } }\n      ]\n    }\n  },\n  \"aggs\": { \"by_user\": { \"terms\": { \"field\": \"user_id\", \"size\": 1000 } } }\n}\n```\n\nWhat\u2019s one likely performance issue here, and how would you fix it using Python (e.g., via elasticsearch-py) or Kibana? Explain briefly.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests Elasticsearch query analysis and optimization, core to search analytics logging (9, minor -1 for Python not in role desc but fits entry-level). Clarity: Crystal clear with concrete log snippet, specific scope, unambiguous (9). Discriminative: Requires applied understanding of ES performance (e.g., high-cardinality terms agg), trade-offs, and fixes; distinguishes candidates with practical knowledge (9).",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-473",
        "role_title": "Senior Backend Engineer - Refund Processing",
        "question": "**Question:**  \nIn a refund processing system using Java and Spring Boot, describe how you would implement partial refunds (e.g., refunding $15 out of a $20 original transaction) while ensuring ledger reconciliation remains balanced. Include how you'd handle a payment provider callback confirming the partial refund, and what data you'd log for audit trails. (Sketch a simple code structure or key methods if possible.)\n\n*(Expected answer outline: Use double-entry ledger (credit original charge, debit refund liability, credit customer on confirmation); idempotent callback handler with transaction ID/versioning; atomic DB updates via Spring @Transactional; audit logs with event sourcing or append-only tables capturing before/after states. Tests Kafka/Redis integration implicitly via scalability mentions.)*",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core refund processing, partial refunds, ledger reconciliation, provider callbacks, and audit trails from role; Java/Spring Boot matches implied stack (-0 red flags). Clarity: Crystal clear, well-scoped with specific scenario, no ambiguity or bundling. Discriminative: Requires applied knowledge of double-entry accounting, idempotency, transactions, and auditing; elicits trade-offs and structure to separate strong candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-481",
        "role_title": "Backend Engineer - Canary Deployments",
        "question": "**Question:**  \nYou're implementing a canary deployment for a Go microservice using Istio and ArgoCD. Walk me through how you'd configure Istio VirtualService and DestinationRule to split 10% of traffic to the canary version (v2), monitor key Prometheus metrics for comparison against the baseline (v1), and set up an automated promotion gate in ArgoCD if the canary succeeds. What specific metrics would you prioritize, and how would you trigger a rollback if they degrade?",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Directly tests core canary deployment skills (traffic splitting, metrics, rollback, promotion gates) with exact role tech (Istio, ArgoCD, Prometheus, Go microservice). Crystal clear, well-scoped walkthrough with specific constraints; elicits trade-offs in metrics, monitoring, and automation, strongly separating experienced candidates. Minor clarity ding for no explicit time constraint, but excellent overall.",
          "composite": 9.666666666666666
        }
      },
      {
        "role_id": "be-482",
        "role_title": "Senior Backend Engineer - API Monetization",
        "question": "**Question:**\n\nYou're building an API monetization platform in Go using Kong as the gateway, PostgreSQL for persistent storage, and Redis for caching. Describe how you would implement usage quota enforcement for API requests, including:\n\n1. Tracking requests per API key and time window (e.g., per hour/day).\n2. Handling quota checks at high scale (e.g., 10k reqs/sec).\n3. Integrating with Stripe for overage billing when quotas are exceeded.\n\nProvide a high-level design with key data structures, Go pseudocode snippets for critical parts (e.g., middleware or service), and trade-offs for consistency vs. performance.",
        "scores": {
          "relevance": 7,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Tests core quota management and billing skills for the role but docked -2 for mentioning Kong/Stripe/Go stack details not explicitly listed in role description. Clarity: Crystal clear, well-scoped with specific subpoints and expectations for design/pseudocode/trade-offs. Discriminative: Excellent at separating candidates via applied high-scale design, trade-offs, and implementation details; not mere recall.",
          "composite": 8.333333333333334
        }
      },
      {
        "role_id": "be-478",
        "role_title": "Backend Engineer - Cost Attribution",
        "question": "**Question:**  \nIn our cost attribution system built with Go and Kubernetes, we need to allocate cloud resource costs (e.g., from AWS or GCP) to teams based on resource tags like `team:engineering` or `team:sales`. Walk me through how you would design and implement a Go function to parse a list of Kubernetes pod metadata (including labels/tags), aggregate costs by team, and output a simple chargeback report. What key considerations would you have for accuracy, performance, and error handling? (Feel free to sketch pseudocode.)\n\n*(Expected: 2-5 min verbal/whiteboard response testing practical Go logic, grouping/aggregation, awareness of K8s labels, and mid-level tradeoffs like concurrency or validation.)*",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (cost attribution, resource tagging, K8s, Go) with no red flags (-0). Clarity: Crystal clear, well-scoped for screening with explicit time/depth expectations. Discriminative: Tests applied Go logic, K8s knowledge, aggregation, and trade-offs (accuracy/perf/error handling), strongly separating mid-level candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-480",
        "role_title": "Junior Backend Engineer - Deployment Automation",
        "question": "**Question:**  \nYou're tasked with deploying a simple Python Flask app to an AWS EC2 instance using Ansible. The app is in a Git repo, and you need to pull the latest code, install dependencies via `pip`, and start the app with `gunicorn`.  \n\nWrite a basic Ansible playbook (or key tasks from it) to automate this deployment on a target host. Assume Docker and AWS setup are already handled, and focus on the Ansible tasks for code deployment and service startup. What error-handling or idempotency features would you add to make it junior-friendly and reliable?  \n\n*(Expected: 5-10 lines of YAML tasks showing `git`, `pip`, `systemd` or similar, plus 1-2 sentences on best practices like `creates:` or `notify:`.)*",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests Ansible for deployment scripts, core to role's config management and environment setup; minor ding as EC2/Docker not explicitly listed but fits AWS deployment context (-1). Clarity: Well-scoped with clear assumptions, expected output, and constraints; no bundling or ambiguity. Discriminative: Requires writing practical YAML tasks plus explaining idempotency/trade-offs, separates those who can apply Ansible from theorists.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-486",
        "role_title": "Backend Engineer - Span Processing",
        "question": "**Question:**  \nIn our span processing pipeline using OpenTelemetry and Go, we're implementing tail sampling to retain only the most relevant traces (e.g., based on error rate or latency). Describe a practical approach you'd take to implement a tail sampler in Go that decides on-the-fly whether to keep a completed trace, including key data structures and any concurrency considerations. What trade-offs would you make for memory usage vs. sampling accuracy?",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core role skill (tail sampling in span processing) with exact tech stack (OpenTelemetry, Go); no red flags. Clarity: Crystal clear, well-scoped for screening with specific focus on data structures/concurrency; no ambiguity or bundling. Discriminative: Tests deep applied knowledge, trade-offs (memory vs accuracy), and concurrency in distributed context; strong separation of strong vs weak candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-484",
        "role_title": "Staff Engineer - Model Registry",
        "question": "**Question:**  \nYou're designing a model registry using MLflow to manage the ML lifecycle across a team of 20 data scientists. Models are trained daily, stored in S3, and deployed via Kubernetes. Walk me through your versioning strategy for models (including how you'd handle staging vs. production versions), and how you'd implement a simple governance workflow to require approval before promoting a model from staging to production. What MLflow features or integrations (e.g., with PostgreSQL or Kubernetes) would you leverage, and why?",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Directly targets core role skills (model registry, versioning, governance, deployment). Specifies MLflow (key for model registry), S3/K8s (storage/deployment), with clear scope. Highly discriminative via trade-offs in versioning/approvals and MLflow integrations; minor ding for assuming MLflow familiarity not explicitly in role desc (-1 relevance). Crystal clear, well-scoped, tests applied deep knowledge.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-489",
        "role_title": "Backend Engineer - Data Quality",
        "question": "**Question:**\n\nIn a dbt project running on Snowflake with Great Expectations for data quality checks, you've noticed intermittent schema drift where a source table occasionally adds a new column, causing downstream dbt models to fail. Describe a practical approach using dbt, Great Expectations, and Python (e.g., in an Airflow task) to detect this proactively and alert your team, including 2-3 key code snippets or configurations you'd implement.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core schema drift detection in dbt/Snowflake/Great Expectations stack for warehouse pipelines (-0 red flags). Clarity: Well-scoped with specific tools, scenario, and request for 2-3 snippets; no ambiguity or bundling. Discriminative: Requires applied knowledge of integration, proactive detection, alerting, and trade-offs in real pipeline failure scenario, strongly separates strong candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-487",
        "role_title": "Senior Backend Engineer - Search Ranking",
        "question": "**Question:**  \nIn a learning-to-rank (LTR) pipeline for search relevance, you've identified that your model's precision@10 is dropping in production after deploying a new version. Walk me through your step-by-step debugging process, including 2-3 key metrics you'd check first and how you'd use your tech stack (e.g., Elasticsearch, Python, ML tools) to investigate and iterate.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core LTR pipeline skills (debugging production issues) matching role's focus on pipelines, feature engineering, model training, and A/B testing; mentions relevant stack without extras (-0 red flags). Clarity: Crystal clear, well-scoped with specific asks (step-by-step, 2-3 metrics, tech stack use), no bundling or ambiguity. Discriminative: Tests applied debugging, trade-offs, and deep production LTR understanding, strongly separates candidates; not recall or trivial.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-485",
        "role_title": "Junior Backend Engineer - Payment Webhooks",
        "question": "**Question:**\n\nYou're building a FastAPI endpoint to receive payment webhooks from a provider like Stripe. The webhook payload contains a `payment_id` and event details. Explain in 3-5 sentences how you'd implement idempotency to avoid duplicate processing if the same webhook is retried. Include relevant Python/FastAPI code snippet (e.g., using Redis or PostgreSQL).",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests idempotency for payment webhooks, core to role; FastAPI/Python matches implied stack, no red flags (-0). Clarity: Precise scope (3-5 sentences, code snippet, specific DB options), unambiguous for screening. Discriminative: Requires applied knowledge of idempotency patterns, DB integration, and trade-offs, strongly separates candidates able to implement vs. theorize.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-488",
        "role_title": "Backend Engineer - Gaming Backend",
        "question": "**Question:**  \nIn a multiplayer game backend using Go and WebSockets, you need to synchronize player positions across 100 connected clients with minimal latency. Redis is available for pub/sub, and PostgreSQL for persistent state.  \n\nDescribe a simple, practical approach to handle state synchronization (e.g., player position updates). Include:  \n- How you'd structure the update flow in Go.  \n- Role of WebSocket vs. Redis.  \n- How to avoid overwhelming clients or the server (1-2 sentences on optimizations).  \n\n*(Expect 3-5 sentence response; follow-up on trade-offs if time allows.)*",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 8,
          "reasoning": "Relevance: Directly targets core role skills (real-time state sync for multiplayer game backend) and exact tech stack (Go, WebSockets, Redis pub/sub, PostgreSQL), no red flags. Clarity: Crystal clear with precise scope, components specified, and expected response length; minor jargon is contextualized. Discriminative: Tests applied architecture knowledge and optimizations with good separation via trade-offs potential, though slightly broad (-1 for possible edge over 10min).",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-493",
        "role_title": "Staff Engineer - Reliability Standards",
        "question": "**Question:**  \nAs a Staff Engineer defining reliability standards for our org, walk me through how you would design a production readiness review (PRR) process for a new Kubernetes-based service. What key criteria would you include, and how would you ensure it catches reliability risks early? (Feel free to sketch a simple checklist or gates on paper if helpful.)",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly matches core role duties (reliability standards, PRR design) with Kubernetes from context; no red flags. Clarity: Well-scoped for screening, clear intent, minor deduction for no explicit time constraint. Discriminative: Tests applied SRE knowledge, trade-offs, and process design, strongly separates experienced Staff Engineers from juniors.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-490",
        "role_title": "Senior Backend Engineer - Environment Management",
        "question": "**Question:**  \nIn our environment management system for preview deployments, we're using Kubernetes namespaces for isolation, with resource quotas and TTL-based cleanup via a Go service. Describe a practical approach you'd take in Go to implement the TTL cleanup logic\u2014specifically, how would you efficiently identify and delete namespaces that have exceeded their TTL (e.g., 24 hours), while minimizing API server load and handling potential race conditions? Include key code snippets or pseudocode for the core loop or query logic.\n\n*(Expected response: Candidate outlines a cron-like scheduler or periodic reconciliation loop using Kubernetes client-go/informers for efficient watching/listing of annotated namespaces, filtering by creation timestamp + TTL annotation, then graceful deletion with finalizers or preconditions. Senior candidates might mention leader election, rate limiting, backoff retries, or integration with custom resources.)*",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core focus on namespace isolation, quotas, and TTL cleanup in Kubernetes/Go (-0 red flags). Clarity: Crystal clear with specific scope (TTL logic, efficiency, race conditions), well-formed for screening; minor deduction for lacking exact time constraint. Discriminative: Tests applied deep knowledge, trade-offs (API load, races), distinguishes seniors via advanced concepts like leader election; not trivial recall.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-494",
        "role_title": "Backend Engineer - API Mocking",
        "question": "**Question:**\n\nYou're setting up WireMock in a Docker container for mocking a PostgreSQL-based user service API during contract testing with TypeScript/Node.js. The API endpoint is `GET /users/:id`, which should return a templated JSON response like `{ \"id\": \"{{request.path.[1]}}\", \"name\": \"John Doe\", \"email\": \"{{randomEmail}}\" }` only if the request includes a valid `Authorization: Bearer <token>` header.\n\nProvide a concise code snippet (or docker-compose.yaml + stubs config) showing:\n1. How to run WireMock in Docker.\n2. How to configure the stub for header-based request matching and response templating.\n\n(Expect ~3-5 lines of key config; explain any non-obvious parts briefly.)",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests Docker, WireMock, TypeScript/Node.js context, templating, and header matching\u2014core to API mocking infrastructure (-1 for not explicitly covering contract verification). Clarity: Precise scope, constraints (3-5 lines), and expectations make it unambiguous. Discriminative: Requires applied knowledge of WireMock config, Docker integration, and templating syntax to distinguish proficient candidates from those with superficial familiarity.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-495",
        "role_title": "Senior Backend Engineer - Audit Logging",
        "question": "**Question:**  \nIn Go, design a simple tamper-proof audit logging function that appends a log entry (e.g., `{userID: string, action: string, timestamp: time.Time}`) to a Kafka topic, ensuring immutability and integrity against tampering. Include hashing for tamper detection and briefly explain how you'd extend it for ClickHouse storage with retention policies. Provide code snippet and rationale (2-3 sentences).",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core audit logging skills (tamper-proofing, hashing) with Go (likely relevant), Kafka for streaming, and ClickHouse for storage/retention per role (-1 for assuming Go/Kafka not explicitly listed). Clarity: Well-scoped with specific log structure, requirements, code snippet request, and brief extension; no ambiguity. Discriminative: Requires applied design (hashing, Kafka producer), code, trade-offs for immutability, and extension thinking, separating strong candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-492",
        "role_title": "Backend Engineer - CD Pipelines",
        "question": "**Question:**  \nIn an ArgoCD GitOps setup using Helm charts for a multi-environment deployment (e.g., dev, staging, prod), describe a practical approach to implement automated promotion from staging to prod, including a manual approval gate and a rollback mechanism if the deployment fails health checks. What Go-based tool or custom controller might you use to automate the rollback, and why?  \n\n*(Expected answer outline: Use ArgoCD App of Apps or ApplicationSets with environment-specific Helm values; implement approval via ArgoCD's resource hooks or integrate with GitHub Actions for manual gates; for rollback, leverage Argo Rollouts with AnalysisTemplates for health checks or a custom Go controller using the ArgoCD API/client-go to detect failures and trigger `argocd app rollback`. Tests GitOps/CD pipeline design at mid-level.)*",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core GitOps/CD pipeline skills (ArgoCD, Helm, multi-env, approvals, rollbacks) from role; minor ding for Go tool not explicitly listed (-1). Clarity: Precise, well-scoped with context, no ambiguity. Discriminative: Requires applied design knowledge, trade-offs (hooks vs Actions, Rollouts vs custom), distinguishes mid-level expertise from basics.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-499",
        "role_title": "Principal Engineer - Search Platform",
        "question": "**Question:**  \nAs a Principal Engineer leading our search platform, imagine our enterprise search service\u2014built on Elasticsearch, Kafka for event streaming, and Redis for caching\u2014is experiencing increased tail latencies during peak traffic, impacting user experience. Walk me through your step-by-step approach to diagnose and resolve this issue, highlighting key trade-offs in scaling decisions you'd consider for long-term platform strategy.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core search platform leadership skills with exact tech stack (Elasticsearch, Kafka, Redis) from role; minor genericism (-1). Clarity: Crystal clear scenario, step-by-step scoping, no ambiguities. Discriminative: Probes applied diagnostics, resolution, and strategic trade-offs, strongly separating experienced principals from juniors.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-491",
        "role_title": "Junior Backend Engineer - Read Replicas",
        "question": "**Question:**\n\nYou're building a Python service using SQLAlchemy and PostgreSQL with a primary database and one read replica. You notice that some customer-facing queries (e.g., fetching a user's recent orders) are occasionally returning stale data due to replication lag on the read replica.\n\nIn 2-3 sentences, explain how you would modify your code to route **only these customer queries** to the primary database while sending all other read-heavy queries (e.g., analytics) to the read replica. Include a brief code snippet showing the routing logic (using SQLAlchemy sessions).",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Directly tests core role skills (read replica routing, replication lag handling) with exact tech stack (Python, SQLAlchemy, PostgreSQL); crystal clear scope (2-3 sentences, specific query types, code snippet); evaluates applied knowledge of conditional routing and trade-offs, distinguishing candidates who understand session management from those reciting basics. Minor deduction for no exact role match on 'customer queries,' but highly targeted.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-496",
        "role_title": "Junior Backend Engineer - Model Serving",
        "question": "**Question:**  \nYou're building a FastAPI endpoint to serve a pre-trained scikit-learn model for predicting house prices from JSON input (e.g., `{\"area\": 1000, \"rooms\": 3}`). The model is saved as a `joblib` file named `house_model.joblib`.\n\nWrite a minimal FastAPI code snippet (including imports) that:  \n1. Loads the model once at startup.  \n2. Accepts a POST request at `/predict` with JSON input.  \n3. Returns the prediction as JSON (e.g., `{\"prediction\": 250000.0}`).\n\nInclude a simple Pydantic model for input validation. No need for error handling or extras\u2014just the core working code.",
        "scores": {
          "relevance": 9,
          "clarity": 10,
          "discriminative": 8,
          "reasoning": "Relevance: Directly tests FastAPI (API deployment), scikit-learn/joblib (ML model loading), Pydantic (validation)\u2014core to role; minor ding as role emphasizes batching/monitoring not tested (-1). Clarity: Crystal clear, well-scoped with explicit requirements, constraints, and example formats. Discriminative: Requires applied integration of multiple tools with correct startup loading and validation; distinguishes those who can write working code from theorists.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-497",
        "role_title": "Backend Engineer - Payout Systems",
        "question": "**Question:**  \nIn our payout system, we process daily batches of marketplace settlements (e.g., 10k+ transfers) using Go and PostgreSQL, with Kafka for event streaming and Redis for caching. Walk me through how you'd design the batch processing flow to ensure exactly-once delivery of payouts, including handling duplicates, failures, and reconciliation with bank responses. What key Go libraries or patterns (e.g., idempotency) would you use, and how would you leverage Kafka/Redis/PostgreSQL?",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Relevance: Perfectly matches role's core focus on payout batch processing, settlements, validation, reconciliation, and exact tech stack (Go, PostgreSQL, Kafka, Redis). No red flags. Clarity: Crystal clear, well-scoped to batch flow with specific elements; minor deduction as no explicit time constraint for screening. Discriminative: Tests deep applied knowledge, trade-offs (duplicates, failures, exactly-once), and role-specific integration, strongly separating strong candidates.",
          "composite": 9.666666666666666
        }
      },
      {
        "role_id": "be-498",
        "role_title": "Senior Backend Engineer - Profiling Infrastructure",
        "question": "**Question:**  \nIn Pyroscope, you've collected a flame graph for a Go service showing high CPU usage in a specific goroutine. Walk me through your step-by-step process to investigate, optimize, and validate the performance issue using Pyroscope's features (e.g., baseline comparison, diffing) and integrate the fix into a Kubernetes-deployed service with Grafana dashboards for ongoing monitoring. What key Pyroscope CLI or UI commands would you use, and how would you store/query the profiling data efficiently in ClickHouse?",
        "scores": {
          "relevance": 3,
          "clarity": 6,
          "discriminative": 7,
          "reasoning": "Relevance: Superficially mentions performance analysis but assumes Pyroscope/ClickHouse expertise not in role description (build infra, flame graphs generically); red flags: tests unlisted techs (-2), generic perf process (-3). Clarity: Understandable but vague on expected depth/scope for screening, bundles multiple steps without constraints (-2). Discriminative: Tests applied knowledge well but too scenario-specific and broad (>10min, -2).",
          "composite": 5.333333333333333
        }
      },
      {
        "role_id": "be-500",
        "role_title": "Staff Engineer - Streaming Architecture",
        "question": "**Question:**  \nYou're designing a real-time user activity feed using Kafka, Redis, and WebSockets in Rust on Kubernetes. Describe how you'd implement exactly-once fan-out delivery from a single Kafka topic to 10,000+ concurrent WebSocket connections, including your approach to event schemas, deduplication, and scaling across K8s pods. What trade-offs would you make for latency vs. durability?",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Relevance: Directly targets core role skills (streaming infra, event schemas, fan-out, delivery guarantees) with Kafka (implied in streaming) and Kubernetes; minor ding for unmentioned Redis/WebSockets/Rust (-1). Clarity: Well-scoped, specific constraints, unambiguous intent for screening depth. Discriminative: Probes deep applied knowledge, trade-offs (latency/durability), architecture decisions, strongly separates experts.",
          "composite": 9.333333333333334
        }
      }
    ]
  },
  "claude-4-5-haiku": {
    "model": "claude-4-5-haiku",
    "num_samples": 100,
    "avg_relevance": 9.15,
    "avg_clarity": 8.86,
    "avg_discriminative": 9.02,
    "avg_composite": 9.010000000000003,
    "results": [
      {
        "role_id": "be-406",
        "role_title": "Senior Backend Engineer - LLM Serving",
        "question": "# Technical Screening Question\n\n**Describe a scenario where you'd need to tune the batch size and max_tokens parameters in vLLM, and walk me through your approach to finding the right balance. What metrics would you monitor, and how would you handle a situation where increasing batch size actually decreases throughput?**\n\n---\n\n## Why This Question Works:\n\n**Practical & Role-Specific:**\n- Directly relevant to the core responsibility (batching optimization)\n- Tests real-world decision-making, not memorization\n\n**Senior-Level Appropriate:**\n- Requires systems thinking (throughput vs. latency tradeoffs)\n- Expects knowledge of multiple failure modes\n- Assesses debugging methodology\n\n**Time-Bound:**\n- Can be answered substantively in 2-5 minutes\n- Allows follow-ups to probe deeper if needed\n\n**Reveals Key Competencies:**\n- Understanding of LLM serving bottlenecks (memory, compute, I/O)\n- Familiarity with vLLM's batching mechanics\n- Monitoring/observability mindset\n- Problem-solving approach under constraints\n\n---\n\n## What You're",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core focus on batching optimization and LLM serving with vLLM (no red flags). Clarity: Crystal clear and well-scoped for screening, minor deduction for lacking explicit time constraint. Discriminative: Tests deep applied knowledge, trade-offs, debugging, and metrics\u2014strong separation of senior talent from juniors.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-409",
        "role_title": "Junior Backend Engineer - File Upload",
        "question": "# Screening Question\n\n**You're building a file upload endpoint in FastAPI that accepts image files and stores them in S3. The endpoint needs to:**\n- Accept files up to 10MB\n- Validate that only PNG and JPG files are allowed\n- Return the S3 URL where the file was stored\n\n**Write the code for this endpoint, including:**\n1. The route definition with appropriate parameters\n2. File validation logic\n3. Basic S3 upload logic (you can use pseudocode/boto3 calls)\n\n---\n\n## Evaluation Criteria\n\n\u2713 **Strong answer includes:**\n- Proper use of `UploadFile` from FastAPI\n- File size and extension validation\n- Correct S3 upload implementation (boto3 `put_object` or similar)\n- Appropriate error handling/HTTP status codes\n- Returns the S3 URL\n\n\u26a0\ufe0f **Common gaps to listen for:**\n- Forgetting to validate file size before upload\n- Not checking file extensions or doing it insecurely\n- Missing error handling for S3 operations\n- Not understanding async/await patterns in FastAPI\n\n**Follow-up if",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Directly tests core file upload API skills (multipart handling, validation, S3 integration) matching role; crystal clear with precise scope, constraints, and expectations; strong separation via applied FastAPI/S3 knowledge, validation pitfalls, and error handling\u2014reveals practical ability beyond trivia.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-402",
        "role_title": "Backend Engineer - Model Deployment",
        "question": "# Technical Screening Question\n\n**You need to deploy two versions of a machine learning model to production simultaneously\u2014a stable v1 serving 90% of traffic and a new v2 serving 10% for canary testing. Both models should be accessible through a single endpoint.**\n\n**How would you implement this using KServe, and what are the key components you'd need to configure?**\n\n---\n\n## Why This Question Works:\n\n\u2705 **Practical relevance** \u2014 Directly addresses the core responsibility (deployment strategies)\n\n\u2705 **Mid-level appropriate** \u2014 Requires KServe knowledge but doesn't demand expert-level optimization details\n\n\u2705 **Reveals competency across the stack** \u2014 Tests understanding of:\n- KServe InferenceService configuration\n- Traffic splitting concepts\n- Kubernetes service routing\n- Model versioning\n\n\u2705 **Timeboxed** \u2014 Candidates can outline the approach in 2-3 minutes, with room for follow-ups on implementation details\n\n## Possible Follow-ups:\n- \"How would you monitor metrics to decide when to shift more traffic to v2?\"\n- \"What happens if v2 has high lat",
        "scores": {
          "relevance": 7,
          "clarity": 9,
          "discriminative": 8,
          "reasoning": "Relevance: Tests core deployment strategies but KServe not mentioned in role description (-2); not generic (-0). Clarity: Crystal clear, well-scoped with specific scenario and components; no red flags. Discriminative: Tests applied knowledge of configuration and trade-offs like traffic splitting, good separation for mid-level roles; timeboxed appropriately.",
          "composite": 8.0
        }
      },
      {
        "role_id": "be-403",
        "role_title": "Backend Engineer - Inference Optimization",
        "question": "# Screening Question\n\n**You're optimizing an inference pipeline for a large language model that's hitting latency SLAs in production. The model runs on a single GPU, and you're seeing:**\n- P95 latency: 850ms (SLA: 500ms)\n- GPU memory utilization: 85%\n- Average batch size: 4\n\n**Walk me through your approach to identify the bottleneck and propose 2-3 optimization strategies. What would you measure first, and why?**\n\n---\n\n## Why This Question Works\n\n**Practical scope:** Tests real-world problem-solving without requiring deep implementation details\n\n**Assesses:**\n- Systems thinking (recognizing multiple potential bottlenecks: compute, memory, batching)\n- Prioritization (what to measure first with limited time)\n- Knowledge of core optimization techniques (quantization, batching, memory optimization)\n- Understanding of the inference stack (where TensorRT/Triton would fit)\n\n**Level-appropriate:** A mid-level engineer should know *which* tools exist and *when* to use them, without needing to implement from scratch\n\n**Follow-ups available:**",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core role skills (quantization, batching, GPU memory) in realistic inference optimization scenario (-0 for no red flags). Clarity: Well-scoped with specific metrics/constraints, unambiguous intent for screening. Discriminative: Requires prioritization, systems thinking, and trade-off knowledge to distinguish strong mid-level candidates from weak ones.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-401",
        "role_title": "Backend Engineer - Training Infrastructure",
        "question": "# Screening Question\n\n**You're designing a Ray cluster that trains models on a dataset too large for a single GPU. Walk me through how you would structure the data loading and training code to efficiently distribute work across multiple GPUs. What are the key things you'd consider to avoid bottlenecks?**\n\n---\n\n## Why This Works\n\n\u2705 **Practical & Real** - Directly mirrors the job's core responsibility  \n\u2705 **Right Difficulty** - Mid-level should know distributed basics, but nuances separate strong candidates  \n\u2705 **Open-Ended** - Reveals depth: data parallelism strategies, Ray actor patterns, bottleneck awareness, Kubernetes resource requests  \n\u2705 **Time-Boxed** - Answerable in 3-4 minutes without being trivial  \n\u2705 **Follow-ups Available** - Can probe deeper on Ray task scheduling, fault tolerance, or GPU memory management\n\n**What you're listening for:**\n- Understanding of data sharding/batching across workers\n- Awareness of I/O bottlenecks (data loading shouldn't block training)\n- Mention of Ray's distributed data or actor model",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core distributed ML training skills with Ray, matching job's focus on GPU allocation and orchestration (-0 red flags). Clarity: Crystal clear, well-scoped for screening, specific context avoids ambiguity. Discriminative: Probes applied knowledge of data parallelism, bottlenecks, and Ray patterns, revealing trade-offs and depth to separate strong candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-404",
        "role_title": "Senior Backend Engineer - Feature Platform",
        "question": "# Technical Screening Question\n\n**Scenario-based question:**\n\n\"You're designing a feature store where ML teams need both batch features (computed daily) and real-time features (computed on-request). Walk me through how you'd handle a situation where a real-time feature depends on aggregations from a batch feature\u2014for example, a user's purchase count from the last 30 days.\n\nWhat would be your approach to:\n1. Computing and storing this in Feast\n2. Ensuring consistency between batch and real-time paths\n3. Handling the latency tradeoff\"\n\n---\n\n## Why this works:\n\n\u2705 **Practical** \u2013 Tests real architectural decisions in feature platforms  \n\u2705 **Seniority-appropriate** \u2013 Expects systems thinking, not just API knowledge  \n\u2705 **Tech-specific** \u2013 Naturally brings up Feast concepts (batch sources, online stores, feature views)  \n\u2705 **Open-ended** \u2013 Senior candidates can discuss tradeoffs (Redis consistency, staleness, compute costs)  \n\u2705 **Timeboxed** \u2013 Can be answered thoroughly in 3-4 minutes with follow-ups",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core feature store skills (batch/real-time computation, dependencies) for ML platform role using Feast (implied in context); minor ding for not quoting role's exact tech but perfectly aligned (-1 red flag avoided). Clarity: Well-scoped scenario with 3 focused subpoints, unambiguous, no bundling issues. Discriminative: Probes trade-offs (consistency, latency), systems thinking, and Feast application, strongly separates seniors from juniors.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-410",
        "role_title": "Backend Engineer - Bulk Operations",
        "question": "# Screening Question\n\n**Design a bulk update API endpoint that processes up to 10,000 records. Walk me through how you'd handle:**\n\n1. **Partial failures** \u2014 some records succeed, others fail validation or hit conflicts\n2. **Idempotency** \u2014 the same request submitted twice shouldn't create duplicate side effects\n3. **Client feedback** \u2014 how would you return results to the caller?\n\n**You can use pseudocode or describe your approach. What would be your main design decisions?**\n\n---\n\n## Why This Question Works\n\n- **Practical:** Tests real-world bulk operation challenges they'd face on day one\n- **Reveals depth:** Answers expose knowledge of idempotency keys, transaction boundaries, error handling patterns, and API design\n- **Mid-level appropriate:** Doesn't require exotic knowledge; a solid mid-level engineer should have encountered these problems\n- **Conversational:** Opens dialogue\u2014you can dig into their Redis/Kafka choices, PostgreSQL transaction strategy, or Go concurrency patterns based on their answer\n- **Time-boxed:** Candidates can sketch a solid answer in 2-5 minutes",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core needs (bulk ops, partial success, idempotency). Clarity: Crystal clear with specific scope (10k records), well-scoped for screening; minor deduction for no explicit time/format constraint. Discriminative: Tests applied design trade-offs (idempotency keys, transactions, error aggregation), reveals depth without being trivial or impossible.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-405",
        "role_title": "Senior Backend Engineer - Model Monitoring",
        "question": "# Screening Question\n\n**You're implementing a monitoring system for a production recommendation model. Over the past week, your metrics show the model's precision dropped from 0.92 to 0.78, but the feature distributions look virtually identical to training data. What are the most likely causes you'd investigate first, and how would you approach diagnosing this systematically?**\n\n---\n\n## Why This Question Works\n\n**Tests practical expertise:**\n- Distinguishes between data drift vs. model/prediction drift (common senior-level insight)\n- Requires knowledge of what monitoring actually catches and what it misses\n- Shows systematic debugging approach for production issues\n\n**Appropriate for seniority:**\n- Not a gotcha question\u2014open-ended to reveal depth of experience\n- Senior engineer should discuss multiple hypotheses, not just one answer\n- Tests judgment about what to prioritize (efficiency matters in production)\n\n**Realistic scenario:**\n- Common real-world situation that trips up junior engineers\n- Directly relevant to the role's core responsibility\n\n**Reveals:**\n- Model monitoring maturity (do they know feature drift \u2260 performance drift?)\n- Production troubleshooting experience\n- Whether they",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core focus on model monitoring, drift detection, and performance tracking; no red flags. Clarity: Crystal clear scenario with precise metrics, well-scoped for screening; minor deduction as no explicit time/format constraint. Discriminative: Tests deep applied knowledge of drift types, prioritization, and systematic diagnosis, strongly separates seniors from juniors via trade-offs and hypotheses.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-407",
        "role_title": "Staff Backend Engineer - ML Infrastructure",
        "question": "# Screening Question\n\n**You're tasked with designing a training platform where data scientists can submit jobs that need to scale from a single GPU to 100+ GPUs across a cluster. Walk me through how you'd approach this problem\u2014what are the key architectural decisions you'd make, and what trade-offs would you consider?**\n\n---\n\n## Why This Works:\n\n**Tests practical, staff-level expertise:**\n- Requires understanding of distributed systems (Ray/Spark fundamentals)\n- Assesses platform thinking beyond just coding\n- Reveals experience with real scaling challenges\n- Tests cost/complexity trade-offs (staff engineers optimize for this)\n\n**Appropriate for seniority:**\n- Not a gotcha question; experienced engineers have concrete opinions\n- Open-ended enough to see their decision-making process\n- Allows them to showcase leadership perspective (defining standards, trade-offs)\n\n**Clear signal in the answer:**\n- Do they mention fault tolerance, checkpointing, resource scheduling?\n- Do they think about DX (data scientist experience)?\n- Do they consider observability, cost monitoring?\n- Do they acknowledge Ray/Spark trade-offs knowingly?\n\n**Time-",
        "scores": {
          "relevance": 7,
          "clarity": 6,
          "discriminative": 8,
          "reasoning": "Relevance: Tests core ML infra scaling and platform design directly matching role, but generic distributed training (no specific tech stack like role implies) and lacks cost optimization emphasis (-1 generic, -1 misses specifics). Clarity: Clear intent for architectural approach but 'design a system' without time/format/constraints/scope (e.g., team size, budget) (-2 red flag), vague depth. Discriminative: Strong on trade-offs, applied knowledge, and deep signals like fault tolerance/DX/observability, good separation for staff-level.",
          "composite": 7.0
        }
      },
      {
        "role_id": "be-408",
        "role_title": "Junior Backend Developer - Health Endpoints",
        "question": "# Screening Question\n\n**Describe the difference between a liveness probe and a readiness probe in Kubernetes. Why would you implement them differently, and what might each endpoint return in Go?**\n\n---\n\n## Why This Question Works:\n\n\u2713 **Practical & Role-Relevant:** Directly addresses the core responsibility (health endpoints)\n\n\u2713 **Right Difficulty Level:** Junior developers should know this\u2014it's foundational Kubernetes knowledge, not advanced\n\n\u2713 **Multi-Part Assessment:** Tests understanding of:\n- Kubernetes concepts (liveness vs readiness)\n- Implementation reasoning (why they differ)\n- Go coding ability (what the endpoints look like)\n\n\u2713 **2-5 Minute Response:** Candidates can sketch a quick answer without deep system design\n\n\u2713 **Open-Ended:** Reveals depth of understanding\u2014a junior might say \"they're different states,\" while someone stronger explains cascading failures or dependency checks\n\n---\n\n## What You're Looking For:\n\n- **Good answer:** Explains that liveness = \"is the app running?\" and readiness = \"can it handle traffic?\"\n- **Better answer:** Mentions that readiness checks",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core focus on health check endpoints (liveness/readiness) in Kubernetes/Go. Clarity: Crystal clear, well-scoped multi-part question for screening, minor vagueness on exact Go return format but not deducting. Discriminative: Excellent separation via conceptual understanding, reasoning (trade-offs like cascading failures), and implementation; reveals depth without being recall or trivial. No red flags.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-411",
        "role_title": "Backend Engineer - Admin APIs",
        "question": "# Technical Screening Question\n\n## Design an audit logging system for an admin API\n\n**Scenario:**\nYou're building an admin API where privileged operations (user suspension, permission changes, data exports) must be logged for compliance. The system needs to:\n- Record *what* action was performed, *who* performed it, and *when*\n- Handle high volume (thousands of operations/day)\n- Allow querying audit logs by user, action type, and date range\n- Ensure logs cannot be tampered with\n\n**Your task:**\n1. Sketch a database schema for storing audit logs\n2. Explain how you'd integrate logging into your FastAPI endpoints (show a brief code example or pseudocode)\n3. Discuss one trade-off you'd make between completeness and performance\n\n---\n\n## What This Tests\n\n\u2713 **Practical API design** - How they structure data for a real constraint  \n\u2713 **Python/FastAPI knowledge** - Middleware, decorators, or dependency injection patterns  \n\u2713 **Database thinking** - Schema design, indexing, query patterns for compliance use cases  \n\u2713 **Mid-level judgment** - Trade-",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core role needs (audit logging for admin APIs), tests FastAPI (implied in role context), no red flags (-0). Clarity: Well-scoped scenario with specific requirements, single focused design task with clear subtasks, no bundling issues. Discriminative: Requires applied schema design, integration knowledge, and trade-off analysis, strongly separates mid-level thinkers via practical judgment.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-413",
        "role_title": "Senior Backend Engineer - API Gateway Logic",
        "question": "# Screening Question\n\n**You're implementing a custom Envoy filter in Go that needs to:**\n1. Extract a user ID from request headers\n2. Apply different rate limits based on user tier (stored in a shared cache)\n3. Add response headers with latency metrics\n4. Reject requests that exceed their tier's limit with a 429 status\n\n**Walk me through your approach:** How would you structure this filter? What are the key extension points in Envoy you'd use, and how would you handle the cache lookup without blocking the request path?\n\n---\n\n## Why This Question Works\n\n\u2713 **Practical & Real** \u2014 Actual API gateway use case (rate limiting, header manipulation, caching)  \n\u2713 **Senior-Level** \u2014 Requires understanding of Envoy architecture, concurrency patterns, and performance trade-offs  \n\u2713 **Tech Stack** \u2014 Tests knowledge of Envoy filters and Go integration  \n\u2713 **Concise** \u2014 Answerable in 3-4 minutes with room for follow-ups  \n\u2713 **Reveals Depth** \u2014 Response shows whether they understand filter lifecycle, async operations, and gateway",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests Envoy filters, Go, request transformation, routing logic, and caching - core to API gateway extensions role (-0 red flags). Clarity: Precise tasks with specific non-blocking constraint, unambiguous for screening scope. Discriminative: Probes architecture, concurrency trade-offs, and Envoy extension points, strongly separates experienced candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-412",
        "role_title": "Backend Engineer - Analytics APIs",
        "question": "# Technical Screening Question\n\n**You're building an analytics API endpoint that returns daily revenue aggregations for the past year. The query currently takes 8 seconds, which is too slow for dashboard load times. Walk me through your approach to optimize this\u2014what would you investigate first, and what specific strategies would you consider?**\n\n---\n\n## Why This Question Works:\n\n**Practical Relevance:** Directly mirrors the core responsibility of aggregating and caching time-series data\n\n**Seniority Appropriate:** \n- Mid-level engineers should recognize multiple optimization layers (query-level, caching, architecture)\n- Not expecting mastery of all, but strategic thinking across the stack\n\n**Reveals Key Competencies:**\n- Understanding of ClickHouse query optimization (sampling, pre-aggregation, partitioning)\n- Redis caching strategy decisions (TTL, key design, invalidation)\n- Go API design (pagination, response formatting)\n- Diagnostic thinking (where to start troubleshooting)\n\n**Time Efficient:** Candidate can outline approach in 2-3 minutes, with room for follow-up depth based on their answers\n\n**Natural Follow-ups:**",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (aggregation queries, time-series, caching) without generic flaws (-0 red flags). Clarity: Crystal clear scenario, well-scoped for screening, no ambiguity or missing constraints. Discriminative: Probes diagnostic thinking, trade-offs (query opt, caching strategies), distinguishes strategic depth from superficial answers; 2-3 min response fits perfectly.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-415",
        "role_title": "Senior Backend Engineer - Multipart APIs",
        "question": "# Screening Question\n\n**You're designing a resumable upload API for large files (up to 10GB). A client uploads a file in 5MB chunks, but the connection drops after chunk 3. When the client reconnects, how would you:**\n\n1. **Identify which chunks were successfully persisted?**\n2. **Handle the case where chunk 4 was partially written to S3?**\n3. **Decide whether to use S3's multipart upload API or manage chunks yourself?**\n\n**What trade-offs would you consider between complexity, cost, and reliability?**\n\n---\n\n## Why This Works\n\n\u2705 **Practical & Real** \u2014 Directly mirrors the job responsibility  \n\u2705 **Senior-level** \u2014 Requires architectural thinking, not just syntax  \n\u2705 **Concise** \u2014 Can be answered in 3-4 minutes with good follow-ups  \n\u2705 **Multi-faceted** \u2014 Tests state management, AWS knowledge, and trade-off analysis  \n\u2705 **Room to dig deeper** \u2014 You can probe their reasoning on idempotency, metadata storage, cleanup logic, etc.",
        "scores": {
          "relevance": 7,
          "clarity": 7,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core resumable upload/chunk skills but assumes S3/AWS (not explicitly listed, -2) and bundles multiple sub-questions (-1 from red flags). Clarity: Clear scenario but multiple bundled questions (-2) make it less focused for screening. Discriminative: Excellent on trade-offs, applied AWS knowledge, and edge cases (partial writes, resumption), strongly separates seniors.",
          "composite": 7.666666666666667
        }
      },
      {
        "role_id": "be-416",
        "role_title": "Staff Backend Engineer - External APIs",
        "question": "# Technical Screening Question\n\n**You're designing an external API for partners that need real-time access to customer transaction data. Some partners are high-volume, others are occasional users. Walk me through how you'd approach the rate limiting and monetization strategy for this API\u2014what would you consider, and what trade-offs would you make?**\n\n---\n\n## Why This Question Works:\n\n**Tests practical, staff-level thinking:**\n- Requires understanding of API gateway capabilities (Kong is their tech stack)\n- Demonstrates monetization strategy knowledge (key requirement)\n- Shows partner enablement thinking (tiered access, fairness)\n- Reveals architectural decision-making at scale\n\n**Allows for 2-5 minute response** covering:\n- Rate limiting approaches (fixed vs. token bucket, per-tier quotas)\n- Monetization models (usage-based, tier-based, hybrid)\n- Technical implementation (where enforcement happens\u2014Kong, application layer)\n- Trade-offs (complexity vs. fairness, cost recovery vs. partner friction)\n\n**Differentiates staff-level candidates** by how they discuss:\n- Partner friction and business implications\n- System design trade-offs",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core API strategy, monetization, and partner enablement; aligns perfectly with role despite not naming Kong (-1 minor). Clarity: Crystal clear, well-scoped for screening with explicit focus on rate limiting/monetization and trade-offs. Discriminative: Excellent at separating staff-level candidates via trade-offs, business implications, and architectural depth; not recall-based.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-418",
        "role_title": "Junior Backend Engineer - Service Clients",
        "question": "# Screening Question\n\n**You're building a gRPC client that calls a downstream service. The service occasionally times out or returns temporary errors. Walk me through how you'd implement a basic retry mechanism in Go. What would you consider to prevent making things worse?**\n\n---\n\n## Why This Question Works\n\n\u2705 **Practical & Real** \u2013 Junior engineers actually build this  \n\u2705 **Right Scope** \u2013 Tests Go basics + distributed systems thinking in 2-5 min  \n\u2705 **Reveals Depth** \u2013 Answer shows if they think beyond \"just retry\":\n- Exponential backoff vs fixed delays\n- Max retries / circuit breaker concepts\n- Idempotency awareness\n- Timeout configuration\n\n\u2705 **Appropriate Level** \u2013 Doesn't require advanced patterns, but rewards good thinking\n\n---\n\n## What You're Listening For\n\n**Strong signals:**\n- Mentions exponential backoff or jitter\n- Considers max retry limits\n- Thinks about idempotency (\"don't retry if it's not safe\")\n- References context deadlines in Go\n\n**Red flags:**\n- \"Just retry forever\"\n- No mention",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests Go implementation of retries for service clients, core to role; minor ding as gRPC unspecified but aligns perfectly (-1 no red flags). Clarity: Crystal clear, well-scoped for screening, single focused question with practical context. Discriminative: Excellent at separating via trade-offs (backoff, idempotency, limits), reveals depth without being trivial or overly broad.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-420",
        "role_title": "Backend Engineer - Graceful Shutdown",
        "question": "# Screening Question\n\n**You're deploying a Go backend service to Kubernetes that handles both gRPC and HTTP requests. During a rolling update, you need to ensure in-flight requests complete gracefully before the pod terminates. Walk me through how you'd implement graceful shutdown\u2014specifically, how you'd handle the signal, drain new connections, and wait for existing requests to finish. What are the key timing considerations with Kubernetes' termination lifecycle?**\n\n---\n\n## Why This Works\n\n\u2705 **Practical & Real**: Directly addresses the role's core responsibility  \n\u2705 **Right Scope**: Answerable in 3-4 minutes without being trivial  \n\u2705 **Tests Understanding**: Requires knowledge of:\n- Go signal handling (`context.Context`, `syscall.SIGTERM`)\n- Kubernetes termination flow (SIGTERM \u2192 grace period \u2192 SIGKILL)\n- Connection/request draining mechanics\n- Trade-offs (grace period timeout vs. request latency)\n\n\u2705 **Mid-Level Appropriate**: Expects practical implementation knowledge, not architectural perfection\n\n---\n\n## Strong Answer Signals\n\n- Mentions `signal.",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core focus on graceful shutdown, signals, and in-flight requests in Go/K8s context (+no red flags). Clarity: Crystal clear, well-scoped for screening, specific components listed (minor deduction for lacking explicit time/format constraints). Discriminative: Tests applied knowledge, trade-offs, and integration of Go signals/K8s lifecycle, strongly separates candidates with practical depth.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-419",
        "role_title": "Backend Engineer - Retry Logic",
        "question": "# Screening Question\n\n**You're designing a retry mechanism for failed Kafka message processing in your Go service. Messages that fail processing should be retried with exponential backoff, but you also need to prevent retry storms that could overwhelm your system.**\n\n**Walk me through how you would implement exponential backoff with jitter, and explain how you'd implement a retry budget to prevent cascading failures. What would you consider when setting these parameters?**\n\n---\n\n## Why This Question Works\n\n**Tests practical distributed systems knowledge:**\n- Exponential backoff implementation (core skill)\n- Jitter concept (prevents thundering herd)\n- Retry budgets (prevents cascading failures)\n- Parameter tuning reasoning (judgment at mid-level)\n\n**Appropriate for mid-level:**\n- Not asking to code it end-to-end (that's senior)\n- Not trivia (they need to think through trade-offs)\n- Evaluates real architectural decisions they'd make\n\n**Reveals important signals:**\n- Do they understand *why* jitter matters?\n- Can they think about resource constraints?\n- Do they consider monitoring/observability?\n- Can they balance reliability",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core focus on retry infrastructure, backoff, jitter, and budgets in Go/Kafka context (+no red flags). Clarity: Crystal clear and well-scoped for screening, minor deduction for unspecified constraints on parameters. Discriminative: Tests deep trade-offs, judgment, and applied knowledge (why jitter, tuning), strong separation without being too broad.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-414",
        "role_title": "Senior Backend Engineer - Long-Polling",
        "question": "# Technical Screening Question\n\n**Design a long-polling endpoint in Go that handles async job status updates. Walk me through:**\n\n1. **How you'd structure the HTTP handler** to accept a poll request and avoid blocking the entire server\n2. **Your timeout strategy** \u2014 how long should you wait before responding, and how do you prevent client/server resource exhaustion?\n3. **How you'd integrate Redis** to check for job completion without hammering PostgreSQL\n4. **One concrete edge case** you'd need to handle and your solution for it\n\n---\n\n## Why This Works\n\n\u2705 **Practical & Scenario-Based** \u2014 Tests real architectural decisions, not memorization\n\n\u2705 **Multi-Layered** \u2014 Assesses Go concurrency, caching patterns, system design thinking, and edge case awareness\n\n\u2705 **Senior-Level** \u2014 Expects thoughtful tradeoffs (timeout tuning, resource management) rather than just \"here's the code\"\n\n\u2705 **Appropriate Scope** \u2014 Answerable with a whiteboard/verbal explanation in 3-4 minutes, room for depth if they know their stuff\n\n\u2705 **Role-",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (long-polling APIs, connection mgmt, timeouts, fallbacks) with Go, Redis, PostgreSQL integration; minor ding as Go not explicitly in desc (-1). Clarity: Well-structured, numbered parts with precise scope, no bundling issues. Discriminative: Probes trade-offs, architecture, integration, and edge cases for strong candidate separation; appropriately scoped for screening.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-417",
        "role_title": "Junior Backend Developer - Event Sourcing",
        "question": "# Screening Question\n\n**\"Walk me through how you would design an event handler for a `UserRegistered` event in an event sourcing system. Specifically:**\n- **What information would the event contain?**\n- **What would the handler do with that event?**\n- **How would you ensure the same event isn't processed twice?\"**\n\n---\n\n## Why This Question Works\n\n**Tests practical understanding:**\n- Event structure and design (domain knowledge)\n- Handler responsibility (core pattern)\n- Idempotency (distributed systems challenge)\n\n**Appropriate for junior level:**\n- No trick answers or obscure knowledge required\n- Can be answered with a simple, concrete example\n- Shows if they've thought about real implementation details\n\n**Conversational flow:**\n- Allows follow-ups (Kafka partitions? Database constraints? Projection updates?)\n- Easy to gauge depth without being overwhelming\n- Natural discussion about event sourcing fundamentals\n\n---\n\n## What You're Listening For\n\n\u2705 **Good signals:** Event payload clarity, handler side effects, mention of idempotency keys or constraints  \n\u26a0\ufe0f **Watch for:** Confusion between events an",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 8,
          "reasoning": "Relevance: Directly tests core event sourcing skills (event handlers, structure) for the role, no red flags (-0). Clarity: Crystal clear, well-scoped bullet points, unambiguous for screening (-0). Discriminative: Tests applied knowledge and trade-offs like idempotency, distinguishes practical understanding from rote knowledge, junior-appropriate (-0).",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-421",
        "role_title": "Senior Backend Engineer - Security Automation",
        "question": "# Screening Question\n\n**Describe how you would design a secrets rotation mechanism for database credentials across a Kubernetes cluster using HashiCorp Vault. Walk me through your approach, including:**\n\n1. **How would you trigger rotation?** (timing, events, manual vs. automated)\n2. **How would you handle in-flight connections** during rotation to minimize disruption?\n3. **What would you implement to prevent a failed rotation from leaving the system in an inconsistent state?**\n\n---\n\n## Why This Question Works\n\n\u2713 **Practical & Real-World** \u2014 directly relevant to the role's core responsibility of secrets management\n\n\u2713 **Senior-Level** \u2014 expects architectural thinking and understanding of failure modes, not just \"how to use Vault\"\n\n\u2713 **Multi-Dimensional** \u2014 tests knowledge of orchestration, state management, and operational concerns simultaneously\n\n\u2713 **Time-Appropriate** \u2014 can be answered substantively in 2-5 minutes with follow-ups available\n\n\u2713 **Tech-Stack Aligned** \u2014 naturally incorporates Vault, Kubernetes, and automation concepts\n\n---\n\n## What You're Evaluating\n\n- Understanding of secrets",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's secrets rotation focus, Vault/K8s alignment, no red flags. Clarity: Crystal clear with specific subpoints, minor deduction for lacking explicit time/format constraints. Discriminative: Tests deep architectural trade-offs, failure handling, and ops concerns, strongly separates seniors from juniors.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-422",
        "role_title": "Junior Backend Engineer - API Testing",
        "question": "# Technical Screening Question\n\n**Write a simple pytest test for a FastAPI endpoint that creates a user. The endpoint accepts a POST request with `name` and `email` fields, returns the created user with an `id`, and should return a 400 status code if `email` is missing.**\n\n**Requirements:**\n- Write the test code (you can assume the FastAPI app is imported as `app`)\n- Include any necessary test setup/fixtures you'd use\n- Explain what your test is validating\n\n---\n\n## Why This Question Works\n\n\u2705 **Practical & Role-Relevant:** Tests core skills (pytest, FastAPI, API testing)  \n\u2705 **Time-Appropriate:** Solvable in 2-5 minutes  \n\u2705 **Junior-Level:** Doesn't require advanced mocking or complex patterns  \n\u2705 **Reveals Understanding:** Shows familiarity with:\n- Test structure and assertions\n- HTTP status codes\n- Request/response handling\n- Basic test organization\n\n**Follow-ups** (if needed):\n- \"How would you test the database was actually updated?\"\n- \"What if you needed to test 10",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 8,
          "reasoning": "Relevance: Directly tests pytest, FastAPI, and API testing - core to entry-level API test engineer role; no red flags. Clarity: Crystal clear with specific endpoint, requirements, and scoped for screening; assumes app import avoids ambiguity. Discriminative: Tests applied knowledge of test structure, fixtures, assertions, and HTTP handling to separate juniors with practical skills from weak ones; not pure recall but lacks trade-offs for 10.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-423",
        "role_title": "Staff Engineer - Incident Response Platform",
        "question": "# Screening Question\n\n**Tell me about a time you designed or improved an on-call system or incident response process. Walk me through:**\n\n1. **What problem were you solving?** (alert fatigue, slow response times, unclear escalation, etc.)\n2. **What metrics or SLOs did you use to measure success?**\n3. **What was the hardest part to get right, and how did you handle the tradeoffs?**\n\n---\n\n## Why This Works\n\n\u2713 **Open-ended** \u2013 Reveals their actual incident response experience and maturity  \n\u2713 **Multi-layered** \u2013 Tests problem-solving, SLO thinking, and pragmatism  \n\u2713 **Staff-level signal** \u2013 Staff engineers own systems holistically; this reveals whether they think about metrics, tradeoffs, and operational reality  \n\u2713 **Conversational** \u2013 Natural follow-ups emerge (escalation logic, automation, tooling choices, team buy-in)  \n\u2713 **Tech-agnostic** \u2013 Their answer matters more than specific tools they've used  \n\n**What you're listening for:**\n- Did they measure the impact",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Directly targets core role responsibilities (incident response, on-call optimization, SLO tracking) with behavioral format testing holistic ownership. Crystal clear structure with specific prompts avoids ambiguity. Strong differentiation via tradeoffs, metrics, and real-world problem-solving; staff-level depth without being generic or recall-based. Minor ding for behavioral over pure technical design but aligns perfectly with role's operational focus.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-425",
        "role_title": "Senior Backend Engineer - Graph Analytics",
        "question": "# Technical Screening Question\n\n**You're building a recommendation engine that needs to find \"friends of friends\" connections across a social network with 500 million users and 10 billion edges in Neo4j. A naive Cypher query doing multi-hop traversals is timing out.**\n\n**Walk me through how you'd approach optimizing this:**\n1. What specific Neo4j performance issues would you investigate first?\n2. Would you consider moving this workload to Spark, and if so, when/why?\n3. How would you use caching (Redis) strategically here without creating stale data problems?\n\n---\n\n## Why This Question Works\n\n\u2713 **Practical & Real** - Directly mirrors the job's core challenge (billion-edge scale, recommendation systems)\n\n\u2713 **Multi-layered** - Tests understanding of Neo4j query optimization, Spark's role in the stack, and caching strategy trade-offs\n\n\u2713 **Senior-level** - Requires architectural thinking beyond syntax; expects them to weigh trade-offs, not just recite solutions\n\n\u2713 **Time-appropriate** - Can be answered in 3-4 minutes with a",
        "scores": {
          "relevance": 7,
          "clarity": 7,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core graph traversal/rec skills at scale and Neo4j (7-8 range), but penalize -2 for Spark/Redis not in role desc (-2). Clarity: Clear intent and structured, but multiple bundled questions (-2) and vague scoping. Discriminative: Excellent on trade-offs/caching strategies at senior level, strong separation.",
          "composite": 7.666666666666667
        }
      },
      {
        "role_id": "be-424",
        "role_title": "Backend Engineer - Message Serialization",
        "question": "# Technical Screening Question\n\n**You're designing a message serialization system where producer services publish events to Kafka. Your schema needs to evolve: you're adding a new required field `userId` to an existing event that previously only had `eventId` and `timestamp`.**\n\n**Walk me through:**\n1. **How would you handle this schema change to ensure consumers aren't broken?**\n2. **What's the difference between backward and forward compatibility here, and which one matters for your scenario?**\n3. **If you're using Avro with Schema Registry, what would you actually do to deploy this change safely?**\n\n---\n\n## Why This Question Works\n\n\u2713 **Tests practical understanding** - Not asking to define terms, but apply them to a real problem  \n\u2713 **Mid-level appropriate** - Assumes familiarity with Kafka/Avro, not mastery of edge cases  \n\u2713 **Reveals key competencies:**\n- Schema evolution strategy (backward vs forward compatibility)\n- Knowledge of schema registry patterns\n- Consideration of producer/consumer deployment order\n- Understanding of breaking vs non-breaking changes\n\n\u2713 **Conversational** -",
        "scores": {
          "relevance": 7,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Tests core schema evolution/compatibility skills for the role and mentions Kafka/Avro (inferred from role's serialization focus), but deduct -3 for being 3 bundled questions (-2) and somewhat generic serialization pattern (-1). Clarity: Clear scenario and intent, well-scoped, but multiple questions bundled (-2). Discriminative: Excellent at testing applied trade-offs, strategies, and tools to separate strong candidates.",
          "composite": 8.0
        }
      },
      {
        "role_id": "be-426",
        "role_title": "Junior Backend Engineer - DevOps Scripting",
        "question": "# Screening Question\n\n**You need to create a simple deployment script that:**\n1. Pulls the latest code from a GitHub repository\n2. Builds a Docker image\n3. Runs a basic health check on the container\n4. Reports success or failure\n\n**Write pseudocode or actual code (Python or Bash) showing how you'd structure this script. What error handling would you include, and why?**\n\n---\n\n## Why This Question Works\n\n\u2705 **Practical** \u2013 Tests real DevOps tasks they'll do daily  \n\u2705 **Appropriate Level** \u2013 Doesn't require advanced knowledge, but reveals fundamentals  \n\u2705 **Open-Ended** \u2013 Shows their thinking: error handling, logging, script structure  \n\u2705 **Time-Bound** \u2013 Can sketch this in 2-5 minutes  \n\u2705 **Reveals Gaps** \u2013 Junior candidates often forget error handling or health checks; strong juniors think about both\n\n## What You're Looking For\n\n- **Basic structure:** Sequential steps, clear logic\n- **Error handling:** Do they check if git pull/docker build succeeded?\n- **Health check thinking:** How do they",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core skills (Git, Docker, scripting for deployments) with no red flags. Clarity: Crystal clear, well-scoped steps, minor deduction for not specifying language preference explicitly. Discriminative: Strong separation via error handling and reasoning, reveals junior gaps in fundamentals without being trivial or overly broad.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-427",
        "role_title": "Backend Engineer - Feature Store",
        "question": "# Technical Screening Question\n\n**Scenario-based question:**\n\n\"You're designing a feature store where you need to serve both batch features for model training and real-time features for inference. A data scientist requests a feature that takes 30 seconds to compute from your data warehouse, but it needs to be available in <100ms for online serving.\n\nWalk me through how you'd approach this problem using Feast. What are the trade-offs you'd consider, and what would you recommend?\"\n\n---\n\n## Why this works:\n\n\u2713 **Practical & real** - Core feature store problem  \n\u2713 **Mid-level appropriate** - Tests architectural thinking, not deep Feast internals  \n\u2713 **Open-ended** - Reveals how they think about latency, caching, and design decisions  \n\u2713 **Timeboxed** - Can explore thoroughly in 3-4 minutes  \n\n**What you're evaluating:**\n- Understanding of batch vs. real-time serving gap\n- Knowledge of Feast's materialization + online store strategy\n- Awareness of caching/pre-computation trade-offs\n- Problem-solving approach (pre-compute? cache",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Directly tests core feature store skills (batch/real-time serving, computation, versioning implied) matching role; Feast aligns perfectly as role focuses on feature store infra. Crystal clear scenario with specific constraints, well-scoped for screening. Strong separation via trade-offs/caching reasoning, Feast strategies distinguish mid-level expertise without trivia. Minor ding: assumes Feast familiarity but it's spot-on for role.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-428",
        "role_title": "Senior Backend Engineer - Payment Orchestration",
        "question": "# Technical Screening Question\n\n**Design a payment provider failover system for a payment orchestration platform. You have 3 payment providers (A, B, C) with different success rates (95%, 92%, 88%), costs, and latencies. A transaction fails with provider A. How would you:**\n\n1. **Decide which provider to failover to?** What factors beyond success rate would you consider?\n2. **Prevent cascading failures?** (e.g., avoid repeatedly routing to a temporarily degraded provider)\n3. **Track this decision** so you can optimize routing over time?\n\n*Assume you have access to: transaction metadata, provider health metrics, PostgreSQL, Redis, and Kafka.*\n\n---\n\n## Why This Question Works\n\n- **Practical & Real**: Directly mirrors the core responsibility (routing + failover)\n- **Senior-Level Depth**: Expects understanding of tradeoffs (cost vs. reliability), circuit breaker patterns, and observability\u2014not just \"use the best provider\"\n- **Tech Stack Integration**: Naturally leads to discussing Redis (circuit breaker state), Kafka (event logging), PostgreSQL (analytics), and Go (implementation",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core focus on failover, routing, optimization, and tracking; integrates exact tech stack. Clarity: Well-scoped with specifics (3 providers, metrics, tech access), minor deduction for bundled sub-questions. Discriminative: Probes deep trade-offs, patterns (circuit breakers), and observability, strongly separates seniors from juniors.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-429",
        "role_title": "Backend Engineer - Log Aggregation",
        "question": "# Screening Question\n\n**You're designing a log aggregation pipeline that needs to handle 2M events/second. Logs arrive via Kafka, get parsed and enriched by Vector, then stored in ClickHouse.**\n\n**Walk me through how you'd handle a scenario where:**\n- **Parsing fails on 5% of incoming logs** (malformed JSON, unexpected fields)\n- **You need to retain raw logs for debugging but also ensure failed logs don't block the pipeline**\n\n**What would you implement, and why?**\n\n---\n\n## Why This Question Works\n\n**Tests practical depth:**\n- Understanding of real failure modes in log pipelines\n- Knowledge of Vector's error handling capabilities\n- ClickHouse schema design decisions (separate tables? columns?)\n- Go considerations if writing custom handling\n\n**Seniority-appropriate:**\n- Mid-level should know basic patterns but may not have all details polished\n- Room to discuss trade-offs (buffering, dead-letter queues, alerting)\n- Acceptable to say \"I'd research Vector's exact features, but here's my approach...\"\n\n**Observable reasoning:**\n- Listen for: asynchronous handling,",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core log pipeline skills (parsing, enrichment, failure handling, retention) with exact tech stack (Kafka, Vector, ClickHouse); no red flags (-0). Clarity: Crystal clear scenario, well-scoped with specific rates and goals; no bundling or vagueness (-0). Discriminative: Probes deep trade-offs (DLQs, buffering, async, alerting, schema design), distinguishes applied knowledge from rote recall; senior-appropriate depth without being too broad (-0).",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-430",
        "role_title": "Junior Backend Engineer - Search Indexing",
        "question": "# Technical Screening Question\n\n**\"Walk me through how you would index a list of 10,000 product records from a PostgreSQL database into Elasticsearch. What would you consider important, and what potential issues might you run into?\"**\n\n## Why This Question Works:\n\n**Tests practical knowledge:**\n- Understanding of data movement between systems\n- Awareness of bulk indexing (not indexing one-by-one)\n- Basic mapping/schema thinking\n\n**Appropriate for junior level:**\n- No requirement for advanced optimization\n- Open-ended enough to show thinking process\n- Candidates can discuss real concerns they'd encounter\n\n**Reveals important gaps:**\n- Do they know about batch/bulk APIs?\n- Are they aware of memory/performance constraints?\n- Do they think about data transformation?\n- Can they identify common failure modes (timeout, connection issues, invalid data)?\n\n**Follow-up prompts** (if needed):\n- \"How would you handle documents that fail to index?\"\n- \"What if the data keeps growing daily\u2014how would you keep the index updated?\"\n- \"Why wouldn't you just index records one at a time?\"\n\n---\n\n*Expected response time: 3",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core skills (bulk indexing from PostgreSQL to Elasticsearch, mapping) for entry-level search indexing role; minor ding as role emphasizes learning but question fits perfectly (-1 generic feel avoided). Clarity: Crystal clear, well-scoped open-ended walkthrough with practical focus, no ambiguity or missing constraints. Discriminative: Excellent at revealing practical understanding, trade-offs (batch vs single, issues like memory/failures), and thought process to separate juniors with real knowledge from weak ones.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-432",
        "role_title": "Backend Engineer - Data Validation",
        "question": "# Technical Screening Question\n\n**Scenario-based question:**\n\n\"You're building a data validation checkpoint in Great Expectations for a daily customer transaction pipeline in S3. The pipeline should catch:\n- Missing values in the `transaction_id` column\n- Transaction amounts that are statistical outliers (beyond 3 standard deviations)\n- Schema changes (new or dropped columns)\n\nWalk me through how you'd set this up. What Great Expectations components would you use, and how would you handle the case where a legitimate spike in transaction amounts (like a holiday sale) triggers false alerts?\"\n\n---\n\n## Why this question works:\n\n\u2705 **Practical** \u2013 Real scenario mid-level engineers face  \n\u2705 **Multi-layered** \u2013 Tests GE knowledge + data validation thinking + problem-solving  \n\u2705 **Seniority-appropriate** \u2013 Expects familiarity with GE but allows room to explore reasoning  \n\u2705 **Time-bound** \u2013 Can be thoroughly answered in 3-4 minutes  \n\u2705 **Reveals depth** \u2013 Follow-up on the \"false alerts\" part shows if they understand monitoring trade-offs",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core data validation (schema, stats, missing values) and lineage-adjacent pipeline quality using Great Expectations, perfectly matching role; no red flags. Clarity: Crystal clear scenario with specific requirements and well-scoped follow-up; no ambiguity or bundling. Discriminative: Tests applied GE knowledge, setup reasoning, and trade-offs (false positives), strongly separating candidates with practical depth.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-431",
        "role_title": "Staff Engineer - Real-time Collaboration",
        "question": "# Technical Screening Question\n\n**Design a real-time presence system for a collaborative editor where multiple users can see who else is currently viewing the document. How would you handle:**\n\n1. **User join/leave events** \u2014 What's your approach to detecting and broadcasting these events reliably?\n2. **Network failures** \u2014 How would you distinguish between a user who temporarily disconnected vs. one who left intentionally?\n3. **Scalability trade-offs** \u2014 Would you use Redis pub/sub, a presence channel pattern, or something else? Why?\n\n**Follow-up based on their answer:** If your solution uses heartbeats/timeouts, walk me through how you'd tune the timeout window and what happens if it's too aggressive or too lenient.\n\n---\n\n## Why This Question Works\n\n- **Seniority-appropriate**: Tests architectural thinking and trade-off reasoning, not implementation details\n- **Practical**: Real problem they'd solve day-one; not theoretical CRDT minutiae\n- **Reveals depth**: Their answer shows understanding of distributed systems, operational concerns, and when to use which tools\n- **Conversational**: Natural follow-ups exist; you can dig into their",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Directly tests core real-time collaboration skills (presence systems) from role description. Crystal clear with specific sub-questions and well-scoped for screening (-0 red flags). Excellent separation via trade-offs, distributed systems depth, and tunable parameters; follow-up probes deeper understanding. Minor deduct for slight bundling but structured well.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-433",
        "role_title": "Senior Backend Engineer - Platform APIs",
        "question": "# Technical Screening Question\n\n**Scenario-based question (2-5 minutes):**\n\n\"You're designing an internal platform API that allows teams to provision and manage database instances. Multiple services will call this API concurrently, and you need to ensure that:\n1. A database instance can only be provisioned once, even if multiple requests arrive simultaneously\n2. The provisioning process takes 30-60 seconds\n3. Callers need to know the status without polling repeatedly\n\nWalk me through how you'd design this in Go. What patterns or mechanisms would you use to handle the concurrency and reduce polling overhead?\"\n\n---\n\n## Why This Works:\n\n\u2713 **Practical** \u2013 Tests real platform API design decisions (idempotency, concurrency, async status tracking)\n\n\u2713 **Senior-level** \u2013 Expects knowledge of sync primitives, request deduplication, and pub/sub or streaming patterns\n\n\u2713 **Go-specific** \u2013 Opens discussion of mutexes, channels, context handling\n\n\u2713 **Platform-focused** \u2013 Directly relevant to service provisioning, a core platform concern\n\n\u2713 **Time-bound** \u2013 Candidates can sketch a",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core platform API skills (provisioning, concurrency) in Go, matching role's service catalog/resource provisioning focus; minor deduction as role doesn't explicitly list Go (-1). Clarity: Crystal clear scenario, specific requirements, well-scoped with time bound. Discriminative: Probes deep applied knowledge of Go sync primitives, idempotency, async patterns (e.g., channels, pub/sub), revealing senior-level trade-offs vs. basic recall.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-436",
        "role_title": "Backend Engineer - API Documentation",
        "question": "# Technical Screening Question\n\n## Question:\n\nYou're building an API documentation system that needs to auto-generate TypeScript SDK code from OpenAPI specs. Your team has discovered that some API endpoints have optional parameters with complex nested objects, and the generated SDK code is either too verbose or missing type safety for these optional fields.\n\n**How would you approach this problem? Walk me through:**\n1. How you'd modify the OpenAPI schema to properly represent optional nested structures\n2. What TypeScript patterns you'd use in the code generator to handle these cases\n3. One trade-off you'd have to consider between SDK usability and spec accuracy\n\n---\n\n## Why This Question Works:\n\n\u2713 **Practical:** Tests real problems encountered in API documentation tooling  \n\u2713 **Multi-faceted:** Requires knowledge of OpenAPI, TypeScript, and code generation  \n\u2713 **Mid-level appropriate:** Doesn't expect perfect answers, but reveals depth of experience  \n\u2713 **Time-bound:** Answerable conversationally in 3-4 minutes  \n\u2713 **Discussion-friendly:** Opens dialogue about trade-offs and decision-making  \n\n**Follow-up prom",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Directly targets core role skills (OpenAPI specs, SDK automation, TypeScript) with practical nested object issue in API tooling. Crystal clear structure with numbered parts and scoped constraints. Strong separation via trade-off analysis revealing deep understanding; minor deduct for bundled sub-questions but well-scoped overall.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-434",
        "role_title": "Principal Engineer - Database Strategy",
        "question": "# Screening Question\n\n**You're advising a company that currently runs a monolithic PostgreSQL database serving a single region. They're expanding to 3 new geographic regions and need <100ms latency for all users. Walk me through how you'd approach this problem\u2014what are the key trade-offs you'd evaluate, and what factors would drive your technology recommendation between solutions like read replicas, CockroachDB, or a sharded approach?**\n\n---\n\n## Why This Works\n\n**Tests Principal-Level Skills:**\n- Strategic thinking (trade-offs, not just \"what tool is best\")\n- Distributed systems depth (consistency models, latency, operational complexity)\n- Business acumen (understanding constraints drive decisions)\n- Mentorship capability (clear communication of complex reasoning)\n\n**Practical & Focused:**\n- Reflects a real architectural challenge\n- Answerable in 2-5 minutes without deep dives\n- Reveals how they'd actually guide a team through decisions\n\n**Distinguishes Principals:**\n- Junior engineers list features; principals discuss trade-offs\n- Expected to mention: RPO/RTO, consistency requirements, cost, team capability, migration risk",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Relevance: Directly tests core role skills in multi-region DB strategy, migration, tech selection with PostgreSQL context; no red flags. Clarity: Crystal clear, well-scoped with specific constraints (regions, latency); minor -1 for no explicit time/format. Discriminative: Excellent at separating principals via trade-offs, applied knowledge, and reasoning depth vs rote recall.",
          "composite": 9.666666666666666
        }
      },
      {
        "role_id": "be-435",
        "role_title": "Junior Backend Engineer - Queue Consumers",
        "question": "# Screening Question\n\n**You're building a RabbitMQ consumer that processes payment transactions from a queue. The consumer receives a message, updates a PostgreSQL database, and then acknowledges the message. However, you notice that sometimes the same payment gets processed twice.**\n\n**Walk me through:**\n1. **What could cause duplicate processing?**\n2. **How would you fix it?**\n3. **What's the difference between auto-acknowledge and manual acknowledge, and which would you use here?**\n\n---\n\n## Why This Question Works\n\n\u2705 **Practical & Real-world** \u2014 Duplicate processing is a common junior engineer problem  \n\u2705 **Right Difficulty** \u2014 Tests understanding of message queue fundamentals without requiring deep distributed systems knowledge  \n\u2705 **Covers Key Concepts** \u2014 Acknowledgment patterns, idempotency, and failure scenarios (all listed in the role description)  \n\u2705 **Open-ended** \u2014 Reveals how they think through problems, not just memorized answers  \n\u2705 **Time-appropriate** \u2014 Can be thoroughly answered in 3-4 minutes\n\n---\n\n## What You're Looking For\n\n**Strong Answer",
        "scores": {
          "relevance": 7,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Tests core acknowledgment patterns and idempotency from role, but RabbitMQ/PostgreSQL are specific unlisted techs (-2); not generic (-3 avoided). Clarity: Crystal clear, well-scoped numbered parts, no bundling issues. Discriminative: Excellent separation via causes/fixes/trade-offs, reveals applied thinking beyond recall.",
          "composite": 8.333333333333334
        }
      },
      {
        "role_id": "be-437",
        "role_title": "Senior Backend Engineer - Secrets Management",
        "question": "# Screening Question\n\n**You're designing a secrets rotation system for database credentials across 50+ microservices in Kubernetes. Walk me through how you'd architect this using Vault, and specifically explain how you'd handle the scenario where a service fails to pick up a rotated credential before the old one expires.**\n\n---\n\n## Rationale\n\nThis question is effective because it:\n\n1. **Tests practical architecture thinking** - Requires understanding of real-world secrets management complexity, not memorized facts\n\n2. **Appropriate for senior level** - Expects thoughtful design decisions around failure modes, not just \"how to use Vault\"\n\n3. **Covers multiple key skills**:\n   - Vault knowledge (rotation policies, lease management)\n   - Kubernetes operational awareness (deployment, service communication)\n   - Go context (they'd likely implement the consumer application)\n   - Security thinking (credential expiry, availability vs security tradeoffs)\n\n4. **Reveals problem-solving approach** - How they handle edge cases shows maturity:\n   - Do they think about grace periods?\n   - Do they consider monitoring/alerting?\n   - Do they discuss rollback strategies?",
        "scores": {
          "relevance": 7,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Tests core secrets rotation but Vault not explicitly listed in role (subtract 2); covers rotation/expiry well. Clarity: Well-scoped architecture question with specific failure scenario, minor missing constraints like time limit (-1). Discriminative: Excellent at testing trade-offs, failure handling, and practical depth for senior roles.",
          "composite": 8.0
        }
      },
      {
        "role_id": "be-439",
        "role_title": "Backend Engineer - Billing Integration",
        "question": "# Screening Question\n\n**\"Walk me through how you would handle a scenario where a customer upgrades their subscription mid-billing cycle and needs to be prorated. What information would you need from Stripe, and how would you structure the logic to calculate and apply the credit?\"**\n\n---\n\n## Why This Question Works:\n\n**Practical & Role-Specific**\n- Directly tests core responsibility (proration, invoicing)\n- Real problem they'll solve day-one\n\n**Right Difficulty Level**\n- Mid-level should understand subscription concepts\n- Allows room to discuss Stripe APIs, calculation logic, and edge cases\n- Not trivial (requires billing domain knowledge) but not PhD-level\n\n**Reveals Key Competencies**\n- Stripe API familiarity (proration_behavior, line items)\n- Billing math (daily rates, cycle dates)\n- System design thinking (data flow, state management)\n- Communication skills (explaining trade-offs)\n\n**Answerable in 2-5 Minutes**\n- Can sketch approach quickly without deep implementation details\n- Follow-ups available: \"What about failed payments?\" or \"How would you test this?\"",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core focus on proration/subscriptions/Stripe, no red flags. Clarity: Crystal clear scenario with specific ask, minor vagueness on exact Stripe API version but well-scoped. Discriminative: Tests applied knowledge of billing logic, Stripe integration, trade-offs/edge cases, strong separation without being too broad or recall-based.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-438",
        "role_title": "Staff Engineer - ML Serving Infrastructure",
        "question": "# Technical Screening Question\n\n**\"Walk me through how you'd design a batching strategy for an ML serving system handling requests with variable latency requirements. Specifically, consider a scenario where you have both latency-sensitive requests (sub-100ms SLA) and batch-optimized requests (up to 1s acceptable). How would you structure this in Triton, and what metrics would you track to validate it's working?\"**\n\n---\n\n## Why This Question Works:\n\n**Tests practical expertise:**\n- Understanding of dynamic batching trade-offs\n- Actual Triton configuration knowledge\n- Ability to handle competing constraints (latency vs throughput)\n\n**Appropriate for staff level:**\n- Requires architectural thinking, not just implementation\n- Evaluates observability mindset (metrics/monitoring)\n- Shows how they'd make real production trade-offs\n\n**Realistic scenario:**\n- This is a common real-world problem at scale\n- Reveals how they think about SLAs and system design\n\n**Answerable in timeframe:**\n- Senior candidates can sketch the solution quickly\n- Follow-ups naturally extend discussion if needed (Kubernetes scheduling, TensorRT optimizations,",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Directly tests core role skills (batching strategies, latency optimization for ML serving) with Triton (common in ML infra). Crystal clear scenario with specific SLAs, scopes to Triton structure and metrics. Strongly differentiates via trade-offs, architectural thinking, and observability\u2014no red flags.",
          "composite": 9.666666666666666
        }
      },
      {
        "role_id": "be-440",
        "role_title": "Junior Backend Engineer - Metrics Collection",
        "question": "# Screening Question\n\n**You're instrumenting a Go service that processes user requests. You need to track how long each request takes. Should you use a Prometheus Counter, Gauge, or Histogram for this metric? Explain why, and describe what data would be available in Prometheus/Grafana afterward.**\n\n---\n\n## Why This Works\n\n\u2705 **Appropriate for Junior Level:** Tests understanding of metric types without requiring deep expertise\n\n\u2705 **Practical:** Directly mirrors Day 1 tasks\u2014instrumenting a real service\n\n\u2705 **Observable Reasoning:** Answer reveals if they understand *why* metrics matter, not just syntax\n\n\u2705 **Time-Bound:** 2-3 minutes for a solid response\n\n---\n\n## What You're Looking For\n\n**Strong Answer** includes:\n- Histogram (correct choice)\n- Explains why: need distribution/percentiles, not just a single value\n- Mentions what becomes available: latency buckets, quantiles (p99, p95), average\n- Maybe: \"Could query request latency spikes in Grafana\"\n\n**Acceptable Answer:**\n- Histogram identified + basic explanation of buckets",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core focus on counter/gauge/histogram patterns for service instrumentation. Clarity: Crystal clear with specific scenario, single focused question, no ambiguity. Discriminative: Requires understanding trade-offs between metric types and explains queryable data (buckets/quantiles), distinguishing conceptual grasp from rote recall; ideal for entry-level.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-442",
        "role_title": "Backend Engineer - Event Streaming",
        "question": "# Screening Question\n\n**You're designing a real-time notification system where one event needs to be delivered to potentially thousands of subscribers. A naive approach fans out the event to every subscriber's queue, but this causes memory issues and slow writes at scale.**\n\n**Walk me through how you'd optimize this using NATS. What patterns or features would you leverage, and why?**\n\n---\n\n## Why This Question Works\n\n**Practical & Role-Specific:**\n- Directly addresses the \"fan-out\" requirement in the job description\n- Tests real understanding of NATS capabilities (subjects, subscriptions, subject hierarchies)\n\n**Mid-Level Appropriate:**\n- Not a gotcha question; assesses experience with actual architectural decisions\n- Expects knowledge of trade-offs, not perfection\n\n**Reveals Key Insights:**\n- Do they know NATS subjects/wildcards reduce fanout complexity?\n- Can they articulate the difference between queue groups vs. broadcast subscriptions?\n- Do they think about memory/performance implications?\n- Can they discuss subject-based filtering as a solution?\n\n**Answerable in 2-5 Minutes:**\n- Candidate can sketch the approach",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests fan-out core to role; assumes NATS in stack (no red flags). Clarity: Well-scoped, unambiguous with context. Discriminative: Probes trade-offs (queue groups vs broadcast, subjects/wildcards), separates experienced users from superficial knowledge in 2-5 min.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-441",
        "role_title": "Senior Backend Engineer - Autocomplete Systems",
        "question": "# Technical Screening Question\n\n## Autocomplete Latency Under Load\n\n**Scenario:**\nYou're building an autocomplete service that needs to handle 100K QPS with sub-50ms latency. Your current implementation queries Elasticsearch for prefix matches, applies personalization filters based on user history (cached in Redis), and ranks results.\n\nUnder load testing, you're seeing:\n- Elasticsearch queries: ~15ms (p99)\n- Redis lookups: ~2ms (p99)\n- Personalization logic: ~8ms (p99)\n- **Total latency: ~80ms (p99) \u2014 exceeding your 50ms SLA**\n\n**Questions:**\n1. Walk me through how you'd diagnose where the latency is actually being lost\n2. What optimizations would you prioritize, and why?\n3. What tradeoffs would you consider?\n\n---\n\n## Why This Works\n\n\u2705 **Tests senior-level thinking:** Requires understanding of systems bottlenecks, not just API knowledge  \n\u2705 **Practical:** Reflects real performance challenges in high-scale autocomplete  \n\u2705 **Open-ended:**",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Relevance: Directly tests core autocomplete optimization, prefix matching, personalization, and exact latency/QPS from role (-1 for no explicit role tech stack mention). Clarity: Crystal clear scenario with precise metrics and focused 3-part structure. Discriminative: Excellent probe of diagnosis, prioritization, and tradeoffs, strongly separates seniors who grasp bottlenecks from juniors.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-443",
        "role_title": "Senior Backend Engineer - Data Lake Architecture",
        "question": "# Screening Question\n\n**You're designing a data lake where business users need to query historical snapshots of customer data from 30 days ago, but the underlying schema has evolved significantly (columns added, renamed, and removed). How would you use Apache Iceberg's features to support this requirement, and what are the tradeoffs you'd consider?**\n\n---\n\n## Why This Question Works\n\n**Tests practical expertise:**\n- Time travel queries (core Iceberg feature)\n- Schema evolution handling (critical for production data lakes)\n- Understanding of snapshots and metadata layers\n\n**Appropriate for senior level:**\n- Requires experience designing systems, not just using APIs\n- Allows discussion of tradeoffs and architectural decisions\n- Open-ended enough to reveal depth of understanding\n\n**Realistic scenario:**\n- Common real-world requirement in mature data lakes\n- Shows how they think about user experience + infrastructure\n\n**Natural follow-ups** (if needed):\n- \"How would your answer change if users queried data from 2 years ago?\"\n- \"What monitoring would you put in place?\"\n- \"How does this interact with your partitioning strategy?\"",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Relevance: Directly tests core role skills (Iceberg table formats, schema evolution, time travel) with no red flags. Clarity: Crystal clear scenario and scope, minor deduction for no explicit time/format constraints but well-formed for screening. Discriminative: Excellent at revealing deep applied knowledge, tradeoffs, and architectural thinking to separate seniors from juniors.",
          "composite": 9.666666666666666
        }
      },
      {
        "role_id": "be-444",
        "role_title": "Backend Engineer - Service Templates",
        "question": "# Technical Screening Question\n\n**You're designing a Cookiecutter template for new Go microservices. A developer will run it once, and the generated service should be immediately deployable to Kubernetes with ArgoCD, including basic observability and security practices.**\n\n**Walk me through:**\n1. **What files/structure would you include** in the template to support this?\n2. **How would you handle variation** \u2014 for example, some services need databases, others don't?\n3. **What's one thing you'd bake into defaults** (vs. leaving for the developer to configure), and why?\n\n---\n\n## Why This Question Works\n\n\u2705 **Practical** \u2014 Tests real templating decisions, not memorization  \n\u2705 **Scope** \u2014 Covers the full stack (scaffolding, CI/CD, K8s, observability)  \n\u2705 **Mid-level appropriate** \u2014 Expects system thinking, not expert-level optimization  \n\u2705 **Reveals priorities** \u2014 Shows what the candidate considers \"essential\" vs. \"nice-to-have\"  \n\u2705 **Conversational** \u2014 Natural follow-ups on trade",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (templating, CI/CD, observability, security, K8s/ArgoCD) with no red flags (-0). Clarity: Well-scoped with 3 focused parts, unambiguous for screening despite multi-part (-0). Discriminative: Tests applied decisions, trade-offs, and priorities, strongly separating candidates via reasoning.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-445",
        "role_title": "Junior Backend Engineer - Database Migrations",
        "question": "# Screening Question\n\n**You need to add a new required column to an existing production table with millions of rows. Write out the steps you would take to do this safely using Alembic, and explain why you can't simply use `Column('new_col', String, nullable=False)` in your migration.**\n\n---\n\n## Why This Works\n\n\u2705 **Practical & Real** \u2014 This is a genuine problem junior engineers face on their first day\n\n\u2705 **Right Level** \u2014 Tests understanding of:\n- Alembic basics (writing migrations)\n- PostgreSQL constraints\n- Production safety thinking (zero-downtime concerns)\n- Schema versioning logic\n\n\u2705 **Time-Appropriate** \u2014 Takes 2-4 minutes for a competent junior to explain\n\n\u2705 **Open-Ended** \u2014 Reveals:\n- Whether they know `nullable=False` + no default = backfill problem\n- If they understand multi-step migration patterns\n- Whether they think about existing data\n- Communication clarity on database concepts\n\n---\n\n## What You're Looking For\n\n**Good Answer Includes:**\n- \"Add the column as nullable first\"\n- \"",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core focus on database migrations, zero-downtime patterns, and Alembic (no red flags). Clarity: Crystal clear with specific scenario and explanation prompt; minor deduction for lacking explicit time/format constraints. Discriminative: Strongly separates candidates by testing applied knowledge of multi-step migration patterns, production safety, and trade-offs (not recall); open-ended but scoped well.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-446",
        "role_title": "Staff Engineer - Distributed Tracing",
        "question": "# Technical Screening Question\n\n**Design a sampling strategy for a distributed tracing system handling 100k requests/second across multiple services. Walk me through how you'd decide between head-based and tail-based sampling, what trade-offs you'd consider, and how you'd handle the challenge of maintaining trace coherence when different spans are sampled at different rates.**\n\n---\n\n## Why This Question Works\n\n**Tests Staff-Level Thinking:**\n- Requires understanding of architectural trade-offs (not just \"what is sampling\")\n- Demonstrates systems thinking across infrastructure components\n- Shows ability to balance operational constraints with observability goals\n\n**Practical & Real:**\n- This is an actual problem at scale (100k RPS is genuine staff-level territory)\n- Candidates will naturally discuss real concerns: cost, trace completeness, debugging capability\n\n**Open-Ended:**\n- Allows candidates to showcase depth (mentioning Jaeger sampling plugins, ClickHouse retention strategies, Kafka buffering, SLO correlation)\n- No single \"correct\" answer\u2014reveals how they think through complexity\n\n**Natural Follow-ups:**\n- \"How would you implement priority-based sampling for errors?\"\n-",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Relevance: Directly targets core role responsibilities (distributed tracing at scale, sampling strategies) with realistic 100k RPS; no red flags. Clarity: Crystal clear, well-scoped for screening, precise trade-offs specified; minor deduction for lacking explicit time constraint. Discriminative: Excellent test of deep trade-offs, systems thinking, and applied knowledge at staff level, strong separation of strong vs weak candidates; open-ended but focused.",
          "composite": 9.666666666666666
        }
      },
      {
        "role_id": "be-447",
        "role_title": "Backend Engineer - Load Shedding",
        "question": "# Screening Question\n\n**\"Walk me through how you would implement a load shedding mechanism that drops low-priority requests when your service is under high load. What signals would you monitor to decide when to start shedding, and how would you prevent cascading failures downstream?\"**\n\n---\n\n## Why This Question Works\n\n**Tests core competencies:**\n- Load shedding patterns and decision logic\n- Backpressure signal understanding (queue depth, latency, CPU)\n- Graceful degradation thinking\n- Awareness of downstream impact\n\n**Seniority-appropriate:**\n- Mid-level should have opinions on *when* to shed and *what* metrics matter\n- Not asking for production-ready code, just architectural thinking\n- Room to discuss trade-offs (false positives vs. user experience)\n\n**Practical depth:**\n- Reveals whether they've actually dealt with throttling vs. just read about it\n- Follow-ups are natural: \"How would Redis fit in?\" or \"How do you test this?\"\n- Can assess if they think about observability (Prometheus integration)\n\n**Time-efficient:**\n- Candidate can outline approach in 2-3",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core focus on load shedding, priority queues, throttling, and backpressure\u2014no red flags, directly tests listed skills. Clarity: Crystal clear and well-scoped for screening, minor deduction only for lacking explicit time constraint. Discriminative: Tests applied architectural thinking, trade-offs, and real-world signals, strongly separates experienced candidates; time-efficient as noted.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-448",
        "role_title": "Senior Backend Engineer - Webhook Infrastructure",
        "question": "# Screening Question\n\n**Design a webhook retry mechanism that handles delivery failures while preventing duplicate deliveries to the customer. What would be your approach to storing state, and how would you handle the edge case where a webhook succeeds but the success acknowledgment is lost?**\n\n---\n\n## Why This Question Works\n\n**Practical Knowledge:** Tests understanding of the core challenge in webhook infrastructure\u2014idempotency and reliability under failure conditions.\n\n**Seniority Appropriate:** Expects the candidate to think about:\n- Idempotency keys and deduplication strategies\n- State management (database, cache, or both)\n- Distributed system failure modes\n- Trade-offs between consistency and operational complexity\n\n**Real-World Relevance:** This exact problem appears in production webhook systems and requires senior-level systems thinking.\n\n**Conversational:** Opens into natural follow-ups:\n- \"How would Kafka fit into this design?\"\n- \"What happens under high volume?\"\n- \"How do you monitor delivery success?\"\n\n**Time Appropriate:** Can be outlined in 2-3 minutes, with depth added based on candidate confidence.",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Directly targets core webhook delivery challenges (retry, idempotency, state, failure modes) from role description. Crystal clear with specific edge case; minor deduct for lacking explicit constraints like scale. Excellent at distinguishing senior systems thinkers via trade-offs and deep understanding.",
          "composite": 9.666666666666666
        }
      },
      {
        "role_id": "be-449",
        "role_title": "Junior Backend Engineer - Auth Integration",
        "question": "# Technical Screening Question\n\n**\"Walk me through how you would implement a refresh token flow in a Node.js application. Specifically, explain:**\n\n1. **Where** you'd store the refresh token (client-side vs server-side) and why\n2. **When** you'd use it (what triggers a refresh?)\n3. **One security concern** you'd need to handle\n\nYou don't need to write code\u2014just explain your approach.\"\n\n---\n\n## Rationale\n\n\u2705 **Tests practical OAuth2 understanding** - Refresh tokens are core to the role  \n\u2705 **Right difficulty level** - Junior engineers should know the basics but may not have all details polished  \n\u2705 **Security-focused** - Demonstrates awareness of auth risks (token rotation, storage, expiration)  \n\u2705 **2-5 minute response** - Conceptual explanation, not a coding exercise  \n\u2705 **Reveals real experience** - Distinguishes between memorized facts and applied knowledge\n\n**Red flags:** Confusion between access/refresh tokens, storing refresh tokens insecurely client-side, or inability to name any security concern.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 8,
          "reasoning": "Relevance: Directly tests core OAuth refresh token flow central to role's token handling focus, matches Node.js context, minor deduction for not being social login specific (-1). Clarity: Well-structured with numbered points, clear scope (no code), unambiguous. Discriminative: Tests applied reasoning on storage/ triggers/security trade-offs, separates practical knowledge from trivia, good for entry-level but not elite depth.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-450",
        "role_title": "Principal Engineer - ML Platform Strategy",
        "question": "# Screening Question\n\n**Describe a situation where you had to choose between multiple ML orchestration or workflow tools for your organization. Walk me through your decision framework\u2014what factors did you weigh, and how did you handle stakeholder concerns about the choice?**\n\n---\n\n## Why This Works\n\n**Tests Principal-Level Thinking:**\n- Requires systems thinking (not just technical merits)\n- Evaluates decision-making under trade-offs\n- Assesses ability to balance technical, organizational, and governance concerns\n\n**Practical & Concrete:**\n- Grounded in real experience (Kubernetes, Kubeflow, MLflow, Ray context)\n- Answerable in 2-5 minutes with specific example\n- Naturally surfaces their approach to architecture and strategy\n\n**Reveals Key Competencies:**\n- **Platform strategy:** How they evaluate tools holistically\n- **Governance:** Maintainability, compliance, cost considerations\n- **Mentorship/Communication:** How they aligned teams around the decision\n- **Technical depth:** Understanding of tool trade-offs (e.g., Kubeflow vs Ray vs MLflow for different workloads)\n\n**Follow-ups",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core ML platform strategy, tooling selection, governance, and stakeholder alignment from role; minor ding for assuming specific experience (-1). Clarity: Crystal clear behavioral question with precise scope for screening. Discriminative: Probes deep trade-offs, frameworks, and real-world application, strongly separating principal-level thinkers from juniors.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-451",
        "role_title": "Backend Engineer - Subscription Billing",
        "question": "# Screening Question\n\n**Walk me through how you'd handle a customer downgrading from a $100/month annual plan to a $50/month annual plan mid-cycle. What are the key considerations, and how would you implement the credit/refund logic?**\n\n---\n\n## Why This Question Works\n\n**Tests practical expertise:**\n- Proration calculations (core to the role)\n- Subscription state management\n- Financial correctness and edge cases\n\n**Appropriate for mid-level:**\n- Assumes familiarity with billing concepts\n- Doesn't require memorized formulas\n- Evaluates problem-solving approach, not just implementation details\n\n**Real-world relevance:**\n- Directly reflects daily work in subscription billing\n- Reveals how they think about atomicity, refunds, and audit trails\n\n---\n\n## What You're Looking For\n\n**Strong answers typically include:**\n- How they calculate prorated amounts (days remaining, daily rate)\n- Where the credit goes (instant refund vs. account balance vs. future invoice)\n- Data consistency concerns (state transitions, idempotency)\n- How they'd validate the calculation and prevent double-refunds\n-",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Directly tests core role skills in proration, plan changes, and billing logic from the role description (no red flags). Crystal clear scenario with strong real-world scope, minor deduction for lacking explicit constraints like time or format. Excellent separation of strong candidates via trade-offs, edge cases, and implementation reasoning (not recall or trivial).",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-453",
        "role_title": "Backend Engineer - Notification Delivery",
        "question": "# Screening Question\n\n**You're designing a notification delivery system where thousands of devices need to receive a push notification within seconds. Walk me through how you'd handle the following scenario:**\n\n*A user sends a notification targeting 50,000 devices. Some devices have both FCM (Android) and APNS (iOS) tokens registered. You need to:*\n1. *Retrieve all device tokens efficiently*\n2. *Batch them appropriately for FCM and APNS APIs*\n3. *Handle partial failures (some batches fail, some succeed)*\n\n**What would your approach be, and what would you consider regarding database queries, batching strategy, and failure handling?**\n\n---\n\n## Why This Question Works\n\n\u2705 **Practical & Real** \u2013 Directly mirrors the actual job responsibility  \n\u2705 **Mid-Level Appropriate** \u2013 Tests architectural thinking without requiring deep expertise  \n\u2705 **Multi-Faceted** \u2013 Covers database efficiency, API integration, and resilience  \n\u2705 **Time-Boxed** \u2013 Can be answered thoroughly in 3-4 minutes  \n\u2705 **Open-Ended** \u2013 Reveals their",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core needs (device registration, batching, delivery tracking) with FCM/APNS directly relevant; no red flags. Clarity: Crystal clear, well-scoped scenario with specific numbered asks, minor deduction for no explicit time/format constraints. Discriminative: Excellent at separating candidates via trade-offs in queries/batching/failures, reveals architectural depth without being too broad or recall-based.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-452",
        "role_title": "Senior Backend Engineer - Faceted Search",
        "question": "# Screening Question\n\n**You're implementing faceted search for an e-commerce platform with millions of products. Users filter by category, price range, brand, and ratings simultaneously. Your aggregation queries are returning accurate facet counts, but query latency has increased from 200ms to 2+ seconds as the product catalog grew.**\n\n**Walk me through how you would diagnose and optimize this. What are the key bottlenecks you'd investigate in Elasticsearch, and what specific techniques would you apply?**\n\n---\n\n## Why This Works\n\n\u2705 **Practical & Real** - Reflects actual performance challenges at scale  \n\u2705 **Multi-layered** - Tests diagnosis, Elasticsearch knowledge, and optimization strategy  \n\u2705 **Senior-appropriate** - Expects understanding of aggregations, query planning, and tradeoffs  \n\u2705 **2-5 min range** - Answerable without deep implementation details  \n\u2705 **Open-ended** - Reveals depth of experience and problem-solving approach\n\n## What You're Evaluating\n\n- Understanding of Elasticsearch aggregation performance (shard-level work, query overhead)\n- Knowledge of practical optimizations (",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core focus on optimizing aggregation queries and facet count accuracy at scale in faceted search; no red flags. Clarity: Crystal clear scenario with specific symptoms, well-scoped for screening; minor deduction for lacking explicit time/format constraints. Discriminative: Strongly separates seniors via diagnosis, trade-offs (e.g., shard overhead, sampling), and ES-specific techniques; open-ended but focused.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-454",
        "role_title": "Junior Backend Engineer - ETL Scripts",
        "question": "# Screening Question\n\n**You have a CSV file with 500,000 customer records. One column contains dates in mixed formats (some are \"2024-01-15\", others are \"01/15/2024\"). You need to load this into PostgreSQL, but the database expects a standard datetime format. Walk me through how you'd handle this using pandas, and what you'd do if some rows have invalid dates.**\n\n---\n\n## Why This Question Works:\n\n\u2705 **Practical & Real** - Common ETL problem junior engineers actually face\n\n\u2705 **Right Difficulty** - Tests pandas basics + problem-solving, not memorization\n\n\u2705 **Multi-faceted** - Covers:\n- Data loading (read_csv)\n- Data cleaning/transformation (pd.to_datetime)\n- Error handling (invalid dates)\n- Database awareness (schema requirements)\n\n\u2705 **Time-appropriate** - Can explain solution in 2-5 minutes\n\n---\n\n## What You're Looking For:\n\n**Strong answer includes:**\n- `pd.read_csv()` to load the file\n- `pd.to_datetime()` with `infer_datetime",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core ETL skills (data transformation, error handling) with pandas for entry-level role; minor ding for assuming pandas without explicit mention (-1). Clarity: Crystal clear scenario, specific tech, well-scoped walkthrough with no ambiguity. Discriminative: Requires applied knowledge of pandas functions, error handling trade-offs (e.g., infer_datetime, errors='coerce'), distinguishes problem-solvers from rote learners.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-455",
        "role_title": "Staff Engineer - Developer Portal",
        "question": "# Technical Screening Question\n\n**You're designing the plugin architecture for a developer portal where different teams will contribute integrations (e.g., deployment tools, monitoring systems, secret management). Walk me through how you'd structure this to ensure:**\n\n1. **Plugin isolation** \u2014 a broken plugin doesn't crash the portal\n2. **Shared concerns** \u2014 plugins can access common APIs (auth, service catalog, notifications) without duplicating code\n3. **Team autonomy** \u2014 teams can develop and deploy plugins independently without coordinating with a central platform team\n\n**What are the key architectural decisions you'd make, and what trade-offs would you accept?**\n\n---\n\n## Why This Works\n\n**For Staff Level:** Tests architectural thinking and systems design, not implementation details. Expects them to reason about trade-offs and constraints.\n\n**Practical:** Directly relevant to Backstage plugin patterns and real portal challenges. Draws on their experience scaling developer tools.\n\n**2-5 Minute Response:** Can outline approach without deep code, but reveals:\n- Understanding of module/plugin boundaries\n- How they think about dependency injection and composition\n- Knowledge of deployment/versioning strategies\n- Awareness of operational concerns (",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core role responsibility (plugin architecture for developer portal), matches Backstage-like patterns; minor generic deduction avoided as it's role-specific (-0). Clarity: Well-scoped with 3 precise requirements, clear expectations for decisions/trade-offs, screening-appropriate depth. Discriminative: Probes deep architectural reasoning, trade-offs, and applied knowledge (isolation, shared deps, autonomy), strongly separates staff-level thinkers from juniors; fits 2-5 min response.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-456",
        "role_title": "Backend Engineer - Query Optimization",
        "question": "# Screening Question\n\n**You're analyzing a slow query on a PostgreSQL table with 10M rows. The query filters by `user_id` and `created_at`, but takes 8 seconds. When you run `EXPLAIN ANALYZE`, you see a Sequential Scan instead of an Index Scan, even though an index exists on `(user_id, created_at).**\n\n**Walk me through:**\n1. **Why the planner might choose Sequential Scan over the index**\n2. **How you'd investigate and fix this**\n3. **What tools or commands you'd use to confirm your fix works**\n\n---\n\n## Why This Works\n\n\u2705 **Practical & Real** - Mirrors actual performance debugging  \n\u2705 **Mid-Level Appropriate** - Requires index knowledge + planner understanding, not deep internals  \n\u2705 **Multi-faceted** - Tests diagnostics (EXPLAIN), root cause analysis, and solution validation  \n\u2705 **Time-Bound** - Can be answered thoroughly in 3-4 minutes  \n\u2705 **Follow-ups Ready** - Natural paths to dig deeper (statistics, query re",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core focus on slow query detection, index recommendations, and plan analysis in PostgreSQL (10/10, no red flags). Clarity: Crystal clear, well-scoped multi-part structure with real scenario; minor deduction for lacking explicit time/format constraints (-1). Discriminative: Excellent at separating candidates via trade-offs (planner decisions, stats, fixes), applied knowledge beyond recall (9/10).",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-458",
        "role_title": "Backend Engineer - API Gateway Plugins",
        "question": "# Technical Screening Question\n\n**Scenario:**\n\nYou're building a Kong plugin that needs to:\n1. Extract a user ID from the request header\n2. Check if that user is rate-limited using Redis\n3. Allow the request through or reject it with a 429 status\n\nWalk me through how you'd structure this plugin in Lua. What are the key lifecycle phases you'd use, and how would you interact with Redis?\n\n---\n\n## Why This Question Works:\n\n\u2705 **Practical** \u2014 Tests real plugin development, not Kong trivia  \n\u2705 **Scoped** \u2014 Can be answered thoroughly in 3-5 minutes  \n\u2705 **Multi-layered** \u2014 Evaluates:\n- Understanding of Kong's request/response lifecycle\n- Lua syntax and plugin structure\n- Redis client usage\n- HTTP status code handling\n\n\u2705 **Mid-level appropriate** \u2014 Assumes familiarity with Kong/Lua but doesn't require advanced optimization\n\n---\n\n## What to Listen For:\n- Mention of `access` phase (where business logic runs before proxying)\n- Use of Kong's Redis connector or similar approach\n- Proper",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Directly tests core Kong plugin development (Lua structure, lifecycle phases, Redis integration) matching API gateway auth/rate-limiting needs. Crystal clear scenario with focused scope, no ambiguity. Strong separation via applied knowledge of phases (access), Redis interaction, and trade-offs, not mere recall. Minor ding for no explicit time constraint but well-scoped for screening.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-459",
        "role_title": "Junior Backend Engineer - Container Security",
        "question": "# Technical Screening Question\n\n**Scenario-Based Question:**\n\n\"You're setting up Trivy to scan Docker images in your CI/CD pipeline. A developer builds an image from `ubuntu:latest` and the scan reports several high-severity vulnerabilities. Walk me through:\n\n1. What those vulnerabilities likely are and why they appear in a base image\n2. How you would approach fixing this\u2014what would you suggest to the developer?\n3. What's one limitation of just scanning and how might you enforce better practices going forward?\"\n\n---\n\n## Why This Works:\n\n\u2705 **Practical** \u2013 Mirrors real junior-level tasks (running scans, interpreting results, suggesting fixes)\n\n\u2705 **Layered** \u2013 Tests three levels: understanding (what vulnerabilities are), problem-solving (how to fix), and systems thinking (policy enforcement)\n\n\u2705 **Time-appropriate** \u2013 Answers in 2-5 minutes; no deep architectural knowledge required\n\n\u2705 **Seniority-aligned** \u2013 Doesn't expect mastery of Kubernetes or complex orchestration; focuses on container fundamentals they'll actually work with day one\n\n\u2705 **",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core container scanning (Trivy/Docker), vulnerability detection, base image management, and policy enforcement for entry-level role (-0 red flags). Clarity: Well-scoped scenario with numbered parts, unambiguous, screening-appropriate depth. Discriminative: Excellent separation via layered testing (understanding -> problem-solving -> systems thinking/trade-offs), not recall/yes-no, practical for juniors.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-457",
        "role_title": "Senior Backend Engineer - SLO Automation",
        "question": "# Screening Question\n\n**You're designing a burn rate alerting system for SLOs in Kubernetes. A service has a 99.9% availability SLO, and you need to alert when the error budget is being consumed too quickly\u2014not just when the SLO is breached.**\n\n**Walk me through:**\n1. **How would you define \"burn rate\"** in this context, and why is it better than just checking current error rates?\n2. **What would be a reasonable alerting strategy?** (e.g., at what burn rate multiples would you trigger alerts, and over what time windows?)\n3. **How would you implement this in Prometheus/Sloth,** or if you've used similar tools, what challenges did you encounter?\n\n---\n\n## Why This Question Works\n\n\u2705 **Practical & Real** \u2013 Senior engineers actually design these systems  \n\u2705 **Multi-layered** \u2013 Tests conceptual understanding, judgment, and implementation knowledge  \n\u2705 **Time-appropriate** \u2013 A strong candidate answers in 3-4 minutes with specifics  \n\u2705 **Reveals depth** \u2013 Distinguishes between someone who's",
        "scores": {
          "relevance": 9,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core SLO/burn rate alerting skills for the role's infrastructure focus, matches Kubernetes/Prometheus context (-1 for minor unlisted 'Sloth' tool). Clarity: Well-structured but multiple bundled sub-questions (-2) and lacks strict time/format constraints. Discriminative: Excellent at separating candidates via definitions, judgment on thresholds/windows, and real implementation trade-offs/challenges.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-460",
        "role_title": "Backend Engineer - Experiment Tracking",
        "question": "# Technical Screening Question\n\n**You're building an experiment tracking service where data scientists log thousands of metrics per experiment run. A team member proposes storing all metrics in a single PostgreSQL table with columns: `run_id`, `metric_name`, `metric_value`, `timestamp`.**\n\n**What issues might this approach have as the system scales, and how would you redesign it?**\n\n---\n\n## Why This Question Works:\n\n\u2705 **Tests practical architecture thinking** \u2013 Assesses ability to anticipate scaling issues, not memorize syntax\n\n\u2705 **Mid-level appropriate** \u2013 Expects knowledge of database design and ML workflows, but not deep distributed systems expertise\n\n\u2705 **Real-world relevance** \u2013 Directly mirrors decisions made when building experiment tracking systems (similar to MLflow's actual design challenges)\n\n\u2705 **Open-ended** \u2013 Reveals how candidates think about tradeoffs (normalization vs. query performance, time-series data handling, retention policies)\n\n---\n\n## What You're Listening For:\n\n- **Good signals:** Time-series considerations, index strategies, schema normalization, partitioning by run_id or time, separate tables for different metric",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core skills for ML experiment tracking infrastructure (metric logging, scaling), matches role exactly, no red flags. Clarity: Crystal clear scenario and ask, well-scoped for screening, minor deduction for no explicit time/format constraints. Discriminative: Excellent at revealing trade-offs (schema, partitioning, time-series), strong separation of mid-level thinkers, open-ended but focused.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-461",
        "role_title": "Senior Backend Engineer - Fraud Detection",
        "question": "# Screening Question\n\n**Walk me through how you'd design a real-time fraud detection system that needs to make a pass/fail decision on a payment transaction in under 100ms. What are the key components, and where would you use Redis, Kafka, and your ML model in this pipeline?**\n\n---\n\n## Why This Question Works:\n\n\u2705 **Tests core competencies:**\n- Real-time system design (critical for payments)\n- Understanding of the tech stack and *why* each tool fits\n- ML model deployment trade-offs\n- Latency constraints in production\n\n\u2705 **Reveals seniority through depth:**\n- Junior: \"Run the model and return a score\"\n- Senior: Discusses caching strategies, async scoring, fallback rules, batch vs. real-time tradeoffs, model serving architecture\n\n\u2705 **Practical & answerable:** Candidates can outline their approach in 3-4 minutes and go deeper on areas they know well\n\n\u2705 **Conversation starter:** Natural follow-ups on model versioning, handling feature freshness, manual review workflows, etc.",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core fraud detection pipeline skills with exact tech stack (Redis, Kafka, ML) and real-time constraints from role. Clarity: Crystal clear with 100ms scope, but minor deduction for no explicit answer format/depth. Discriminative: Strong separation via trade-offs, component reasoning, and latency handling; reveals seniority without being too broad.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-462",
        "role_title": "Backend Engineer - Alert Routing",
        "question": "# Screening Question\n\n**You're designing an alert routing system where multiple alerts from different services need to be deduplicated and grouped before being sent to PagerDuty. An alert for a database connection timeout might come from 5 different services simultaneously, but you only want to trigger one PagerDuty incident.**\n\n**How would you approach deduplication in this system? Walk me through:**\n1. **What would you use as a deduplication key?**\n2. **Where in your architecture would you implement this check\u2014before or after Alertmanager?**\n3. **How would you handle the storage and TTL of these deduplication records?**\n\n---\n\n## Why This Question Works\n\n- **Practical & Real:** Directly mirrors the actual problem described in the job\n- **Shows System Design Thinking:** Evaluates architectural decisions, not just syntax\n- **Mid-Level Appropriate:** Requires Go/backend experience but doesn't demand expert-level mastery\n- **Reveals Technical Depth:** Answers expose knowledge of:\n  - Alert fingerprinting/grouping concepts\n  - Trade-offs between Alertmanager config vs custom logic",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (alert deduplication/grouping) with PagerDuty mention; minor ding for assuming Alertmanager familiarity not explicit in role (-1). Clarity: Crystal clear, well-scoped multi-part structure with no ambiguity. Discriminative: Excellent separation via trade-offs (keys, architecture placement, storage/TTL), reveals deep applied knowledge beyond recall.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-463",
        "role_title": "Junior Backend Engineer - Full-text Search",
        "question": "# Screening Question\n\n**You're building a search feature for a product catalog. A user searches for \"red shoes\" and you need to return results ranked by relevance. Walk me through how you'd implement this in PostgreSQL using full-text search, and explain what a `tsvector` is and why it matters.**\n\n---\n\n## Evaluation Criteria\n\n**Strong Answer** includes:\n- Explains `tsvector` as a preprocessed text format optimized for search (tokenization, lemmatization)\n- Mentions creating a `tsvector` column or generating it on-the-fly with `to_tsvector()`\n- References `tsquery` for converting search input\n- Discusses ranking (e.g., `ts_rank()` or `ts_rank_cd()`) to order results by relevance\n- Shows awareness of indexing with GIN or GIST for performance\n\n**Acceptable Answer** includes:\n- Core concept of `tsvector` as processed text\n- Basic query matching using `@@` operator\n- At least mentions ranking or indexing\n\n**Red Flags:**\n- Confuses `tsvector` with regular string",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests PostgreSQL full-text search core skills (tsvector, ranking) from role description, no red flags. Clarity: Crystal clear and well-scoped for screening, minor deduction for lacking explicit time constraint. Discriminative: Requires applied knowledge and trade-offs (e.g., indexing choices), strong separation via detailed strong/acceptable criteria provided.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-465",
        "role_title": "Backend Engineer - CDC Pipelines",
        "question": "# Screening Question\n\n**\"Walk me through how you would handle a schema change (e.g., adding a new column) in a source PostgreSQL table when you have an active Debezium CDC pipeline running to Snowflake. What could go wrong, and how would you ensure no data is lost?\"**\n\n---\n\n## Why This Question Works\n\n**Practical & Real:** Schema evolution is a constant challenge in production CDC pipelines\u2014not theoretical.\n\n**Mid-Level Appropriate:** Tests understanding of:\n- Debezium's behavior during schema changes\n- Downstream implications (Snowflake compatibility)\n- Data loss risks and recovery strategies\n- Operational problem-solving\n\n**Opens Conversation:** Candidates can discuss:\n- Signal tables / schema history topics\n- Backfilling strategies\n- Rolling deployments or pipeline restarts\n- Exactly-once semantics under schema changes\n\n**Time-Boxed:** Naturally fits a 3-5 minute discussion with follow-ups like:\n- \"How would you test this?\"\n- \"What if the column is NOT NULL?\"\n- \"How do you monitor for pipeline lag during this?\"\n\nThis reveals both depth",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's CDC pipelines, schema changes, PostgreSQL/Debezium/Snowflake stack. Clarity: Crystal clear scenario with example, well-scoped for screening; minor deduction for no explicit time constraint. Discriminative: Tests deep applied knowledge of schema evolution risks, recovery, trade-offs\u2014strong separation of mid-level expertise.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-467",
        "role_title": "Backend Engineer - Connection Pooling",
        "question": "# Screening Question\n\n**You're implementing a connection pooling layer for a high-traffic application using PgBouncer. Your application has 50 concurrent users, but you're seeing connection timeouts and occasional \"too many connections\" errors even though your PostgreSQL max_connections is set to 200.**\n\n**Walk me through how you would:**\n1. **Diagnose** what's happening (what metrics/logs would you check?)\n2. **Configure** PgBouncer pool_size and reserve_pool_size to fix it\n3. **Explain** the tradeoff you're making with your chosen pool size\n\n---\n\n## Why This Question Works\n\n\u2705 **Practical** \u2013 Tests real troubleshooting skills, not theoretical knowledge  \n\u2705 **Appropriate Level** \u2013 Mid-level should understand pooling concepts and basic PgBouncer config  \n\u2705 **Multi-layered** \u2013 Assesses diagnosis, configuration, and architectural thinking  \n\u2705 **Time-bound** \u2013 Answerable with concrete examples in 3-4 minutes  \n\u2705 **Role-relevant** \u2013 Directly tests the core responsibility (pool",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core connection pooling, PgBouncer config, sizing, and tradeoffs matching role's pool sizing/failover focus (-0 red flags). Clarity: Well-scoped with numbered parts, specific scenario, no ambiguity. Discriminative: Requires diagnosis (applied knowledge), config decisions, and tradeoff explanation, strongly separating candidates with deep understanding from rote learners.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-464",
        "role_title": "Staff Engineer - Presence Systems",
        "question": "# Screening Question\n\n**Design a heartbeat protocol for a presence system that needs to handle 100K concurrent users. Walk me through:**\n\n1. **What information would you include in each heartbeat message, and why?**\n2. **How would you handle a client that stops sending heartbeats\u2014what's your timeout strategy?**\n3. **How would you reduce bandwidth while keeping presence data reasonably fresh?**\n\n**Follow-up:** *If a user has flaky network connectivity and heartbeats are dropping 30% of the time, how would that change your design?*\n\n---\n\n## Why This Works\n\n\u2705 **Practical & Real** \u2014 Staff engineers make these architectural tradeoffs daily  \n\u2705 **Shows Systems Thinking** \u2014 Reveals how they balance freshness, cost, and reliability  \n\u2705 **Appropriate Scope** \u2014 Not a deep-dive, but exposes their reasoning at scale  \n\u2705 **Seniority Signal** \u2014 Listen for mentions of:\n- Exponential backoff, jitter\n- State machine thinking (connected \u2192 stale \u2192 offline)\n- Tradeoffs between accuracy and resource cost\n-",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Directly targets core role skills (heartbeat protocols, presence at scale, connection management) with no red flags. Crystal clear structure with focused sub-questions and scoped follow-up; no ambiguity or overload. Strong separation via trade-offs (bandwidth, timeouts, flaky networks), state machines, backoff\u2014reveals senior systems thinking vs rote knowledge.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-466",
        "role_title": "Senior Backend Engineer - GitOps Tooling",
        "question": "# Technical Screening Question\n\n**\"Walk me through how you would design a reconciliation loop for a GitOps tool that detects and corrects drift between desired state (in Git) and actual state (in a Kubernetes cluster). What are the key components, and what challenges would you anticipate around frequency, performance, and handling conflicts?\"**\n\n---\n\n## Why This Question Works\n\n**Practical & Senior-Level:**\n- Tests hands-on understanding of core GitOps concepts, not just terminology\n- Requires systems thinking (architecture, trade-offs, edge cases)\n- Appropriate for someone who's built or debugged similar systems\n\n**Observable Skills:**\n- Understands reconciliation patterns (controller loops, watch mechanisms)\n- Thinks about operational trade-offs (sync frequency vs. load, false positives)\n- Considers real-world complications (failed reconciliations, user-initiated changes, resource limits)\n- Likely mentions: controllers, informers/watches, exponential backoff, conflict resolution strategies\n\n**Time-Appropriate:**\n- Can sketch a solid answer in 2-5 minutes\n- Allows follow-ups on specifics (ArgoCD's approach",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Directly targets core GitOps reconciliation loops, drift detection, and trade-offs matching role's declarative infra focus (no red flags). Crystal clear with specific scope (components, challenges: frequency/performance/conflicts), minor deduction for lacking explicit time/format constraints. Strong separation via trade-offs, edge cases, and applied systems knowledge; not recall/trivia, scannable in <10min.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-468",
        "role_title": "Junior Backend Engineer - CI Pipelines",
        "question": "# Technical Screening Question\n\n**\"Walk me through how you would set up a GitHub Actions workflow that builds a Docker image and runs Python unit tests in parallel across 3 different test suites. What would you include to make sure the workflow doesn't rebuild the Docker image if the Dockerfile hasn't changed?\"**\n\n---\n\n## Why This Question Works:\n\n**Tests practical skills:**\n- GitHub Actions workflow syntax (job configuration, steps)\n- Docker integration in CI pipelines\n- Job parallelization (matrix strategy)\n- Caching strategies (layer caching, dependency caching)\n\n**Appropriate for junior level:**\n- Not asking for advanced optimization techniques\n- Tests fundamentals they'd use day-one\n- Allows candidates to show what they know without requiring production experience\n\n**Realistic scenario:**\n- Mirrors actual tasks in the role (Docker builds + test parallelization)\n- Shows how they think about pipeline efficiency\n\n**Flexible depth:**\n- Junior candidates can answer at basic level (matrix jobs, basic caching)\n- Strong candidates naturally mention docker/build-cache action, artifacts, or conditional steps\n- Easy to probe deeper with follow-ups",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Directly tests core role skills (GitHub Actions workflows, Docker builds, test parallelization, caching) for entry-level CI pipeline engineer. Crystal clear with specific scenario and efficiency constraint; minor deduction for lacking explicit time/format constraints. Strong separation via trade-offs in caching/parallelization strategies, flexible for junior levels without being trivial.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-470",
        "role_title": "Backend Engineer - GraphQL Federation",
        "question": "# Screening Question: Backend Engineer - GraphQL Federation\n\n## Question:\n\n**You have two federated subgraphs: a \"Users\" subgraph and an \"Orders\" subgraph. The Users subgraph defines a `User` entity with a `@key` directive, and the Orders subgraph needs to reference User data.**\n\n**Walk me through:**\n1. **How would you define the `@key` directive on the User entity, and why is it necessary?**\n2. **How would you extend the User type in the Orders subgraph to add order-related fields?**\n3. **What would happen if a gateway query requests both user information and their orders - briefly describe the query planning process.**\n\n---\n\n## Why This Works:\n\n\u2705 **Practical & Scenario-Based** \u2014 Tests real federation patterns candidates will implement daily\n\n\u2705 **Multi-Layered** \u2014 Assesses understanding of keys, entity references, and gateway query execution\n\n\u2705 **Time-Appropriate** \u2014 Answerable in 3-4 minutes with concrete code examples or explanations\n\n\u2705 **Mid-Level Calibration** \u2014 Doesn",
        "scores": {
          "relevance": 7,
          "clarity": 8,
          "discriminative": 8,
          "reasoning": "Relevance: Directly tests core GraphQL federation skills (schema stitching, entity resolution, query planning) but deduct -3 for generic multi-part structure applicable to many backend roles. Clarity: Well-scoped and unambiguous, but -2 for bundling 3 questions into one. Discriminative: Strong separation via applied knowledge of federation mechanics and trade-offs, not just recall; scenario-based with good depth for screening.",
          "composite": 7.666666666666667
        }
      },
      {
        "role_id": "be-472",
        "role_title": "Backend Engineer - Vector Search",
        "question": "# Screening Question\n\n**You're implementing a hybrid search system that combines keyword matching with semantic similarity. A user searches for \"affordable running shoes,\" but your vector search returns results for \"expensive athletic footwear\" with high semantic similarity, while keyword search correctly filters by price. How would you structure this to balance both signals, and what are the trade-offs you'd consider?**\n\n---\n\n## Why This Works:\n\n\u2705 **Practical** \u2013 Tests real-world problem-solving in vector search systems  \n\u2705 **Mid-level appropriate** \u2013 Requires understanding of embeddings + ranking, but not deep ML theory  \n\u2705 **Time-scoped** \u2013 Can be answered meaningfully in 2-5 minutes  \n\u2705 **Role-specific** \u2013 Directly relevant to hybrid search, index management, and semantic limitations  \n\u2705 **Open-ended** \u2013 Reveals thinking about weighting strategies, re-ranking, and system design\n\n## What You're Listening For:\n\n- Understanding that semantic similarity \u2260 intent matching\n- Awareness of ranking/weighting approaches (score normalization, fusion algorithms)\n- Knowledge of when keyword search catches what vectors",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core hybrid search, semantic retrieval, and vector limitations from role (-0 red flags). Clarity: Crystal clear scenario with precise scope for screening. Discriminative: Probes trade-offs in weighting/fusion/ranking, revealing deep applied understanding vs rote knowledge.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-473",
        "role_title": "Senior Backend Engineer - Refund Processing",
        "question": "# Screening Question\n\n**Design a refund processing system that needs to handle partial refunds and ensure ledger consistency. Walk me through:**\n\n1. **How would you structure the data model** to support multiple partial refunds against a single transaction, ensuring you can always answer \"what's the refund status of this order?\"\n\n2. **What consistency challenges do you anticipate** when a partial refund is initiated but the provider callback hasn't arrived yet? How would you handle a callback that arrives out of order?\n\n3. **How would you use Kafka and/or a database transaction** to ensure the ledger entry and refund record stay in sync, especially if the service crashes between these two operations?\n\n---\n\n## Why This Question Works\n\n\u2713 **Practical depth:** Tests real architectural decisions (data modeling, eventual consistency, idempotency)  \n\u2713 **Seniority-appropriate:** Expects knowledge of distributed systems trade-offs, not just syntax  \n\u2713 **Stack-relevant:** Naturally incorporates Kafka, PostgreSQL, and ledger thinking  \n\u2713 **Open-ended:** Senior engineers show depth through how they prioritize concerns (consistency vs.",
        "scores": {
          "relevance": 7,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Tests core refund/ledger skills but assumes Kafka/PostgreSQL not explicitly listed (-2 for unlisted tech, -1 generic distributed design). Clarity: Crystal clear, well-scoped sub-questions with precise intent. Discriminative: Excellent at probing trade-offs in consistency, out-of-order events, and sync mechanisms, strongly separates seniors.",
          "composite": 8.333333333333334
        }
      },
      {
        "role_id": "be-476",
        "role_title": "Backend Engineer - Live Updates",
        "question": "# Technical Screening Question\n\n## Question:\n\n**\"Walk us through how you'd implement a feature where different users see different real-time updates based on their assigned dashboard. For example, User A should only receive updates for Dashboard 1, while User B only gets updates for Dashboard 2. How would you use Socket.io rooms to prevent data leakage, and what would you do if a user's dashboard assignment changes while they're connected?\"**\n\n---\n\n## Why This Works:\n\n\u2705 **Practical & Real-World** \u2014 Directly mirrors the job's core responsibility  \n\u2705 **Tests Multiple Competencies** \u2014 Room management, broadcasting logic, reconnection handling  \n\u2705 **Appropriate Depth** \u2014 Mid-level should know Socket.io basics but may need to think through edge cases  \n\u2705 **Timeboxed** \u2014 Can be answered with a clear architecture in 3-4 minutes  \n\u2705 **Reveals Problem-Solving** \u2014 Shows how they handle permission boundaries and state transitions\n\n---\n\n## What You're Looking For:\n\n- **Strong Answer:** Uses Socket.io `join()`/`leave()` rooms, considers leaving",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Perfect relevance to role's core needs (rooms, selective broadcasting, reconnections). Crystal clear with specific Socket.io focus and edge case, well-scoped for screening. Excellent at distinguishing via trade-offs in room management and state changes; minor subtract for lacking explicit time/format constraints.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-469",
        "role_title": "Senior Backend Engineer - Chaos Engineering",
        "question": "# Screening Question\n\n**You're designing a chaos experiment to test the resilience of a microservices platform. Walk me through how you would approach injecting a network partition between two critical services for 2 minutes, then validate that the system recovered correctly. What would you monitor, and how would you ensure you don't accidentally take down the entire platform?**\n\n---\n\n## Why This Question Works\n\n**Practical & Open-Ended:** Tests real chaos engineering thinking\u2014not just tool knowledge\n- Covers blast radius control (critical for senior level)\n- Requires understanding of observability (Prometheus/Grafana)\n- Allows candidates to discuss Chaos Mesh implementation naturally\n\n**Senior-Level Indicators to Listen For:**\n- Mention of blast radius limiting (namespace selectors, label targeting)\n- Observability strategy (what metrics matter, when to abort)\n- Recovery validation (how do you know when it's safe?)\n- Rollback/circuit breaker mentality\n- Discussion of staged rollout or dry-run approaches\n\n**Time-Appropriate:** 2-5 minute response with natural follow-ups available (Chaos Mesh manifest details, Go-based tool",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Directly tests core chaos engineering skills (failure injection, blast radius control, recovery validation) matching role; crystal clear with specific scenario, duration, and constraints; strong separation via trade-offs in monitoring, safety, and validation\u2014senior indicators like namespace targeting distinguish experts. Minor ding for assuming microservices without role confirmation, but excellent overall.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-475",
        "role_title": "Junior Backend Engineer - Search Analytics",
        "question": "# Technical Screening Question\n\n**\"Walk me through how you would track and analyze search queries in a system. Specifically, if you wanted to identify which search terms users click on the most, what data would you need to collect, and how would you structure it in a database?\"**\n\n---\n\n## Evaluation Rubric\n\n**Strong Answer (Hire Signal):**\n- Mentions capturing both query text and click events with timestamps\n- Suggests linking queries to results clicked via some identifier (query ID, session ID, etc.)\n- Proposes aggregating/counting clicks by query term\n- References a reasonable schema (e.g., separate tables for queries and clicks, or a denormalized approach)\n\n**Acceptable Answer (Proceed):**\n- Understands the need to log queries and clicks\n- Can describe a basic approach to counting popular queries\n- May lack detail on schema design but shows foundational thinking\n\n**Weak Answer (Red Flag):**\n- Only mentions logging queries without connecting to user actions\n- No consideration of how to relate queries to clicks\n- Vague on database structure or data relationships\n\n---\n\n## Why This Question Works\n\n\u2713 Tests real, day-",
        "scores": {
          "relevance": 9,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core search analytics skills (query logging, click tracking) for the role (-1 for no specific tech stack mention). Clarity: Clear intent and specific focus, but lacks explicit constraints like scale or format (8). Discriminative: Strong separation via trade-offs in data collection, linking, and schema design; rubric shows good candidate differentiation.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-474",
        "role_title": "Backend Engineer - Dashboards API",
        "question": "# Technical Screening Question\n\n**You're building an API endpoint that retrieves a dashboard with templated variables (like `$datasource` and `$environment`). A user requests the dashboard with query parameters `?datasource=prometheus&environment=prod`.**\n\n**Walk me through:**\n1. How would you handle variable substitution in the dashboard template?\n2. What would you consider for caching this endpoint, and what cache invalidation strategy would you use?\n3. What potential issues might arise if multiple dashboards share variable definitions?\n\n---\n\n## Why This Question Works:\n\n\u2705 **Practical & Real** \u2014 Directly reflects work they'd do (templating, variables, caching)  \n\u2705 **Mid-Level Appropriate** \u2014 Tests design thinking beyond just coding syntax  \n\u2705 **Stack-Relevant** \u2014 Naturally leads to discussing Redis caching, PostgreSQL storage, and Grafana patterns  \n\u2705 **Multi-Layered** \u2014 Allows candidates to show depth while keeping it conversational (2-5 min answer)  \n\u2705 **Reveals Problem-Solving** \u2014 Shows how they think about performance, data consistency",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core dashboard templating, variable substitution, caching, and sharing\u2014matches role exactly (-0 red flags). Clarity: Well-scoped with concrete scenario, clear numbered parts, no ambiguity or bundling issues. Discriminative: Multi-part probes applied knowledge, trade-offs (caching/invalidation), and edge cases (shared vars), strongly separates mid-level thinkers.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-477",
        "role_title": "Senior Backend Engineer - Data Catalog",
        "question": "# Technical Screening Question\n\n**Scenario-based question:**\n\n\"Walk me through how you would design a metadata extraction pipeline that automatically captures lineage information when data flows through multiple Kafka topics into PostgreSQL tables. \n\nSpecifically:\n- How would you identify and track the relationship between source topics and destination tables?\n- What metadata would you extract and store in Apache Atlas?\n- How would you handle schema changes or topic reconfigurations without breaking the lineage?\"\n\n---\n\n## Why this works:\n\n\u2705 **Practical & Role-Specific** \u2014 Directly applies to data catalog, lineage tracking, and the exact tech stack (Kafka \u2192 PostgreSQL \u2192 Atlas)\n\n\u2705 **Senior-Level** \u2014 Requires architectural thinking, not just tool knowledge; tests design decisions and trade-offs\n\n\u2705 **Answerable in 2-5 mins** \u2014 Candidate can outline approach without writing code; allows follow-ups to probe depth\n\n\u2705 **Reveals Real Experience** \u2014 Distinguishes between someone who's built similar systems vs. someone who's only read documentation",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfect match to role's data catalog, lineage tracking, and exact tech stack (Kafka, PostgreSQL, Atlas). Clarity: Crystal clear scenario with specific sub-questions; minor deduction for lacking explicit time/format constraints. Discriminative: Excellent at testing architectural trade-offs, real-world handling of schema changes, and deep understanding vs. superficial knowledge.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-471",
        "role_title": "Staff Engineer - Zero Trust Architecture",
        "question": "# Screening Question\n\n**You're designing a zero trust architecture for a large microservices platform running on Kubernetes. Your team wants to enforce mutual TLS (mTLS) between all services, but you've identified a bootstrapping problem: when a new service pod starts, it needs a valid certificate to communicate, but it can't authenticate to get one without already having credentials.**\n\n**Walk me through how you'd solve this using SPIFFE, and explain the trade-offs of your approach compared to alternatives like sidecar injection or init containers.**\n\n---\n\n## Why This Question Works\n\n**For Staff Level:**\n- Requires systems thinking (bootstrapping paradox isn't trivial)\n- Tests understanding of SPIFFE's design philosophy, not just syntax\n- Expects them to articulate trade-offs\u2014a staff-level responsibility\n\n**Practical & Scoped:**\n- Solves a real problem they'll face on day one\n- Answerable in 2-5 minutes without whiteboarding\n- Reveals how they think about identity federation\n\n**What You're Listening For:**\n- \u2705 Mention of **Workload API** and how SPIFFE si",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Relevance: Perfectly matches role's zero trust, mTLS, and identity focus; directly tests SPIFFE for microservices auth. Clarity: Crystal clear with precise scenario and scope, minor truncation in 'why' section irrelevant. Discriminative: Excellent at separating via systems thinking, trade-offs, and deep SPIFFE understanding\u2014no recall pitfalls.",
          "composite": 9.666666666666666
        }
      },
      {
        "role_id": "be-478",
        "role_title": "Backend Engineer - Cost Attribution",
        "question": "# Technical Screening Question\n\n**Scenario:** Your cost attribution system needs to allocate costs for a shared Kubernetes cluster across 10 teams. Some pods have clear team labels, but about 30% lack proper tagging. Additionally, you need to handle shared infrastructure costs (control plane, networking) that don't map to individual pods.\n\n**Question:** Walk me through how you would design a solution to:\n1. Handle the untagged pods in a way that's fair but traceable\n2. Allocate shared infrastructure costs across teams\n3. What data would you store in PostgreSQL, and how would you structure it to make cost queries flexible?\n\n---\n\n## Why This Works:\n\n\u2705 **Tests practical mid-level experience** \u2013 Real problem they'll face, not theoretical  \n\u2705 **Covers core stack** \u2013 Kubernetes labels, PostgreSQL schema design, cost logic  \n\u2705 **Reveals decision-making** \u2013 How they balance accuracy, traceability, and maintainability  \n\u2705 **Answerable in timeframe** \u2013 High-level design, not implementation details  \n\u2705 **Role-appropriate** \u2013 Directly tied to \"cost",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (K8s tagging, cost allocation, PostgreSQL for reports); no red flags (-0). Clarity: Well-scoped scenario with 3 clear sub-questions; no bundling or ambiguity issues (-0). Discriminative: Probes trade-offs in fairness/traceability, allocation logic, and schema design for flexibility; strong separation of deep thinkers (-0). Exceptional question for the role.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-480",
        "role_title": "Junior Backend Engineer - Deployment Automation",
        "question": "# Screening Question\n\n**You need to deploy a web application to three different AWS environments (dev, staging, production). Each environment requires different configuration settings (database URLs, API keys, logging levels, etc.), but the application code itself remains the same.**\n\n**How would you approach this using Ansible, and what would be your strategy for managing these environment-specific configurations?**\n\n---\n\n## Why This Question Works:\n\n\u2705 **Practical & Real-World** \u2014 Directly mirrors actual deployment scenarios  \n\u2705 **Appropriate Level** \u2014 Doesn't require advanced expertise, but shows if they understand core concepts  \n\u2705 **Covers Key Skills** \u2014 Tests understanding of Ansible, configuration management, and deployment strategy  \n\u2705 **Open-Ended** \u2014 Allows candidates to demonstrate their thinking process  \n\u2705 **Time-Appropriate** \u2014 Answerable in 2-5 minutes with room for follow-ups\n\n---\n\n## What You're Looking For:\n\n**Strong Answer Indicators:**\n- Mentions Ansible inventory groups (dev, staging, prod)\n- References `group_vars` or `host_vars` for environment-specific configs\n-",
        "scores": {
          "relevance": 7,
          "clarity": 9,
          "discriminative": 8,
          "reasoning": "Relevance: Tests core configuration management and deployment skills for the role, but Ansible isn't explicitly listed in role description (-2); not generic but assumes specific tool. Clarity: Crystal clear scenario, well-scoped, precise expectations. Discriminative: Requires applied Ansible knowledge and trade-offs (e.g., group_vars strategy), distinguishes candidates familiar with practical deployment from theorists.",
          "composite": 8.0
        }
      },
      {
        "role_id": "be-479",
        "role_title": "Principal Engineer - Data Architecture",
        "question": "# Screening Question\n\n**You're architecting a system that needs to handle both real-time transactional writes (millions of orders/day) and complex analytical queries (daily reports on 6+ months of historical data). Walk me through how you'd approach this problem\u2014what are the key trade-offs you'd consider, and how would PostgreSQL and ClickHouse fit into your solution?**\n\n---\n\n## Why This Question Works\n\n**Tests Principal-Level Thinking:**\n- Requires understanding of OLTP vs OLAP tradeoffs (not just knowing the difference)\n- Evaluates strategic decision-making, not just technical knowledge\n- Appropriate for someone who will guide architecture decisions for a team\n\n**Practical & Realistic:**\n- Common real-world scenario (lambda/kappa architecture patterns)\n- Lets candidate showcase depth in the tech stack provided\n- Can discuss: data pipeline design, eventual consistency, tool selection rationale\n\n**Time-Appropriate:**\n- Can be answered substantively in 2-5 minutes\n- Natural follow-ups available (Kafka's role, scaling considerations, etc.)\n- No \"gotcha\" trivia\u2014open-ended enough",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's data architecture for analytics/transactional workloads, directly tests PostgreSQL/ClickHouse in context (no red flags). Clarity: Crystal clear scenario and scope, minor deduction for no explicit time/format constraints. Discriminative: Excellent at probing trade-offs, strategic thinking, and applied knowledge to separate principal-level candidates from others.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-481",
        "role_title": "Backend Engineer - Canary Deployments",
        "question": "# Screening Question\n\n**You're implementing a canary deployment where you need to automatically rollback if the new version's error rate exceeds the stable version's by more than 5%. Walk me through how you'd set this up with Istio and Prometheus\u2014specifically, what metrics you'd query, how you'd compare them, and where you'd implement the decision logic.**\n\n---\n\n## Why This Works\n\n**Practical Application:** Tests real canary deployment thinking\u2014the candidate must understand the full feedback loop from metrics collection through automated decision-making.\n\n**Appropriate Depth for Mid-Level:** \n- Assumes familiarity with Istio traffic splitting and Prometheus queries\n- Doesn't require deep implementation details, but reveals gaps quickly\n- Mid-level engineers should know *where* logic lives (Prometheus rule, ArgoCD hook, custom controller)\n\n**Reveals Key Competencies:**\n- Metric selection (error rate, how to calculate per-version)\n- Istio concepts (VirtualService traffic weights, how to query per-canary metrics)\n- Integration thinking (how do these components talk?)\n- Automation mindset (not manual monitoring)\n\n**",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core canary deployment skills (traffic shifting, metrics, rollback) with exact tech stack (Istio, Prometheus) from role; no red flags (-0). Clarity: Precise scope on metrics, comparison, logic placement; well-formed for screening, no bundling or vagueness (-0). Discriminative: Requires applied knowledge of integrations, trade-offs in metric selection/automation; distinguishes mid-level expertise effectively (-0).",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-483",
        "role_title": "Backend Engineer - WAF Integration",
        "question": "# Screening Question\n\n**You're integrating ModSecurity into an Nginx reverse proxy to protect an API. The security team reports that legitimate requests from a partner's webhook service are being blocked by ModSecurity rules, causing false positives. Walk me through your approach to diagnosing and resolving this issue while maintaining security.**\n\n---\n\n## Why This Works\n\n**Practical depth:** Tests real-world WAF troubleshooting that mid-level engineers encounter regularly\n\n**Seniority-appropriate:** Requires understanding of:\n- ModSecurity rule mechanics and logging\n- Nginx configuration and request flow\n- How to balance security vs. availability\n- Root cause analysis methodology\n\n**Reveals important competencies:**\n- Can they interpret WAF logs effectively?\n- Do they understand rule exclusions and tuning?\n- How do they approach security trade-offs?\n- Can they communicate with security teams?\n\n**Answerable in timeframe:** Candidates can outline key steps (check audit logs \u2192 identify triggering rule \u2192 implement exception) without needing to write code",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core WAF integration, rule management, and balancing security/availability from role (-1 for assuming ModSecurity/Nginx not explicitly listed). Clarity: Crystal clear scenario with well-scoped troubleshooting task. Discriminative: Requires applied knowledge of logs, rules, exclusions, and trade-offs, strongly separating capable engineers.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-484",
        "role_title": "Staff Engineer - Model Registry",
        "question": "# Technical Screening Question\n\n**\"Walk me through how you would design a model registry system that handles concurrent model uploads from multiple teams while maintaining version integrity and preventing accidental deployments of untested models. What are the key components you'd implement, and where would you expect bottlenecks or failure modes?\"**\n\n---\n\n## Why This Question Works:\n\n**Practical & Architectural:** Tests real-world system design rather than memorized facts about MLflow.\n\n**Seniority-Appropriate:** Expects staff-level thinking about concurrency, governance, and trade-offs\u2014not just implementation details.\n\n**Conversational:** Naturally opens follow-ups on:\n- Database schema decisions (PostgreSQL locking, transactions)\n- Workflow orchestration (approval gates, automation)\n- Scalability concerns (S3 artifacts, metadata storage)\n- Safety mechanisms (version tags, rollback strategies)\n\n**Time-Bound:** Candidate can outline a solid answer in 2-5 minutes, with depth emerging naturally in discussion.\n\n**Tech Stack Coverage:** Touches Python/MLflow, PostgreSQL, S3, and implicit Kubernetes deployment concerns without forcing artificial references.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Directly tests core role skills in model registry architecture, versioning, deployment automation, and governance. Crystal clear with focused scope on concurrency, integrity, and failure modes. Excellent at probing trade-offs, bottlenecks, and deep architectural thinking to separate strong architects from average engineers. Minor ding on relevance for not explicitly naming role's tech stack (-1).",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-485",
        "role_title": "Junior Backend Engineer - Payment Webhooks",
        "question": "# Technical Screening Question\n\n**\"Walk me through how you would implement idempotency for a payment webhook handler. Specifically, what would you store, where would you store it, and how would you handle a duplicate webhook arriving?\"**\n\n---\n\n## Why This Question Works:\n\n\u2705 **Tests core competency** - Idempotency is fundamental to payment processing and directly mentioned in the role focus\n\n\u2705 **Practical and realistic** - Handles a real problem they'll face (payment providers retry webhooks)\n\n\u2705 **Right difficulty level** - Junior engineers may not know the answer, but can reason through it with guidance\n\n\u2705 **Opens conversation** - Allows follow-ups about trade-offs (Redis vs. DB), TTL strategies, or race conditions\n\n\u2705 **Evaluates system thinking** - Shows if they understand *why* this matters (financial accuracy, duplicate charges)\n\n---\n\n## What You're Looking For:\n\n- **Good answer**: \"Store the webhook ID in Redis/DB, check before processing, return success if duplicate\"\n- **Great answer**: Mentions TTL, eventual consistency, or idempotency keys from payment provider",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests idempotency, a core skill explicitly listed in role description for payment webhooks (no red flags). Clarity: Crystal clear and well-scoped, minor deduction for lacking explicit constraints like tech stack. Discriminative: Strong separation via applied knowledge, trade-offs (storage, TTL, duplicates), and reasoning; not trivial recall.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-486",
        "role_title": "Backend Engineer - Span Processing",
        "question": "# Screening Question\n\n**You're designing a tail sampling processor in Go that needs to decide whether to keep or drop traces based on error rates and latency percentiles. The sampler receives spans from Kafka and must make decisions within 100ms to avoid blocking the pipeline.**\n\n**Walk me through:**\n\n1. **How would you structure the sampler to aggregate span data efficiently?** (What data do you need to track, and how do you avoid memory bloat?)\n\n2. **What challenges arise when sampling decisions depend on data you might not have received yet?** (e.g., a trace's final latency or whether it contains errors)\n\n3. **How would you handle the case where a sampling decision arrives too late\u2014after some spans have already been dropped?**\n\n---\n\n## Why This Question Works\n\n- **Practical**: Tests real pain points in building observability pipelines\n- **Multi-layered**: Can't just recite OpenTelemetry docs\u2014requires systems thinking\n- **Level-appropriate**: Mid-level engineers should know basic concepts but might struggle with the edge cases\n- **Time-bounded**: Candidate can answer parts of it at different depths (2-5 min",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (tail sampling, span processing) with Go, Kafka, and distributed tracing constraints; minor ding for not mentioning storage/attributes (-1). Clarity: Well-scoped with 100ms limit, clear multi-part structure without bundling overload. Discriminative: Probes deep systems thinking, trade-offs (memory, incomplete data, late decisions), strong separation beyond rote knowledge.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-482",
        "role_title": "Senior Backend Engineer - API Monetization",
        "question": "# Screening Question\n\n**\"Walk me through how you would design a rate-limiting and quota enforcement system for an API monetization platform. Specifically, consider: (1) how you'd track usage across different pricing tiers in real-time, (2) where you'd store this data, and (3) how you'd handle the trade-off between accuracy and performance when a customer approaches their quota limit.\"**\n\n---\n\n## Why This Question Works\n\n**Practical & Relevant:** Directly addresses core responsibilities (quota management, usage tracking) without being contrived.\n\n**Seniority-Appropriate:** Senior engineers should think about architectural trade-offs, not just implement a basic counter. Expect discussion of:\n- Redis for low-latency tracking vs. PostgreSQL for durability\n- Eventual consistency vs. strict enforcement\n- Rate limiting algorithms (sliding window, token bucket, etc.)\n- Handling distributed systems challenges\n\n**Reveals Key Competencies:**\n- System design thinking\n- Understanding of Go/Redis/PostgreSQL in real-world constraints\n- Awareness of billing/monetization edge cases (overage handling, billing cycles)\n- Communication of complex trade-offs\n\n**",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core needs (quota management, usage tracking, pricing tiers) with no red flags. Clarity: Crystal clear, well-scoped with specific numbered points, minor deduction for no explicit time/format constraints. Discriminative: Excellent at probing trade-offs, distributed systems thinking, and tech choices (Redis/PostgreSQL implied), strongly separates strong candidates.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-489",
        "role_title": "Backend Engineer - Data Quality",
        "question": "# Technical Screening Question\n\n**Scenario-based question:**\n\n\"You're building a data quality check in Great Expectations for a critical fact table in Snowflake that's loaded daily via Airflow. The table should have at least 100k rows, but this morning it only has 50k rows. Walk me through:\n\n1. How would you structure a Great Expectations validator to catch this volume anomaly?\n2. What would you check first in Airflow to diagnose the root cause?\n3. How would you prevent this from silently failing again?\"\n\n---\n\n## Why This Works\n\n\u2705 **Practical & Real:** Mid-level engineers face this exact scenario  \n\u2705 **Multi-faceted:** Tests Great Expectations, Airflow troubleshooting, and systems thinking  \n\u2705 **Observable Depth:** Reveals whether they understand:\n- Data quality framework mechanics (not just syntax)\n- Pipeline debugging methodology\n- Proactive monitoring vs. reactive fixes\n\n\u2705 **Time-bound:** Can be answered substantively in 3-4 minutes  \n\u2705 **Seniority-appropriate:** Doesn't require deep",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core data quality (volume anomalies) and key tools (Snowflake, Airflow) from role; minor ding as Great Expectations unmentioned but fits perfectly (-1). Clarity: Crystal clear scenario with numbered parts, well-scoped for screening. Discriminative: Tests applied knowledge, troubleshooting methodology, and trade-offs (reactive vs proactive), strongly separating mid-level expertise.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-487",
        "role_title": "Senior Backend Engineer - Search Ranking",
        "question": "# Technical Screening Question\n\n**\"Walk me through how you'd approach building a learning-to-rank model for search relevance. Specifically, what features would you engineer, and how would you handle the challenge of getting reliable training labels at scale?\"**\n\n## Why This Question Works:\n\n\u2713 **Tests Core Competency** - Directly assesses learning-to-rank expertise and understanding of the full pipeline\n\n\u2713 **Practical & Realistic** - Mirrors actual senior-level decisions (feature selection, labeling strategies)\n\n\u2713 **Multiple Dimensions** - Reveals:\n- Feature engineering thinking (BM25 scores, click data, semantic similarity, etc.)\n- Label generation approaches (clicks, ratings, implicit signals, crowdsourcing tradeoffs)\n- Awareness of real-world constraints (scale, data quality, cold-start problems)\n\n\u2713 **Senior-Level Depth** - Expects thoughtful tradeoffs and system-level thinking, not just textbook answers\n\n\u2713 **Natural Follow-ups** - Easy to probe deeper:\n- \"How would you validate model quality before A/B testing?\"\n- \"What do you",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core focus on learning-to-rank pipelines, feature engineering, and implicit label challenges (no red flags). Clarity: Crystal clear and well-scoped for screening, minor deduction for lacking explicit time/format constraints. Discriminative: Tests deep applied knowledge, trade-offs (e.g., labeling at scale), and senior-level thinking to separate strong candidates.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-488",
        "role_title": "Backend Engineer - Gaming Backend",
        "question": "# Technical Screening Question\n\n**Design a real-time state synchronization system for a multiplayer game where 100 players are in the same match. A player performs an action (e.g., moves their character), and all other players need to see that update within 100ms.**\n\n**Walk me through:**\n1. How would you structure the message flow from one client's action to all other clients?\n2. Where would you use Redis vs. PostgreSQL in this system?\n3. What's one potential issue with naive broadcasting, and how would you handle it?\n\n---\n\n## Why This Question Works\n\n\u2713 **Practical & Concrete** \u2014 Directly mirrors the role's core responsibility  \n\u2713 **Mid-Level Appropriate** \u2014 Doesn't require deep systems knowledge, but reveals architectural thinking  \n\u2713 **Tech Stack Validation** \u2014 Naturally incorporates Go, WebSocket, Redis, and PostgreSQL  \n\u2713 **Time-Bound** \u2014 Candidate can outline a solid approach in 3-4 minutes  \n\u2713 **Reveals Gaps** \u2014 Shows whether they've thought about latency, consistency, and scaling\n\n---\n\n## What",
        "scores": {
          "relevance": 7,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Tests core state synchronization but assumes unlisted tech (Go/WebSocket; Redis/PostgreSQL mentioned but not confirmed in role; -2 for extras). Clarity: Crystal clear, well-scoped with numbered parts and 100ms constraint. Discriminative: Excellent at probing architecture, trade-offs (e.g., naive broadcasting pitfalls), and DB choices to separate strong thinkers from basics.",
          "composite": 8.333333333333334
        }
      },
      {
        "role_id": "be-490",
        "role_title": "Senior Backend Engineer - Environment Management",
        "question": "# Technical Screening Question\n\n**\"Walk us through how you'd design a Kubernetes namespace isolation strategy for preview environments that need to be automatically cleaned up after 48 hours. What would you consider for resource quotas, network policies, and the TTL mechanism itself?\"**\n\n---\n\n## Why This Question Works\n\n**Practical & Real:** Directly mirrors the job's core responsibility of building an environment management system.\n\n**Multi-layered:** Tests understanding across several domains:\n- Kubernetes architecture (namespaces, resource quotas, network policies)\n- Infrastructure automation (TTL cleanup logic)\n- System design thinking (tradeoffs between isolation, cost, complexity)\n\n**Seniority-appropriate:** Expects a senior engineer to think holistically about constraints and edge cases, not just implement a feature.\n\n**Time-boxed:** Can be answered substantively in 2-5 minutes while allowing for follow-ups on specifics (e.g., \"How would you handle in-flight requests during cleanup?\" or \"Why network policies vs. service mesh?\").\n\n**Signals you're listening for:**\n- Understanding of resource limits and quota enforcement\n- Awareness of cleanup complexity (final",
        "scores": {
          "relevance": 10,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core needs (namespace isolation, quotas, TTL cleanup) with exact Kubernetes tech. Clarity: Clear intent but lacks explicit constraints like timebox or answer format (-2 for 'design a system' style). Discriminative: Excellent at testing trade-offs, applied K8s knowledge, and edge cases to separate seniors from juniors; appropriately scoped for screening.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-491",
        "role_title": "Junior Backend Engineer - Read Replicas",
        "question": "# Screening Question\n\n**Scenario & Question:**\n\nYour application uses PostgreSQL with a read replica. You need to route a user's login request to the primary database, but route their \"view order history\" request to a read replica to reduce load.\n\nWrite pseudocode or a short Python example showing:\n1. How you'd structure the routing logic to direct queries to primary vs. replica\n2. One potential issue that could occur and how you'd handle it\n\n---\n\n## Why This Question Works\n\n\u2713 **Practical**: Directly mirrors the job responsibility (query routing)  \n\u2713 **Junior-appropriate**: Tests understanding of *why* read replicas are useful and basic routing patterns, not deep system design  \n\u2713 **Time-bound**: Can sketch a solution in 2-5 minutes  \n\u2713 **Reveals thinking**: Shows if candidate understands read-write separation, and whether they think about edge cases (e.g., replication lag where users see stale data)  \n\u2713 **Tech-relevant**: Uses Python + PostgreSQL context from the stack\n\n---\n\n## What You're Looking For\n\n**Good signals:**\n- Routes writes to primary",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core focus on read replica routing and replication lag (no red flags, directly tests PostgreSQL context). Clarity: Crystal clear scenario, well-scoped for screening with numbered asks (minor -1 for lacking explicit time/format constraint). Discriminative: Strong separation via applied routing logic + edge case reasoning (e.g., lag handling), reveals junior-level trade-offs beyond recall.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-493",
        "role_title": "Staff Engineer - Reliability Standards",
        "question": "# Screening Question\n\n**Describe a time when you designed or implemented a production readiness standard or checklist for a team. Walk me through:**\n\n1. **What specific reliability gaps or incidents** prompted you to create it?\n2. **What 3-4 key criteria did you include**, and why those over others?\n3. **How did you handle resistance or adoption challenges** from engineering teams?\n4. **What metrics or signals told you it was actually working?**\n\n---\n\n## Why This Question Works\n\n\u2705 **Practical & Scoped** \u2013 Tests real-world experience with tangible artifacts (standards/checklists), not theoretical SRE knowledge\n\n\u2705 **Staff-Level Signal** \u2013 Distinguishes between someone who *follows* standards vs. someone who *designs and drives adoption* of them across teams\n\n\u2705 **Reveals Key Competencies:**\n- Diagnosis of reliability problems\n- Prioritization and decision-making\n- Stakeholder influence and change management\n- Measurement and iteration mindset\n\n\u2705 **Conversational** \u2013 Opens natural follow-ups on specific choices (observability, deployment safety, runbook quality, etc",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Directly tests core role responsibilities (defining reliability standards, production readiness reviews, incident frameworks) with behavioral depth on design, adoption, and measurement. Crystal clear structure with numbered prompts; highly discriminative for staff-level SRE experience distinguishing implementers from leaders. Minor subtract for bundled sub-questions but well-scoped overall.",
          "composite": 9.666666666666666
        }
      },
      {
        "role_id": "be-492",
        "role_title": "Backend Engineer - CD Pipelines",
        "question": "# Screening Question\n\n**Describe how you would structure a GitOps-based CD pipeline for promoting an application across dev \u2192 staging \u2192 production environments using ArgoCD. Walk us through your approach to handling approval gates and rollbacks.**\n\n---\n\n## Evaluation Rubric\n\n**Strong Answer (Hire):**\n- Explains separate ArgoCD Application manifests per environment\n- Mentions Git as source of truth and how promotion works (e.g., updating image tags in Git)\n- Describes approval mechanisms (manual sync, pull request reviews, or notification integrations)\n- Addresses rollback strategy (Git history, previous commits, or ArgoCD sync to prior state)\n- Shows awareness of practical concerns (drift detection, secrets management)\n\n**Acceptable Answer (Proceed):**\n- Outlines basic environment separation in ArgoCD\n- Explains Git-driven promotion workflow\n- Mentions one approval/rollback mechanism\n- May lack depth on operational edge cases\n\n**Weak Answer (Pass):**\n- Vague about ArgoCD mechanics or how Git triggers deployments\n- No clear approval or rollback strategy\n- Confuses GitOps with traditional CI/",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core GitOps CD pipelines, multi-environment promotion, approvals, and rollbacks with ArgoCD (exact match to role; no red flags). Clarity: Crystal clear and well-scoped for screening, minor deduction for no explicit time/format constraints. Discriminative: Excellent separation via trade-offs (approvals, rollbacks, drift), supported by rubric; not trivial recall or overly broad.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-495",
        "role_title": "Senior Backend Engineer - Audit Logging",
        "question": "# Screening Question\n\n**Design an audit log entry that must be tamper-proof and queryable. Walk me through:**\n\n1. **What data would you include** in each audit log entry beyond the obvious (who, what, when)?\n2. **How would you detect tampering** if someone modified a log entry in the database?\n3. **What's the tradeoff** between immutability guarantees and query performance on large datasets?\n\n**Follow-up:** If you had to choose between storing logs in PostgreSQL vs. ClickHouse for this use case, what factors would drive your decision?\n\n---\n\n## Why This Question Works\n\n\u2705 **Practical & Specific** \u2014 Tests real decisions they'd make on day one (hashing chains, checksums, columnar vs. row storage)\n\n\u2705 **Senior-Level** \u2014 Expects understanding of tradeoffs, not just textbook answers (compliance needs vs. query speed)\n\n\u2705 **Security-Focused** \u2014 Directly assesses tamper-proof thinking without being a riddle\n\n\u2705 **Time-Appropriate** \u2014 Can answer core concept in 2-3 min, elaborate in",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core audit logging skills (tamper-proofing, querying, retention implications) for the role (-0 red flags). Clarity: Well-structured with numbered parts and scoped follow-up, unambiguous for screening. Discriminative: Probes trade-offs, applied security knowledge, and DB choice factors, strongly separating senior candidates who understand hashing/immutability vs. columnar storage.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-494",
        "role_title": "Backend Engineer - API Mocking",
        "question": "# Screening Question\n\n**Scenario:**\n\nYou're building an API mock for a payment service that needs to:\n1. Match requests by specific headers (e.g., `X-API-Key`) and request body fields\n2. Return different responses based on the matched request\n3. Simulate realistic delays and occasional failures\n\n**Question:**\n\nWalk me through how you would configure a WireMock stub to handle a POST request to `/api/payments` that:\n- Only matches when `X-API-Key` header is present AND the request body contains `\"amount\": 100`\n- Returns a success response (HTTP 200) with a transaction ID\n- Returns a failure response (HTTP 400) for requests with `\"amount\": 0`\n\nWhat challenges might you encounter if you later needed to verify that a client called this endpoint with the correct payload during testing?\n\n---\n\n## What This Assesses\n\n\u2705 **Practical WireMock knowledge** \u2013 Request matching strategies and stub configuration  \n\u2705 **API mocking reasoning** \u2013 Understanding when/why to use specific matching rules  \n\u2705 **Contract testing awareness** \u2013 Recognizing the need for verification",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 8,
          "reasoning": "Relevance: Directly tests WireMock for API mocking, core to role's request matching/response templating; minor deduct for not explicitly covering contract verification (-1). Clarity: Precise scenario, unambiguous matching/response specs, well-scoped. Discriminative: Requires applied WireMock config knowledge and foresight on verification challenges (e.g., statefulness, assertions); good separation but not elite trade-off depth.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-496",
        "role_title": "Junior Backend Engineer - Model Serving",
        "question": "# Screening Question\n\n**You need to create a FastAPI endpoint that accepts an image, runs it through a pre-trained ML model, and returns predictions. The model takes 2 seconds to load from disk, but your endpoint might receive multiple requests per second.**\n\n**Walk me through:**\n1. How would you avoid reloading the model on every request?\n2. What would happen if two requests arrive simultaneously and hit your endpoint?\n3. How would you handle a request that times out while waiting for model inference?\n\n---\n\n## Why This Works\n\n\u2705 **Practical**: Tests real deployment scenarios (model lifecycle, concurrency, timeouts)  \n\u2705 **Appropriate Level**: Junior engineer should know basic patterns; doesn't require advanced optimization  \n\u2705 **Reveals Thinking**: Shows understanding of FastAPI startup events, async/concurrency, and basic operational concerns  \n\u2705 **Conversational**: Naturally leads to follow-ups (model size, batching, monitoring) without being yes/no\n\n## What You're Looking For\n\n- **Strong answer**: Loading model in startup event, async handlers, request timeouts via `timeout` parameter\n- **Good answer",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core ML model deployment skills (loading, concurrency) for the entry-level API role, matches FastAPI implicitly via context (-0 red flags). Clarity: Well-structured with 3 focused sub-questions, unambiguous scope for screening (-0 red flags). Discriminative: Probes applied knowledge of startup events, async handling, and timeouts with trade-offs, strongly separates candidates revealing practical thinking.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-497",
        "role_title": "Backend Engineer - Payout Systems",
        "question": "# Technical Screening Question\n\n**You're building a payout system that needs to process 100,000 seller payouts daily. Each payout requires:**\n- Validating the seller's bank account against an external bank API\n- Calculating final amounts (accounting for fees and adjustments)\n- Storing the transaction record\n- Publishing an event for downstream systems\n\n**Walk me through your approach:**\n1. How would you structure this to avoid bottlenecks?\n2. What would you consider the critical path vs. non-critical operations?\n3. How would you handle failures in the bank validation API?\n\n---\n\n## Why This Works\n\n\u2705 **Practical**: Mirrors real payout system design decisions  \n\u2705 **Mid-level appropriate**: Requires understanding of batch processing, system design, and fault tolerance\u2014not deep expertise  \n\u2705 **Tech-relevant**: Opens discussion on async processing (Kafka), caching (Redis), and database design (PostgreSQL)  \n\u2705 **Time-bound**: Can be answered in 3-4 minutes with follow-ups  \n\u2705 **Reveals thinking**: Shows whether they understand concurrency, resilience",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core payout system skills (batch processing, validation, reconciliation) matching role; minor ding for assuming unlisted tech like Kafka/Redis (-1). Clarity: Well-structured with clear scenario, numbered parts, scoped for screening. Discriminative: Probes trade-offs in bottlenecks, critical path, failures\u2014reveals deep system design thinking and resilience understanding.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-499",
        "role_title": "Principal Engineer - Search Platform",
        "question": "# Screening Question\n\n**\"Tell me about a time you designed or significantly improved a search infrastructure that handled scale challenges. What were the bottlenecks you identified, how did you prioritize them, and what was the outcome? If you were advising a team on whether to invest in a new search platform versus optimizing their existing one, what factors would influence that decision?\"**\n\n---\n\n## Why This Works\n\n**Tests practical knowledge:**\n- Real experience with search systems at scale\n- Ability to diagnose performance issues (indexing, query latency, throughput)\n- Understanding of tradeoffs in search architecture\n\n**Appropriate for Principal level:**\n- Requires strategic thinking beyond just technical implementation\n- Assesses mentorship/advisory capability (\"advising a team\")\n- Evaluates decision-making frameworks, not just tools\n\n**Reveals key competencies:**\n- Search platform expertise (core requirement)\n- Systems thinking (bottleneck identification and prioritization)\n- Leadership maturity (guiding technology decisions)\n- Communication (explaining complex tradeoffs clearly)\n\n**Realistic time frame:** 3-4 minutes for a strong answer\n\n**Red flags to listen for:**",
        "scores": {
          "relevance": 9,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core search platform strategy, scale challenges, and advisory decision-making from role description (-1 for no specific tech stack mention). Clarity: Clear behavioral + hypothetical structure, well-scoped for screening (-2 for bundling two related but distinct questions). Discriminative: Strong separation via real experience, bottleneck analysis, prioritization, tradeoffs, and strategic factors; reveals depth vs superficial answers.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-498",
        "role_title": "Senior Backend Engineer - Profiling Infrastructure",
        "question": "# Technical Screening Question\n\n**You're implementing continuous profiling for a Go microservice that processes 10,000 requests/second. The profiling data is being stored in ClickHouse, but engineers are reporting that flame graph queries take 30+ seconds to load when comparing profiles across a 1-hour window.**\n\n**Walk me through how you'd approach diagnosing this performance issue. What are the key factors you'd investigate, and what optimizations might you consider\u2014both at the profiling collection level and the storage/query level?**\n\n---\n\n## Why This Question Works\n\n\u2713 **Practical & Real:** Reflects an actual senior-level challenge in profiling infrastructure  \n\u2713 **Depth:** Tests understanding across the full stack (Pyroscope \u2192 ClickHouse \u2192 Grafana frontend)  \n\u2713 **Seniority-Appropriate:** Expects architectural thinking, not just syntax knowledge  \n\u2713 **Time-Bounded:** Can explore 2-5 minutes; natural stopping points at each layer  \n\u2713 **Open-Ended:** Reveals how they prioritize (sampling rates vs. schema vs. indexing strategy)",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (profiling, flame graphs, storage optimization in ClickHouse) for high-throughput Go services; minor ding for assuming Pyroscope/Grafana not explicitly listed (-1). Clarity: Crystal clear scenario with specific constraints (10k req/s, 1hr window, 30s queries), well-scoped. Discriminative: Excellent at separating seniors via trade-offs (sampling, schema, indexing, collection optimizations); reveals prioritization and depth without being too broad.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-500",
        "role_title": "Staff Engineer - Streaming Architecture",
        "question": "# Screening Question\n\n**You're designing a real-time notification system that needs to fan out a single user event to potentially thousands of subscribers across multiple geographic regions. The system uses Kafka as the event source and must support both WebSocket connections and traditional HTTP polling clients.**\n\n**Walk me through how you'd approach the architecture\u2014specifically:**\n1. **How would you structure the Kafka topic(s) and consumer groups?**\n2. **Where would you use Redis, and for what specific purposes?**\n3. **What delivery guarantee would you target, and what trade-offs does that involve?**\n\n**Feel free to make reasonable assumptions about scale, latency requirements, or failure modes.**\n\n---\n\n## Why This Works\n\n- **Practical depth**: Tests whether they've actually designed multi-consumer streaming systems, not just used them\n- **Appropriate scope**: A Staff engineer should reason through infrastructure tradeoffs (exactly-once vs. at-least-once, centralized vs. distributed fan-out, caching strategy)\n- **Tech stack coverage**: Naturally incorporates Kafka, Redis, and real-time concerns without feeling forced\n- **Reveals thinking**: Their assumptions and tradeoffs",
        "scores": {
          "relevance": 7,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core fan-out and streaming skills from role, but penalizes -2 for mentioning Redis (not in role description) and -1 for generic design feel. Clarity: Crystal clear, well-scoped with specific numbered points and assumption guidance. Discriminative: Excellent at probing trade-offs, applied architecture knowledge, and reasoning depth to separate strong Staff engineers.",
          "composite": 8.333333333333334
        }
      }
    ]
  },
  "baseline": {
    "model": "baseline",
    "num_samples": 100,
    "avg_relevance": 8.5,
    "avg_clarity": 8.04,
    "avg_discriminative": 8.48,
    "avg_composite": 8.340000000000003,
    "results": [
      {
        "role_id": "be-401",
        "role_title": "Backend Engineer - Training Infrastructure",
        "question": "\"Design a scalable and fault-tolerant system to manage distributed machine learning training jobs using Ray and Kubernetes. Your solution should handle job scheduling, GPU resource allocation, and recovery from failures. Provide a high-level architecture, including how Ray clusters are managed, how jobs are scheduled, and how failures are detected and recovered. Explain how you would ensure scalability and fault tolerance across a growing number of training jobs and GPUs.\"",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core needs (distributed ML infra, scheduling, GPU allocation, orchestration) using relevant tools (Ray, K8s); no red flags. Clarity: Crystal clear, well-scoped high-level design with specific components; minor deduction for lacking explicit time/format constraints. Discriminative: Tests deep applied knowledge, trade-offs in scalability/fault-tolerance, architecture decisions; strong separation of strong vs weak candidates; not too broad for screening.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-402",
        "role_title": "Backend Engineer - Model Deployment",
        "question": "\"When deploying a machine learning model using KServe, how would you design a deployment strategy that includes automated rollback in case of a failed inference request, and what role does Redis play in this process?\"",
        "scores": {
          "relevance": 4,
          "clarity": 7,
          "discriminative": 7,
          "reasoning": "Relevance: Low due to KServe and Redis not mentioned in role (red flag -2, generic -3); tests deployment/rollback but wrong specific tech. Clarity: Clear intent and well-formed, but lacks explicit constraints (-1 for minor jargon). Discriminative: Good test of applied design with trade-offs, differentiates ML deployment expertise.",
          "composite": 6.0
        }
      },
      {
        "role_id": "be-403",
        "role_title": "Backend Engineer - Inference Optimization",
        "question": "\"Design a Python script that uses TensorRT to optimize a model for inference latency. The script should:\n- Load a PyTorch model (e.g., ResNet50) and convert it to a TensorRT engine.\n- Apply dynamic batching to reduce GPU memory usage.\n- Include a simple benchmark to measure inference latency before and after optimization.\n- Output a report showing the latency improvement and GPU memory per batch.\"",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core inference optimization skills (quantization implied via TensorRT, batching, GPU memory) for latency-sensitive models; minor ding as role doesn't explicitly list TensorRT but fits perfectly (-1 red flag avoided). Clarity: Crystal clear with specific requirements, well-scoped steps, no ambiguity. Discriminative: Requires applied knowledge of model conversion, optimization techniques, benchmarking, and trade-offs in batching/memory\u2014strong separation of strong vs weak candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-404",
        "role_title": "Senior Backend Engineer - Feature Platform",
        "question": "\"Design a feature pipeline for a real-time feature serving system using Feast and Redis. The pipeline should support both online and offline feature computation. Explain how you would structure the feature store, how you would handle feature versioning, and how you would ensure consistency between online and offline feature values. Include considerations for feature discovery and how you would monitor system health.\"",
        "scores": {
          "relevance": 9,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core feature platform skills (computation, serving, discovery) with Feast (feature store standard); minor ding for specifying Redis (not in role desc, -2) but still highly relevant. Clarity: Well-formed and unambiguous, but 'design a pipeline' lacks explicit time/format constraints (-2). Discriminative: Excellent at probing trade-offs (versioning, consistency), architecture, monitoring; distinguishes deep Feast expertise from superficial knowledge.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-405",
        "role_title": "Senior Backend Engineer - Model Monitoring",
        "question": "\"Design a drift detection system for a production ML model using Python and Prometheus/Grafana. The system should detect data drift, model drift, and concept drift. Explain how you would collect, store, and visualize the metrics. Include a high-level architecture and at least one concrete example of how you'd implement a drift detection algorithm in Python (e.g., Kullback-Leibler divergence, Kolmogorov-Smirnov test, or a statistical test).\"",
        "scores": {
          "relevance": 9,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core role skills (drift detection, monitoring, alerting via Prometheus/Grafana); minor ding for not explicitly covering performance tracking/alerting (-1). Clarity: Well-scoped with specific drifts, components, and example required; lacks minor constraints like time complexity (-1, -1 for design scope). Discriminative: Tests architecture, metrics pipeline, visualization, and concrete algo implementation with trade-offs, strongly separates skilled ML engineers.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-406",
        "role_title": "Senior Backend Engineer - LLM Serving",
        "question": "\"Design and implement a system to optimize token-level batching in vLLM for a high-throughput LLM serving environment. The system should handle dynamic request arrivals, manage KV cache efficiently, and support streaming responses. Provide a high-level design including key components, and describe how you would evaluate its performance and scalability.\"",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core role responsibilities (batching optimization, KV cache, streaming) and specifies vLLM from LLM serving context. Clarity: Well-scoped high-level design with clear requirements, minor deduction for lacking explicit time/format constraints. Discriminative: Tests applied system design, trade-offs in dynamic batching/KV management, and evaluation\u2014strong separation of strong vs weak candidates.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-407",
        "role_title": "Staff Backend Engineer - ML Infrastructure",
        "question": "\"Design a scalable and fault-tolerant ML training pipeline using Ray, Kubernetes, and Spark. Your design should include:  \n- A high-level architecture diagram (text-based or ASCII)  \n- How you would handle data preprocessing and feature engineering  \n- How you would manage training jobs and distributed execution  \n- How you would ensure fault tolerance and recovery  \n- How you would monitor and optimize training jobs for cost and performance\"",
        "scores": {
          "relevance": 7,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Tests core ML infra skills (pipelines, scaling, fault tolerance, optimization) but assumes unmentioned tech (Ray, Spark) and ignores serving/cost strategy focus (-2 for tech not in role). Clarity: Well-structured with specific components, minor vagueness on diagram format and scope. Discriminative: Excellent at probing trade-offs, architecture, and applied ML ops knowledge, strongly separates candidates.",
          "composite": 8.0
        }
      },
      {
        "role_id": "be-408",
        "role_title": "Junior Backend Developer - Health Endpoints",
        "question": "\"Design and implement a health check endpoint in Go that returns a JSON response indicating the status of liveness, readiness, and metrics. The endpoint should:\n- Return a 200 OK status with a JSON body when healthy\n- Return a 503 Service Unavailable status when unhealthy\n- Include a `status` field (value: 'healthy' or 'unhealthy')\n- Include a `liveness` field (value: 'healthy' or 'unhealthy') \n- Include a `readiness` field (value: 'healthy' or 'unhealthy')\n- Include a `metrics` field (value: 'available' or 'unavailable') \n- Be implemented in a minimal Go HTTP server (no external dependencies)\n- Include error handling for unexpected issues\n- Return 503 when the system is unhealthy (e.g., due to a failed internal check)",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 8,
          "reasoning": "Relevance: Directly tests core role skills (liveness/readiness/metrics endpoints) in Go (implied by role context), minor deduct for not explicitly stating Go in question (-1). Clarity: Crystal clear specs, well-scoped for screening, no bundling or ambiguity. Discriminative: Tests applied HTTP/JSON/Go skills with error handling and state logic, distinguishes solid entry-level devs from weak ones; slightly implementation-heavy but feasible.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-409",
        "role_title": "Junior Backend Engineer - File Upload",
        "question": "Design and implement a FastAPI endpoint that allows users to upload a file. The endpoint should:\n1. Accept a multipart form with `file` and `metadata` fields.\n2. Validate the file (ensure it's not empty and has a valid MIME type).\n3. Store the file in AWS S3 using a unique key (e.g., `user/{user_id}/file_{timestamp}.ext`).\n4. Store metadata in a PostgreSQL database.\n5. Return a success response with the file's URL and stored metadata.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core file upload API skills, multipart handling, validation, S3 storage, and DB integration from role (-1 for minor assumption of user_id source, not generic). Clarity: Precise requirements with clear fields, validation, storage, and response specs. Discriminative: Requires integrating FastAPI, AWS SDK, SQLAlchemy/psycopg2, validation logic, error handling, and trade-offs like key generation/security\u2014strong separation of capable entry-level engineers.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-410",
        "role_title": "Backend Engineer - Bulk Operations",
        "question": "\"Design a Go backend API endpoint to handle bulk data imports for a system that manages user profiles. The endpoint should support:\n- Accepting a batch of user profile updates via a JSON array\n- Ensuring idempotency (no duplicate processing of the same user updates)\n- Handling partial success (some updates may succeed, others may fail)\n- Processing the data in a background job via Kafka\n- Using Redis to track processed user IDs to ensure idempotency\n- Returning a detailed response with success/failure counts and error details for each update",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (bulk ops, partial success, idempotency) with specified tech (Go, Kafka, Redis); minor ding for assuming user profiles. Clarity: Crystal clear with precise requirements and scope. Discriminative: Requires applied knowledge of async processing, idempotency patterns, error handling, and tech integration, forcing trade-offs and strong separation.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-411",
        "role_title": "Backend Engineer - Admin APIs",
        "question": "\"Design and implement a secure admin API endpoint in Python using FastAPI that allows an admin user to create a new user account. The endpoint must:\n- Require a valid admin token (JWT) for access\n- Validate the admin token using a secret key\n- Accept a user\u2019s email and password (hashed using bcrypt)\n- Store the user in PostgreSQL with a unique constraint on email\n- Prevent duplicate email creation\n- Log all admin actions (user creation) to a Redis cache with a structured log entry (timestamp, user email, admin token ID, action type), and persist the log to a PostgreSQL audit table\n- Include error handling for invalid tokens, duplicate emails, and other common issues",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core admin API skills (auth, auditing, workflows) and exact tech stack (FastAPI, JWT, bcrypt, PostgreSQL, Redis) from role; minor ding for not explicitly mentioning privileged access beyond JWT (-1). Clarity: Crystal clear with precise requirements and error handling specified; well-scoped for screening. Discriminative: Excellent separation via applied knowledge of security, DB constraints, logging pipeline, and error handling; tests trade-offs in auth/logging without being too broad.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-412",
        "role_title": "Backend Engineer - Analytics APIs",
        "question": "Design and implement a Go-based analytics API that serves time-series data from ClickHouse with caching using Redis. Your solution should support efficient aggregation queries (e.g., daily, hourly) and handle data freshness and cache invalidation. Describe your design decisions, including how you handle cache invalidation, query performance, and data consistency.",
        "scores": {
          "relevance": 7,
          "clarity": 7,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core time-series, aggregation, and caching skills but assumes unlisted tech (Go, ClickHouse, Redis) (-2). Clarity: Clear intent but lacks scoping constraints like time/format (-1). Discriminative: Excellent at probing trade-offs in caching, invalidation, performance, and consistency, strongly separating candidates.",
          "composite": 7.666666666666667
        }
      },
      {
        "role_id": "be-413",
        "role_title": "Senior Backend Engineer - API Gateway Logic",
        "question": "Design and implement a Lua script in Envoy that validates incoming HTTP requests and transforms the request headers based on a custom business rule. The rule is:  \n- If the `X-Client-Id` header is present and starts with `PROD-`, then:\n  - Add a `X-Environment: production` header.\n  - Strip the `X-Client-Id` header and set it to `PROD-<client-id>` (e.g., `PROD-123`).\n- If the `X-Client-Id` header is present and starts with `TEST-`, then:\n  - Add a `X-Environment: staging` header.\n  - Strip the `X-Client-Id` header and set it to `TEST-<client-id>` (e.g., `TEST-123`).\n- If the `X-Client-Id` header is missing, do nothing.\n- All other headers must be preserved.",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core Envoy/Lua skills for API gateway extensions, request transformation, and custom routing rules. No red flags. Clarity: Crystal clear with precise business rules and constraints; minor ambiguity on 'strip and set' intent but well-scoped. Discriminative: Requires applied Lua/Envoy knowledge, header manipulation, conditionals, and edge cases for strong separation; not pure recall.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-414",
        "role_title": "Senior Backend Engineer - Long-Polling",
        "question": "Design and implement a long-polling endpoint in Go that efficiently handles concurrent clients, manages connection timeouts, and gracefully degrades when Redis is unavailable. The endpoint should support:\n1. A simple `POST /poll` that accepts a `pollId` and returns a message or `204 No Content` if no message is available.\n2. Connection timeout handling (e.g., 30 seconds) with automatic reconnection on timeout.\n3. Timeout cleanup after 30 seconds of inactivity.\n4. Fallback to polling every 10 seconds if Redis is unreachable (use a simple retry mechanism).\n5. Connection management using Redis to track active clients and avoid duplicate polling.\n6. Use Redis as a message store (store messages in Redis with TTL of 300 seconds).\n7. Use PostgreSQL to store client session metadata (e.g., `poll_id`, `client_id`, `last_active`, `status`).\n8. Implement a simple health check at `/health` that returns 200 if both Redis and PostgreSQL are reachable.",
        "scores": {
          "relevance": 4,
          "clarity": 3,
          "discriminative": 2,
          "reasoning": "Relevance: Superficially mentions Go and long-polling but heavily diverges into Redis/PostgreSQL management, health checks, and full system design beyond core role focus (-3 generic design, -2 extras not in role). Clarity: Multiple bundled requirements (8 points) make it ambiguous and unscoped for screening (-2); no constraints on implementation depth/format (-2). Discriminative: Far too broad (>10min, full system impl) for screening, tests everything at once with weak separation (-2).",
          "composite": 3.0
        }
      },
      {
        "role_id": "be-415",
        "role_title": "Senior Backend Engineer - Multipart APIs",
        "question": "Design and implement a resumable upload system using FastAPI, Python, and S3. The system should support chunked uploads, store upload metadata in S3, and allow resuming from any point. Include a simple example of how a client would initiate and resume an upload.",
        "scores": {
          "relevance": 10,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core focus on resumable uploads, chunk management, and S3 (core tech stack). Clarity: Clear intent and scope, but lacks explicit constraints (e.g., chunk size limits, time limits for screening). Discriminative: Tests deep applied knowledge of APIs, state management, and trade-offs in resumable systems; strong separation between good and weak candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-416",
        "role_title": "Staff Backend Engineer - External APIs",
        "question": "\"Design a scalable and secure external API using Go and OpenAPI, with Kong as the gateway, deployed on Kubernetes. Your design should include key components, security considerations, rate limiting, logging, and observability. Provide a high-level architectural overview, and explain how you would implement API monetization for partners. Include how you'd structure the API to support partner enablement and integration patterns.\"",
        "scores": {
          "relevance": 9,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core role skills (API strategy, monetization, partner enablement, integration patterns) with relevant tech (Go, OpenAPI, Kong, K8s); minor ding for assuming specific tools not in desc (-1). Clarity: Well-scoped high-level design with clear components listed, but lacks time/format constraints (-1) and bundles multiple aspects. Discriminative: Excellent at testing applied architecture, trade-offs (security, scaling, monetization), and role-specific strategy, strongly separating strong candidates.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-417",
        "role_title": "Junior Backend Developer - Event Sourcing",
        "question": "\"Design a simple event handler for a `UserCreated` event in an event-sourced system using Kafka and PostgreSQL. The event should be consumed by a handler that persists the user's state in a PostgreSQL database. Implement the handler class, describe how it would handle state transitions (e.g., from `UNKNOWN` to `ACTIVE`), and explain how you\u2019d ensure data consistency between Kafka and PostgreSQL.\"",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core event handling, state management, and persistence for event sourcing role (-1 for specifying Kafka/PostgreSQL not in role desc). Clarity: Well-scoped design with specific event, tech, and consistency focus. Discriminative: Requires applied knowledge of handlers, state transitions, and exactly-once semantics, distinguishing strong candidates via trade-offs.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-418",
        "role_title": "Junior Backend Engineer - Service Clients",
        "question": "> \"Design and implement a gRPC client in Go that supports automatic retry with backoff and circuit breaker pattern. The client should handle transient errors (e.g., connection timeout, network errors) and gracefully degrade when the service is unavailable. Use Protobuf for the service interface and Docker to containerize the client. Provide a minimal working example with a simple service interface and a client that demonstrates retry and circuit breaker behavior.\"",
        "scores": {
          "relevance": 3,
          "clarity": 6,
          "discriminative": 7,
          "reasoning": "Relevance: Superficially mentions tech stack (Go, gRPC) but demands full implementation of core skills (retry, circuit breaker) PLUS unrelated extras (Protobuf service, Docker containerization) not in role; generic design question (-3), tests minor/unlisted tech (-2). Clarity: Understandable but vague on retry/backoff params, circuit breaker thresholds, expected depth; multiple components bundled (-2), missing constraints. Discriminative: Good test of applied patterns with trade-offs, but overly broad for screening (>10min effort).",
          "composite": 5.333333333333333
        }
      },
      {
        "role_id": "be-419",
        "role_title": "Backend Engineer - Retry Logic",
        "question": "Imagine you're implementing a retry mechanism for a distributed service that handles order processing. The service may fail due to transient issues like network latency or database timeouts. You need to implement a retry strategy that uses exponential backoff with jitter, and includes a retry budget to prevent infinite retries on persistent failures.",
        "scores": {
          "relevance": 10,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core focus on retry infrastructure, exponential backoff, jitter, and retry budgets. Clarity: Clear scenario and requirements, but lacks explicit constraints (e.g., max retries, time limits, code format). Discriminative: Strong test of applied knowledge and trade-offs in retry design, distinguishes candidates who understand distributed systems retries from those with superficial knowledge.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-420",
        "role_title": "Backend Engineer - Graceful Shutdown",
        "question": "*In a Kubernetes environment, how would you implement a graceful shutdown for a Go gRPC server that handles in-flight requests and supports connection draining, especially when receiving a SIGTERM signal?*",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core role requirements (graceful shutdown, connection draining, in-flight requests, SIGTERM) with exact tech stack (Kubernetes, Go, gRPC). No red flags. Clarity: Precise and well-scoped for screening; minor deduction as it lacks explicit time/format constraints. Discriminative: Requires applied knowledge of signal handling, gRPC lifecycle, and K8s deployment patterns, testing trade-offs effectively without being trivial or overly broad.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-421",
        "role_title": "Senior Backend Engineer - Security Automation",
        "question": "*You're tasked with creating a Python script that automates the rotation of database secrets stored in HashiCorp Vault. The script should:*\n1. *Authenticate to Vault using a Kubernetes service account (Kubernetes `ServiceAccount` with `vault` role bound via `RoleBinding`)*\n2. *Read a list of database secrets from a configuration file (e.g., JSON or YAML)*\n3. *For each secret, generate a new random password (12 characters, lowercase, alphanumeric) and rotate it in Vault*\n4. *Log the operation with timestamps and success/failure status*\n5. *Fail fast if any step fails, and output a human-readable error message*",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core security automation (secrets rotation in Vault) and Python scripting for cloud infra; minor ding as Vault auth is specific but fits role (-1). Clarity: Precise requirements with clear steps, auth method, constraints, and fail-fast policy. Discriminative: Requires applied knowledge of Vault API, K8s auth, error handling, and secure practices; distinguishes via trade-offs in auth/secrets handling.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-422",
        "role_title": "Junior Backend Engineer - API Testing",
        "question": "Write a pytest test for a FastAPI endpoint that returns a list of users. The endpoint should accept a query parameter `page` and `limit`, and return a JSON response with a `users` array and `total` count. Your test should:\n- Use `test_client` to make a request\n- Test the response structure and data\n- Include a fixture for the test database\n- Verify that the query parameters are correctly applied (e.g., page=1, limit=10)\n- Handle edge cases like invalid page or limit values (e.g., negative numbers, non-numeric)",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests pytest, FastAPI testing with test_client, fixtures, and query param handling\u2014core to API test writing role (-1 for not explicitly mentioning contract testing). Clarity: Precise requirements, well-scoped with specific actions and edge cases. Discriminative: Requires applied knowledge of fixtures, client usage, assertions, and edge case reasoning to separate capable entry-level testers.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-423",
        "role_title": "Staff Engineer - Incident Response Platform",
        "question": "> \"Design a system to manage on-call rotations for a team of 10 engineers, where each engineer is on-call for 5 days, then off for 3 days. The system must handle automatic rotation, provide real-time visibility into on-call schedules, and support manual overrides. It should be implemented in Go, with a PostgreSQL database to store schedules and a Slack API to notify engineers when their shift begins and ends. Explain your approach, including data model, how you'd handle edge cases (e.g., engineers on leave, shift overlaps), and how you'd implement the rotation logic.\"",
        "scores": {
          "relevance": 7,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Tests core on-call optimization skill but too narrowly focused on rotations (minor aspect) vs broader incident response/SLO tooling; specifies unmentioned Go/PostgreSQL/Slack (-2). Clarity: Well-scoped with specific constraints/tech, minor ambiguity on full implementation depth. Discriminative: Excellent at probing design, data modeling, edge cases, and logic trade-offs, strongly separates candidates.",
          "composite": 8.0
        }
      },
      {
        "role_id": "be-424",
        "role_title": "Backend Engineer - Message Serialization",
        "question": "\"Design a message serialization system for a distributed system that uses Kafka as the messaging backbone. The system must support schema evolution with backward and forward compatibility, using Avro with Schema Registry. Handle schema validation at both producer and consumer sides. Provide a concrete example of how a schema change would be handled when a new field is added to a message, and explain how the system ensures backward compatibility. Also, describe how you would validate schemas at runtime and what monitoring or alerting strategies you would implement to detect schema mismatches.\"",
        "scores": {
          "relevance": 9,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core schema evolution, compatibility, and validation skills; Kafka/Avro/Schema Registry are spot-on for message serialization infra (-1 for assuming Kafka not explicitly listed). Clarity: Well-formed and unambiguous, but multiple components bundled (-2). Discriminative: Excellent on trade-offs, concrete examples, runtime validation, and monitoring; strong separation of deep vs shallow knowledge.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-425",
        "role_title": "Senior Backend Engineer - Graph Analytics",
        "question": "Design a scalable graph analytics pipeline to compute user-based recommendations using Neo4j and Apache Spark. The pipeline should handle a billion-edge dataset and support real-time updates. Describe how you would structure the pipeline, the data flow, and how you would optimize traversal queries for performance and scalability. Include considerations for handling real-time updates and potential bottlenecks.",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's graph analytics, social/rec pipelines, billion-edge scale, traversal optimization, and implied tech (Neo4j/Spark common for graphs). No red flags. Clarity: Well-scoped with specific tech/data size/updates; minor deduction as no explicit time/format constraints. Discriminative: Probes deep applied knowledge of pipeline design, trade-offs (real-time vs batch, bottlenecks), optimization\u2014strong separation of strong vs weak candidates.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-426",
        "role_title": "Junior Backend Engineer - DevOps Scripting",
        "question": "Write a Bash script that builds and pushes a Docker image to a private Docker registry (e.g., Docker Hub or AWS ECR). The script should:\n- Accept a Docker image name (e.g., \"myapp:latest\") as a command-line argument\n- Validate that the image name is provided and has a valid format (e.g., contains a tag)\n- Use `docker build` to build the image from a Dockerfile in the current directory\n- Push the built image to a private Docker registry using `docker push`\n- Include error handling for common issues (e.g., missing Dockerfile, failed build, failed push)\n- Output a success or failure message at the end",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core DevOps automation skills (Bash scripting, Docker builds/push, private registries) essential for deployment scripts/CI pipelines; minor ding as role doesn't explicitly list private registries (-1). Clarity: Crystal clear with specific requirements, validation, and error handling; well-scoped for screening. Discriminative: Requires applied knowledge of Bash args/parsing, Docker CLI, validation, and error handling; distinguishes those who can write robust scripts from superficial knowledge.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-427",
        "role_title": "Backend Engineer - Feature Store",
        "question": "\"Design a feature store pipeline that ingests streaming data from Kafka, computes features using a Python function, stores them in Redis for real-time serving, and persists them to PostgreSQL for historical tracking. The pipeline should support feature versioning and handle data drift detection. Explain how you would implement feature versioning and data drift detection in this system.\"",
        "scores": {
          "relevance": 9,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core role skills (feature store, computation, versioning, real-time serving) with relevant tech; minor ding for assuming Kafka/Redis/PostgreSQL not explicitly listed (-1). Clarity: Well-scoped design question with specific components, but lacks explicit constraints like scale or time (-2). Discriminative: Excellent at testing applied ML infra knowledge, trade-offs in versioning/drift, distinguishes strong candidates.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-428",
        "role_title": "Senior Backend Engineer - Payment Orchestration",
        "question": "*Design a failover strategy for a payment orchestration system that routes transactions to multiple payment providers (e.g., Stripe, PayPal, Braintree). The system must ensure transaction success, minimize cost, and maintain low latency. Consider provider availability, transaction cost, and success rates. Provide a step-by-step plan including how you would evaluate provider health, dynamically route transactions, and handle failures. Include considerations for cost optimization and transaction latency.*",
        "scores": {
          "relevance": 9,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core role skills (failover, routing, cost optimization, success rates) with relevant providers; minor generic feel but no red flags (-0). Clarity: Well-scoped with clear requirements and step-by-step ask, but 'design' lacks explicit time/format constraints (-1, -1 red flag). Discriminative: Excellent at probing trade-offs (availability/cost/latency), dynamic routing, health checks\u2014separates deep thinkers from superficial answers.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-429",
        "role_title": "Backend Engineer - Log Aggregation",
        "question": "**\"Design a scalable log aggregation pipeline using Go and Kafka to ingest, parse, and store logs in ClickHouse. Include key considerations for performance, reliability, and observability.\"**",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly matches role's core focus on high-volume log pipelines (millions/sec), parsing/enrichment/retention; uses relevant tech (Go, Kafka, ClickHouse) with no red flags. Clarity: Well-scoped design question with specific components and considerations; minor deduction for lacking explicit time/format constraints. Discriminative: Requires deep trade-offs in scalability/performance/reliability for high-throughput systems, strongly separates strong candidates.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-430",
        "role_title": "Junior Backend Engineer - Search Indexing",
        "question": "You're building a search index for a product catalog with 100,000 products. Each product has a title, description, and price. You need to index these records into Elasticsearch using Python. How would you design the indexing pipeline to ensure efficient, reliable, and scalable performance?",
        "scores": {
          "relevance": 7,
          "clarity": 6,
          "discriminative": 8,
          "reasoning": "Relevance: Tests core indexing skills and Elasticsearch (implied by search index role), but generic/not entry-level specific and no Python details in role (-1 generic, -2). Clarity: Clear intent but lacks constraints on time, batch size, error handling scope (-2 missing constraints). Discriminative: Good for applied pipeline design with trade-offs (efficiency/reliability/scalability), separates those understanding bulk indexing basics.",
          "composite": 7.0
        }
      },
      {
        "role_id": "be-431",
        "role_title": "Staff Engineer - Real-time Collaboration",
        "question": "Design a real-time collaboration system that supports multiple users editing a shared document simultaneously. The system must handle concurrent edits, resolve conflicts when users edit overlapping regions, and ensure that all users see the same content in real time. Use CRDTs to manage conflict resolution. You must support a simple document structure (e.g., a text document with line breaks and text segments). Describe your design choices, including how you would store the document state, how you would handle concurrent edits, and how you would resolve conflicts using a CRDT.",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core focus on real-time collaboration, conflict resolution, and operational transforms via CRDTs (no red flags). Clarity: Crystal clear scope with simple document structure and specific design aspects requested; minor deduction as no explicit time/format constraints. Discriminative: Tests deep applied knowledge of CRDTs, trade-offs in concurrent editing, and system design choices, strongly separating strong architects from weaker candidates.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-432",
        "role_title": "Backend Engineer - Data Validation",
        "question": "\"Design and implement a data validation framework using Great Expectations to validate data from a PostgreSQL table (e.g., `sales` table) before loading it into S3. The validation should check for missing values, data type correctness, and statistical outliers (e.g., sales > 1000000). Include a minimal example of how this would be used in an Airflow DAG to trigger validation and fail the pipeline if any validation fails.\"",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core data validation (schema, stats, outliers) and pipeline integration with key tools (Great Expectations, PostgreSQL, S3, Airflow); minor ding as GE not explicitly listed but perfectly matches role focus (-1). Clarity: Crystal clear, well-scoped with specific table, checks, and Airflow integration; no bundling or ambiguity. Discriminative: Requires applied design/implementation knowledge, trade-offs in validation/outliers, and pipeline failure handling; strong separation of strong vs weak candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-433",
        "role_title": "Senior Backend Engineer - Platform APIs",
        "question": "\"Design a scalable, secure, and consistent service catalog API in Go using gRPC that allows developers to discover, provision, and manage platform services (e.g., databases, storage, compute). The API must support versioned service definitions, support both synchronous and asynchronous provisioning, and provide a way to list available services with filtering by category, region, and tags. Additionally, the API must support rate limiting and authentication via JWT tokens. Describe your design decisions, including how you would handle versioning, asynchronous provisioning, and security.\"",
        "scores": {
          "relevance": 9,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core role responsibilities (service catalog, provisioning APIs) with Go/gRPC matching platform API focus; minor ding for assuming gRPC specificity (-1). Clarity: Well-scoped with clear requirements, but lacks explicit time/format constraints for screening (-2). Discriminative: Excellent at probing trade-offs in versioning, async flows, security, and scalability, distinguishing strong API designers.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-434",
        "role_title": "Principal Engineer - Database Strategy",
        "question": "\"Design a database strategy for a multi-region, globally distributed application using PostgreSQL, CockroachDB, Redis, and Kafka. Your strategy must include:  \n- A clear rationale for selecting each database technology (PostgreSQL, CockroachDB, Redis, Kafka)  \n- A detailed architecture for data replication, consistency, and availability across regions  \n- A plan for handling data migration from a monolithic PostgreSQL database to this distributed setup  \n- A discussion of operational excellence and monitoring for database performance and health  \n- A brief mention of how you would mentor junior engineers in database strategy and architecture\"",
        "scores": {
          "relevance": 4,
          "clarity": 8,
          "discriminative": 3,
          "reasoning": "Relevance: Tangentially related to core role (multi-region DB strategy) but generic and bundles unrelated techs (Redis, Kafka not core DBs); forces use of unmentioned CockroachDB/Redis/Kafka (-2 each, generic -3). Clarity: Well-structured and unambiguous, but lacks time/format constraints for screening (-1, multiple bundled parts -2 mitigated by bullets). Discriminative: Too broad (design full system + migration + ops + mentoring >10min -2), tests recall over deep tradeoffs, weak separation.",
          "composite": 5.0
        }
      },
      {
        "role_id": "be-435",
        "role_title": "Junior Backend Engineer - Queue Consumers",
        "question": "\"Design and implement a Python consumer for a RabbitMQ queue that processes messages with the following requirements:  \n1. Acknowledge messages only after successful processing (with a 5-second timeout for processing).  \n2. Implement idempotency by using a PostgreSQL database to track processed message IDs.  \n3. Handle message failures gracefully by moving failed messages to a dead-letter queue (DLQ).  \n4. Ensure the consumer restarts automatically after a crash (using Docker and a health check).  \n5. Include basic logging and error handling.",
        "scores": {
          "relevance": 7,
          "clarity": 6,
          "discriminative": 8,
          "reasoning": "Relevance: Directly tests core skills (acks, DLQ, idempotency) and exact tech (Python, RabbitMQ, PostgreSQL), but Docker/health checks go beyond entry-level focus (-2 for minor tech not core). Clarity: Clear requirements but vague on implementation scope, depth, and exact Docker setup; multiple bundled aspects (-2). Discriminative: Strong on applied knowledge/trade-offs in message handling, separates good entry-level candidates well.",
          "composite": 7.0
        }
      },
      {
        "role_id": "be-436",
        "role_title": "Backend Engineer - API Documentation",
        "question": "\"Design and implement a function in TypeScript that takes an OpenAPI 3.0 schema object and generates a human-readable, markdown-formatted documentation block for a given endpoint. The function should include:  \n- Path parameters, query parameters, request body, and response schema  \n- Example values for all parameters and responses  \n- Versioning support (e.g., `/v1/users`)  \n- Proper handling of nested schemas  \n- Markdown formatting with headers and code blocks  \n- Error handling for invalid schema structures\"",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests OpenAPI schema handling, documentation generation, versioning, and nested schemas\u2014core to API docs tooling role (-1 for no explicit SDK mention). Clarity: Precise scope with exact requirements listed, unambiguous for screening. Discriminative: Requires applied TypeScript skills, design trade-offs (e.g., example gen, nesting, errors), distinguishes strong implementers from weak ones.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-437",
        "role_title": "Senior Backend Engineer - Secrets Management",
        "question": "\"Design a secure secrets management solution using HashiCorp Vault and Kubernetes, including automatic secret rotation for database credentials, encryption-at-rest for stored secrets, and audit logging. How would you configure Vault to ensure that Kubernetes pods can securely access secrets without exposing them in logs or configuration files? Explain your approach to managing access policies, rotating database credentials automatically, and handling audit logs.\"",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core focus on secrets management, Vault, rotation, encryption-at-rest, and audit logging in Kubernetes context. Clarity: Crystal clear and well-scoped for screening, minor deduction for lacking explicit time/format constraints. Discriminative: Tests deep applied knowledge, trade-offs in Vault config, policies, and integration, strongly separating strong candidates.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-438",
        "role_title": "Staff Engineer - ML Serving Infrastructure",
        "question": "\"Design a scalable and fault-tolerant ML serving solution using Triton Inference Server with Kubernetes. Your design should include model versioning, dynamic batching, and latency optimization strategies. Include a high-level architecture diagram and explain how you would implement dynamic batching and model versioning. Also, describe how you would monitor system health and detect model drift.\"",
        "scores": {
          "relevance": 10,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core needs (ML serving, batching, versioning, latency) with Triton/K8s as relevant tools. Clarity: Clear and well-scoped but bundles diagram request and multiple components (-2). Discriminative: Tests deep applied knowledge, trade-offs in design/monitoring/drift; strong separation for experts.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-439",
        "role_title": "Backend Engineer - Billing Integration",
        "question": "\"Design and implement a Python function that handles the proration of a subscription price change in a billing system using Stripe. The function should:\n- Accept the old and new subscription prices, the subscription start date, and the current date.\n- Calculate the prorated amount for the current period based on the time elapsed between the start date and the current date.\n- Account for the fact that proration only applies to the current billing period, not future periods.\n- Return the prorated amount to be applied to the invoice.\"",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core proration skill for billing/subscription role using Stripe (mentioned), minor ding for no full role desc tech stack but spot-on focus (-1). Clarity: Crystal clear inputs/outputs, well-scoped function, no ambiguity. Discriminative: Requires applied math/logic for proration (time ratios, period boundaries), tests deep billing understanding and edge cases like partial periods, strong separation.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-440",
        "role_title": "Junior Backend Engineer - Metrics Collection",
        "question": "You're adding metrics to a Go service using Prometheus. The service has a health check endpoint that returns a 200 status code when healthy and a 500 status code when unhealthy. You want to expose the health status as a metric, so you decide to use a Gauge. You set up the metric with a constant value of 1 for healthy and 0 for unhealthy. However, you notice that the metric doesn't update when the service becomes unhealthy \u2014 it stays at 1 even when the service fails.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 8,
          "reasoning": "Relevance: Directly tests core metrics skills (Gauge pattern) and Go/Prometheus from role, entry-level appropriate (-0 red flags). Clarity: Crystal clear scenario with specific issue, well-scoped. Discriminative: Tests applied Prometheus knowledge (likely Set vs. Inc), reveals understanding of metric mutation; good separation but not extreme trade-offs.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-441",
        "role_title": "Senior Backend Engineer - Autocomplete Systems",
        "question": "**\"Design a distributed autocomplete system that supports real-time personalization and achieves sub-50ms latency at 100K QPS. How would you structure the data pipeline from ingestion to user query processing, and what specific optimizations would you apply to ensure low latency and high throughput? Please describe how you'd handle personalization (e.g., user-specific recommendations) and how you\u2019d balance consistency and performance in a distributed environment.\"**",
        "scores": {
          "relevance": 10,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core requirements (autocomplete, personalization, exact latency/QPS targets). Clarity: Clear intent and well-scoped, but lacks explicit time/format constraints for screening (-1) and slightly bundles pipeline + optimizations + personalization (-1). Discriminative: Excellent at testing trade-offs, architecture, and deep optimizations; distinguishes strong systems thinkers from weak ones.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-442",
        "role_title": "Backend Engineer - Event Streaming",
        "question": "Design a system to fan out real-time updates from a PostgreSQL database to multiple downstream services using NATS and Redis. The system must support filtering by event type and allow replay of events from a specific point in time. Describe how you would structure the event ingestion pipeline, handle filtering, and ensure durability and replayability.",
        "scores": {
          "relevance": 9,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (fan-out, filtering, replay) with PostgreSQL (likely DB in role context); minor ding for specifying NATS/Redis not in role desc (-2 red flag adjusted). Clarity: Well-scoped design question with specific requirements, but lacks explicit constraints like scale/throughput (-1 from red flag). Discriminative: Requires applied knowledge of pipelines, trade-offs in durability/filtering/replay with specific tech, strongly separates candidates.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-443",
        "role_title": "Senior Backend Engineer - Data Lake Architecture",
        "question": "\"Design a data pipeline for ingesting and processing daily sales data into a data lake using Apache Iceberg. The data comes from a variety of sources (e.g., point-of-sale systems, web analytics) and is stored in S3. The pipeline must support schema evolution, allow efficient partition pruning for performance, and enable time travel queries for auditability. Describe how you would structure the pipeline, including the data flow, table format configuration, partitioning strategy, and how you would handle schema evolution and time travel.\"",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core needs (data lake, Iceberg table formats, schema evolution, partition pruning, time travel). No red flags. Clarity: Crystal clear, well-scoped design question with explicit requirements; minor deduction for no strict time/format constraints. Discriminative: Tests applied architecture knowledge, trade-offs in partitioning/schema/time travel, strongly separates experienced candidates.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-444",
        "role_title": "Backend Engineer - Service Templates",
        "question": "\"Design a service template using Cookiecutter that generates a Go-based microservice with best practices, including proper directory structure, configuration management, logging, and CI/CD setup via GitHub Actions. The template should also include a minimal observability layer (e.g., logging with structured format and metrics via Prometheus) and enforce security best practices (e.g., environment-specific configuration, secure secrets handling, and use of default TLS settings). Describe how you would structure the template, what files would be included, and how GitHub Actions would validate and deploy the service to Kubernetes with ArgoCD.\"",
        "scores": {
          "relevance": 9,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (service scaffolding with Cookiecutter, CI/CD, observability, security) and exact tech (Go, GitHub Actions, Kubernetes/ArgoCD); minor ding for not explicitly mentioning 'templates with best practices' but covers it (-1). Clarity: Well-formed and specific, but 'design a service template' lacks minor constraints like expected detail level or time (-2). Discriminative: Excellent at testing applied knowledge, trade-offs in structure/secrets/observability/CI/CD, distinguishes deep expertise from superficial knowledge.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-445",
        "role_title": "Junior Backend Engineer - Database Migrations",
        "question": "You're working on a Python application using SQLAlchemy and Alembic for database migrations. You need to create a migration that adds a new column `email` to the `users` table, but you want to ensure that the migration handles the case where some users may already have an email in the database.",
        "scores": {
          "relevance": 9,
          "clarity": 6,
          "discriminative": 7,
          "reasoning": "Relevance: Directly tests Alembic/SQLAlchemy migrations and handling existing data, core to role's database migration focus (-1 for generic 'handles the case' without zero-downtime/rollback specifics). Clarity: Clear scenario but vague on 'handles the case' (backfill? nullability? constraints?), missing explicit constraints (-2 for vagueness, -1 for ambiguous jargon). Discriminative: Good test of applied migration knowledge like upgrade/downgrade with defaults/backfills, separates those understanding real-world pitfalls from basics.",
          "composite": 7.333333333333333
        }
      },
      {
        "role_id": "be-446",
        "role_title": "Staff Engineer - Distributed Tracing",
        "question": "\"Design a sampling strategy for a high-throughput, distributed system using OpenTelemetry and Jaeger, where traces are generated at 10,000 transactions per second (TPS) and must be stored in ClickHouse. Consider the trade-offs between different sampling strategies (e.g., rate-based, probabilistic, deterministic) and the impact on observability, performance, and cost. Propose a hybrid sampling strategy that balances trace visibility, performance, and cost, and explain how you would implement and validate this strategy in production.\"",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core focus on distributed tracing, sampling strategies at scale, with exact tech stack (OpenTelemetry, Jaeger, ClickHouse). No red flags. Clarity: Crystal clear, well-scoped with specific TPS and trade-offs; minor deduction for lacking explicit time constraint. Discriminative: Tests deep trade-offs, design, implementation, and validation\u2014distinguishes architects from juniors effectively.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-447",
        "role_title": "Backend Engineer - Load Shedding",
        "question": "*Design a load shedding mechanism for a high-traffic API endpoint that uses Envoy as a proxy and Redis for rate limiting. The endpoint experiences random spikes in traffic, causing service degradation. When the system detects a spike, it should automatically trigger load shedding to prioritize critical requests (e.g., user authentication) over non-critical ones (e.g., analytics reporting).*",
        "scores": {
          "relevance": 9,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core load shedding, prioritization, and adaptive handling from role; minor ding for specifying Envoy/Redis not in role desc (-1). Clarity: Clear intent and scenario, but lacks explicit constraints like time or output format (-1). Discriminative: Excellent at probing trade-offs in detection, prioritization, and integration, separating strong systems thinkers.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-448",
        "role_title": "Senior Backend Engineer - Webhook Infrastructure",
        "question": "\"Design a resilient webhook delivery system in Go using Kafka and PostgreSQL/Redis for delivery guarantees, retry logic, and analytics. Specifically, how would you ensure message delivery is guaranteed, handle failures, and track delivery status with analytics?\"",
        "scores": {
          "relevance": 10,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core role requirements (guaranteed delivery, retry logic, analytics) with relevant tech (Go, Kafka, PostgreSQL/Redis). Clarity: Clear intent and specific, but lacks scoping constraints like time or detail level (-2 red flag). Discriminative: Tests applied system design, trade-offs in resilience/failures/analytics, strong separation of strong vs weak candidates (not pure recall).",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-449",
        "role_title": "Junior Backend Engineer - Auth Integration",
        "question": "*Design a secure OAuth2 flow for a social login (e.g., Google) integrated into a Node.js + TypeScript application using PostgreSQL. Include how you would handle token storage, refresh, and prevent replay attacks.*",
        "scores": {
          "relevance": 9,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core OAuth2 flows, token handling, refresh, and social login from role; matches Node.js/TS stack (assuming common for entry-level), PostgreSQL storage relevant (-1 minor for no explicit Node/TS mention in role). Clarity: Clear and well-scoped for screening, but lacks explicit constraints like time or response format (-2). Discriminative: Excellent at probing security trade-offs, applied knowledge of refresh/prevention, distinguishes deep vs superficial understanding.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-450",
        "role_title": "Principal Engineer - ML Platform Strategy",
        "question": "\"Design a scalable and secure ML platform strategy for a large enterprise with distributed teams across multiple geographies, including considerations for data governance, model lifecycle management, and team enablement. Your design must address how to ensure consistent model quality, compliance with regulatory requirements (e.g., GDPR, HIPAA), and integration with existing CI/CD pipelines. Please describe your approach to tooling selection, governance models, and how you would onboard new ML engineers to the platform.\"",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core responsibilities (strategy, tooling, governance, enablement) with no red flags. Clarity: Crystal clear and well-scoped for screening, minor deduction for lacking explicit time/format constraints. Discriminative: Tests deep applied knowledge, trade-offs (scalability/security/governance), and strategic thinking to separate strong strategists from juniors.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-451",
        "role_title": "Backend Engineer - Subscription Billing",
        "question": "\"Design a system to handle proration for a subscription billing model where users can change plans mid-month. Your design should include how usage metering is integrated, how proration is calculated, and how it's stored and updated in the database. Use the following constraints:  \n- All calculations must be precise to the second (e.g., 10.5 seconds of usage = 10.5).  \n- The system must support plan changes at any time, not just at the start of a billing period.  \n- You must use the provided tech stack: Java, PostgreSQL, Kafka, Spring Boot, Redis.  \n- Consider edge cases like partial billing periods and overlapping changes.  \n- Avoid using external libraries for financial calculations.\"",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (proration, usage metering, plan changes) with exact tech stack; minor ding for over-specifying stack (-1). Clarity: Well-scoped with precise constraints, unambiguous despite system design scope. Discriminative: Probes deep applied knowledge, trade-offs, edge cases, and tech integration for strong separation.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-452",
        "role_title": "Senior Backend Engineer - Faceted Search",
        "question": "\"Design and implement a faceted search system that supports dynamic filtering and accurate facet counts at scale. Consider performance, correctness, and scalability when designing your solution. Focus on the challenges of maintaining facet count accuracy when dealing with large datasets and high query loads. How would you ensure accurate facet counts without sacrificing performance?\"",
        "scores": {
          "relevance": 10,
          "clarity": 6,
          "discriminative": 8,
          "reasoning": "Relevance: Perfectly matches role's core focus on faceted search, dynamic filtering, and facet accuracy at scale (no red flags). Clarity: Clear intent but 'design and implement' is overly broad for screening without constraints like time, tech stack, or scope (-2 for lacking constraints). Discriminative: Strong on trade-offs (accuracy vs performance at scale), separates candidates with deep understanding from superficial ones.",
          "composite": 8.0
        }
      },
      {
        "role_id": "be-453",
        "role_title": "Backend Engineer - Notification Delivery",
        "question": "\"Design a system to deliver push notifications in real-time to mobile devices using Go, Redis, and FCM/APNS. Your solution should handle device registration, batching, and delivery tracking. Explain how you\u2019d ensure reliability, scalability, and performance, and how you\u2019d handle failure scenarios.\"",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core requirements (device registration, batching, delivery tracking) and specifies Go/Redis/FCM/APNS. Clarity: Well-scoped system design with specific components; minor deduction for lacking explicit time/format constraints. Discriminative: Tests deep applied knowledge of trade-offs, reliability, scalability, and failures, strongly separating strong candidates.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-454",
        "role_title": "Junior Backend Engineer - ETL Scripts",
        "question": "Write a Python script using pandas and PostgreSQL that reads a CSV file, performs a basic data transformation (e.g., filtering, aggregating), and writes the result back to a PostgreSQL table. The script should include error handling for common issues like missing files, invalid data types, and database connection failures. Use Airflow to schedule the script to run daily at 2 AM.",
        "scores": {
          "relevance": 3,
          "clarity": 4,
          "discriminative": 5,
          "reasoning": "Relevance: Directly mentions pandas/PostgreSQL/ETL (core) but Airflow scheduling is advanced for entry-level role focused on learning basics (-3 generic ETL, -2 Airflow not entry-level core). Clarity: Bundled multi-part (script + Airflow) without constraints (-2), vague on transformation specifics. Discriminative: Tests ETL basics but Airflow adds unnecessary barrier for screening; weak separation at entry-level.",
          "composite": 4.0
        }
      },
      {
        "role_id": "be-455",
        "role_title": "Staff Engineer - Developer Portal",
        "question": "*Design a scalable plugin architecture for a developer portal built with Backstage using React and Node.js. Your design should support dynamic service discovery and documentation generation, and enable plug-in teams to onboard new services with minimal friction. Include considerations for plugin lifecycle, configuration, and extensibility. Describe how you would structure the core plugin system to support modular development and ensure maintainability across a growing team.*",
        "scores": {
          "relevance": 9,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core role responsibilities (plugin architecture, service discovery, documentation, onboarding) and exact tech stack (Backstage, React, Node.js); minor generic feel but no red flags. Clarity: Well-formed and unambiguous, but lacks explicit screening constraints (e.g., time/depth/format). Discriminative: Excellent at testing trade-offs, architectural thinking, and deep Backstage knowledge to separate strong architects from average devs.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-456",
        "role_title": "Backend Engineer - Query Optimization",
        "question": "\"Explain how you would instrument and analyze a PostgreSQL database to detect and diagnose slow queries using pganalyze, and walk through a real-world scenario where you used this approach to improve performance. Include specific SQL queries, indexes, and plan analysis steps.\"",
        "scores": {
          "relevance": 7,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Tests core slow query detection and plan analysis but assumes pganalyze (not mentioned in role) and demands personal real-world experience (-3 generic red flag partially, -2 unlisted tech). Clarity: Well-formed and scoped but bundles explanation, scenario, and specifics without time/format constraints (-2). Discriminative: Excellent at separating via applied trade-offs, tools, and real experience; not trivial recall.",
          "composite": 8.0
        }
      },
      {
        "role_id": "be-457",
        "role_title": "Senior Backend Engineer - SLO Automation",
        "question": "\"Design a system that automatically detects and alerts on SLO burn rate violations in a Kubernetes environment using Go, Prometheus, and PostgreSQL. Include how you would track error budgets, handle alerting, and ensure reliability in production. Be specific about how you'd use Sloth for SLO definitions and how you'd integrate with Kubernetes monitoring (e.g., via Prometheus).\"",
        "scores": {
          "relevance": 7,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core SLO/burn rate/error budget skills and key tech (Kubernetes, Prometheus, Go, PostgreSQL), but deduct -2 for Sloth (unmentioned in role) and -1 for generic design scope. Clarity: Well-formed and specific, minor deduction for lacking time/format constraints. Discriminative: Excellent at testing applied system design, integrations, trade-offs, and production reliability, strongly separating candidates.",
          "composite": 8.0
        }
      },
      {
        "role_id": "be-458",
        "role_title": "Backend Engineer - API Gateway Plugins",
        "question": "What are the key differences between Kong's `on_pre_balance` and `on_post_balance` lifecycle events in a plugin, and how would you use them to implement a custom authentication flow that validates user session state and enriches the response with user metadata?",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests Kong plugin development (core to API gateway plugins for auth/transformation), no red flags. Clarity: Precise and well-scoped, minor deduction only if constraints like code format unspecified. Discriminative: Requires deep understanding of Kong lifecycle, trade-offs in auth flow placement, and applied implementation for strong candidate separation.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-459",
        "role_title": "Junior Backend Engineer - Container Security",
        "question": "\"Walk me through how you would set up a CI/CD pipeline to scan container images for vulnerabilities using Trivy, and explain how you would ensure these scans are both effective and non-disruptive to the development workflow.\"",
        "scores": {
          "relevance": 7,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Tests core container scanning skills for the role but specifies Trivy (unmentioned in role desc, -2) and CI/CD (relevant but entry-level focus is implementation/learning vuln detection). Clarity: Well-formed with clear intent, though lacks minor constraints like tools/time. Discriminative: Excellent at probing applied knowledge, trade-offs (effective vs non-disruptive), and practical workflow integration, strongly separating candidates.",
          "composite": 8.0
        }
      },
      {
        "role_id": "be-460",
        "role_title": "Backend Engineer - Experiment Tracking",
        "question": "\"Using MLflow, design and implement a Python script that tracks a simple machine learning experiment. The script should:  \n1. Initialize an MLflow experiment with a unique name (e.g., 'experiment_123').  \n2. Log hyperparameters (e.g., learning rate, batch size) and metrics (e.g., accuracy, loss).  \n3. Save a model artifact (e.g., a trained scikit-learn model) to MLflow's artifact store.  \n4. Ensure the experiment is properly tracked and closed.  \n5. Use Docker to containerize the script so it can be run in a production-like environment.  \n6. Provide instructions on how to run the script locally and in Docker.\"",
        "scores": {
          "relevance": 5,
          "clarity": 6,
          "discriminative": 5,
          "reasoning": "Relevance: Directly tests MLflow (relevant for experiment tracking) but adds Docker (unmentioned in role) and goes beyond core metric logging/model artifacts (-2); generic scripting could apply broadly (-3, net 10-5=5). Clarity: Clear steps but bundles multiple tasks (script + Docker + instructions) without time/format constraints (-2); vague on 'simple ML' details (7-2=5, adjusted to 6). Discriminative: Tests applied MLflow knowledge (good) but Docker/containerization dilutes ML infra focus and risks over-broad scope (>10min, -2); basic implementation over trade-offs (7-2=5).",
          "composite": 5.333333333333333
        }
      },
      {
        "role_id": "be-461",
        "role_title": "Senior Backend Engineer - Fraud Detection",
        "question": "\"Design a real-time fraud scoring pipeline for a payment system that must handle 10K transactions per second. The pipeline should integrate a rule engine with a machine learning model and support manual review queues. Describe how you would structure the data flow, key components, and how you'd balance real-time performance with model accuracy and reliability.\"",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core skills (fraud pipelines, rule engines, ML integration, manual queues) with exact tech focus. Clarity: Crystal clear and well-scoped with 10K TPS constraint, though minor deduction for unspecified output format/depth. Discriminative: Excellent at testing architecture trade-offs (performance vs accuracy/reliability), deep understanding, and applied knowledge for strong candidate separation; appropriately scoped for screening (<10min high-level).",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-462",
        "role_title": "Backend Engineer - Alert Routing",
        "question": "\"Design a system to route alerts from a distributed system to the appropriate on-call team using Alertmanager, with support for dynamic escalation policies, silence management, and deduplication. Explain how you would structure the routing logic, manage silences, and ensure alerts are not duplicated during high-volume events.\"",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Directly tests core role skills (routing, escalation, silence, deduplication) using specified Alertmanager; crystal clear with focused scope on key components; probes applied knowledge, trade-offs in high-volume scenarios for strong differentiation. Minor ding for assuming Alertmanager familiarity without role confirmation, but aligns closely.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-463",
        "role_title": "Junior Backend Engineer - Full-text Search",
        "question": "You are tasked with implementing a simple full-text search feature in a Python application using PostgreSQL. The application has a table `products` with columns `id`, `name`, and `description`. Users can search for products by entering a query string. When a query is submitted, return a list of products matching the search term, ranked by relevance.",
        "scores": {
          "relevance": 9,
          "clarity": 8,
          "discriminative": 8,
          "reasoning": "Relevance: Directly tests PostgreSQL full-text search core to role (tsvector/ranking implied), minor ding for not explicitly naming tsvector (-1). Clarity: Clear task and goal, but lacks constraints on output format, query handling, or depth (-2). Discriminative: Requires applied knowledge of FTS implementation and ranking, separates those who know SQL/Python integration from theorists; not trivia but needs trade-offs for 10.",
          "composite": 8.333333333333334
        }
      },
      {
        "role_id": "be-464",
        "role_title": "Staff Engineer - Presence Systems",
        "question": "Design a presence system that tracks real-time user presence and typing indicators using Go, Redis, and WebSockets. The system must support millions of concurrent users. Describe your architecture, data model, and how you would handle scale and reliability.",
        "scores": {
          "relevance": 9,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (presence, typing indicators, scale) and specifies Go/Redis/WebSockets (assuming role match); minor deduction for not explicitly mentioning heartbeats/state sync. Clarity: Well-scoped for screening but lacks precise constraints (e.g., exact scale metrics, latency targets). Discriminative: Excellent at probing architecture, trade-offs, scaling/reliability decisions for strong separation.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-465",
        "role_title": "Backend Engineer - CDC Pipelines",
        "question": "*Design a CDC pipeline using Debezium and Kafka to replicate data from a PostgreSQL database to Snowflake. Include the configuration of Debezium's connector, handling of schema changes, and an approach to ensure exactly-once delivery. Explain how you would handle initial data synchronization (snapshot) and ensure data consistency during schema changes.*",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Directly tests core CDC pipeline skills (schema changes, snapshots, exactly-once) with exact tech stack (Debezium, Kafka, PostgreSQL, Snowflake) from role. Crystal clear and well-scoped for screening; minor deduction for no explicit time/format constraints but highly specific. Strong trade-offs in exactly-once and consistency distinguish experts.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-466",
        "role_title": "Senior Backend Engineer - GitOps Tooling",
        "question": "\"Design a drift detection mechanism in a GitOps pipeline using ArgoCD and Helm. Your solution should detect when a deployed Kubernetes manifest diverges from its Git source, and automatically trigger a rollback to the last known good state if drift exceeds a configurable threshold. Explain your approach, including how you would implement reconciliation, detect drift, and handle rollback. Consider edge cases such as partial drift, temporary mismatches, and network issues.\"",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core GitOps skills (drift detection, reconciliation, rollback) with exact tech stack (ArgoCD, Helm, K8s manifests) from role. No red flags. Clarity: Crystal clear, well-scoped with specific requirements and edge cases; minor deduction for no explicit time/format constraint. Discriminative: Tests deep applied knowledge, trade-offs, and edge cases, strongly separating strong candidates who can reason through GitOps complexities from weak ones.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-467",
        "role_title": "Backend Engineer - Connection Pooling",
        "question": "\"Design and implement a Go-based connection pool manager that uses PgBouncer as a proxy to manage PostgreSQL connections. The manager should support dynamic pool sizing based on load, automatic failover to a backup PostgreSQL instance when primary fails, and monitor connection health using a health check endpoint. How would you structure the configuration, handle failover, and ensure connection health is maintained in a Kubernetes environment with horizontal pod autoscaling (HPA)?\"",
        "scores": {
          "relevance": 4,
          "clarity": 5,
          "discriminative": 7,
          "reasoning": "Relevance: Superficially mentions Go and PostgreSQL but heavily focuses on PgBouncer/Kubernetes/HPA not in role (red flag -2 tech not listed, generic infra design -3). Clarity: Bundled multi-part design question without scope/time constraints in screening context (red flag -2 multiple questions, -2 design w/o constraints). Discriminative: Tests applied trade-offs in pooling/failover but too broad for screening (>10min, advanced infra knowledge).",
          "composite": 5.333333333333333
        }
      },
      {
        "role_id": "be-468",
        "role_title": "Junior Backend Engineer - CI Pipelines",
        "question": "\"Design a GitHub Actions workflow that builds and tests a Python project using Docker. The workflow should include caching for Python dependencies, parallelize tests across multiple CPU cores, and run tests in a containerized environment. Provide a clear explanation of your choices for each step, including caching strategy and parallelization method.\"",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core CI pipeline skills (workflow syntax, caching, test parallelization) for GitHub Actions in Python/Docker context (-1 for minor Docker addition not explicit). Clarity: Well-scoped with specific requirements and explanation request, unambiguous for screening. Discriminative: Requires applied knowledge of caching strategies, parallelization trade-offs, and design choices, strongly separates candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-469",
        "role_title": "Senior Backend Engineer - Chaos Engineering",
        "question": "*Design a failure injection experiment using Chaos Mesh to simulate a node failure in a Kubernetes cluster, and implement a recovery validation mechanism that ensures the application remains available and consistent after the failure. Include details on how you would configure the experiment, monitor the impact, and validate recovery using metrics and logs.*",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets Chaos Mesh, Kubernetes node failure, blast radius control, and recovery validation\u2014core to chaos engineering role (no red flags). Clarity: Crystal clear, well-scoped for screening with specific components, minor deduction for lacking explicit time/format constraints. Discriminative: Tests applied design, configuration, monitoring, trade-offs in Chaos Mesh/K8s, and validation logic\u2014strong separation of strong vs weak candidates.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-470",
        "role_title": "Backend Engineer - GraphQL Federation",
        "question": "*Design a schema stitching strategy for a federated GraphQL gateway that connects a user subgraph and a product subgraph. The user subgraph defines a `User` type with fields `id`, `name`, and `email`, while the product subgraph defines a `Product` type with fields `id`, `name`, and `field`. The user subgraph also contains a field `products` of type `[Product!]!` which references the product subgraph. Implement a solution that correctly resolves the `products` field on a `User` object when queried via the gateway. Provide a clear explanation of how the query planning and entity resolution work in this scenario, including how the gateway handles the cross-subgraph resolution.*",
        "scores": {
          "relevance": 9,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core federated GraphQL skills (schema stitching, query planning, entity resolution) with exact role tech stack; minor deduction for not mentioning supergraph explicitly (-1). Clarity: Well-scoped with specific schemas and clear intent, but lacks explicit constraints on format/response depth (-1) and bundles design/explanation (-1). Discriminative: Excellent at probing trade-offs in stitching, planning, and resolution, distinguishing deep Apollo Federation knowledge from superficial understanding.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-471",
        "role_title": "Staff Engineer - Zero Trust Architecture",
        "question": "*Design a solution using SPIFFE and mTLS to secure inter-service communication in a Kubernetes cluster where services need to be authenticated and authorized using identity-based policies. Include a flow from service startup to actual communication. Explain how SPIFFE ensures zero trust, and describe how you would integrate with HashiCorp Vault for identity management and policy enforcement.*",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core role skills (zero trust, mTLS, policy enforcement) with exact tech (SPIFFE, Kubernetes) from description; no red flags. Clarity: Well-scoped with specific flow and integration; minor deduction as no explicit time/format constraint. Discriminative: Tests deep applied knowledge, trade-offs, and architecture; strong separation between superficial and expert responses; not trivial recall.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-472",
        "role_title": "Backend Engineer - Vector Search",
        "question": "Design a system to handle semantic search with hybrid retrieval using Pinecone and PostgreSQL. The system should support embedding generation, index management, and retrieval with a fast, scalable, and reliable pipeline.",
        "scores": {
          "relevance": 8,
          "clarity": 5,
          "discriminative": 7,
          "reasoning": "Relevance: Directly tests core skills (embeddings, index mgmt, hybrid search) and matches role, but assumes Pinecone/pgSQL not explicitly listed (-2 for unlisted techs). Clarity: Clear intent but 'design a system' lacks scope/constraints like scale/size (-2 red flag, vague depth). Discriminative: Probes applied knowledge/tradeoffs in pipeline design, separates good from weak, but broad scope risks >10min answers (-2).",
          "composite": 6.666666666666667
        }
      },
      {
        "role_id": "be-473",
        "role_title": "Senior Backend Engineer - Refund Processing",
        "question": "Design a system to process partial refunds for a payment platform, ensuring ledger reconciliation and audit trails. The system must handle asynchronous provider callbacks and support rollback on failure. How would you structure the refund processing pipeline in a distributed, fault-tolerant environment using Java, Spring Boot, Kafka, and PostgreSQL? Include considerations for state management, error handling, and audit logging.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core role skills (partial refunds, ledger reconciliation, audit trails, provider callbacks) with exact tech stack (Java, Spring Boot, Kafka, PostgreSQL); minor -1 for not explicitly mentioning rollback but it's implied. Clarity: Well-scoped design question with clear constraints, tech specified, no bundling or vagueness. Discriminative: Probes deep applied knowledge of distributed systems, state management, fault-tolerance, and trade-offs in pipeline design using specified stack, strongly separates strong candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-474",
        "role_title": "Backend Engineer - Dashboards API",
        "question": "Write a Go function to render a Grafana dashboard JSON template with dynamic variables. The function should:\n- Accept a dashboard JSON as input\n- Accept a map of variable values (e.g., `map[string]string{\"instance\": \"db1\", \"region\": \"us-east\"}`)\n- Replace all variable references in the dashboard JSON (e.g., `{{instance}}` or `{{region}}`) with their values\n- Output the rendered JSON as a string\n- Handle edge cases like missing variables, invalid JSON, and malformed variable references",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests Go (core for APIs), JSON handling, template rendering, and variable substitution\u2014exact match for dashboard management role (-0 red flags). Clarity: Precise inputs/outputs, explicit edge cases, well-scoped for screening. Discriminative: Requires applied Go/JSON skills, error handling, edge cases with trade-offs (e.g., strict vs. graceful failure), distinguishes strong implementers.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-475",
        "role_title": "Junior Backend Engineer - Search Analytics",
        "question": "\"Imagine you're building a query logging system for a search engine. Users submit search queries, and we want to record the query, the user's session ID, and the timestamp. After processing, we want to store these logs in Elasticsearch. However, we also need to ensure that the logs are not duplicated due to network issues or retries.",
        "scores": {
          "relevance": 8,
          "clarity": 8,
          "discriminative": 8,
          "reasoning": "Relevance: Directly tests core query logging skill for search analytics role using Elasticsearch (inferred relevance); minor deduction for not mentioning click tracking/relevance metrics (-2 per red flag). Clarity: Well-scoped scenario with clear requirements, but lacks explicit constraints on solution format/depth (-2 per red flag). Discriminative: Good test of applied knowledge on deduplication (idempotency, unique keys), distinguishing candidates who understand trade-offs vs basic logging.",
          "composite": 8.0
        }
      },
      {
        "role_id": "be-476",
        "role_title": "Backend Engineer - Live Updates",
        "question": "Design a scalable real-time dashboard update system using Socket.io and Redis. The system must support room-based selective broadcasting, automatic reconnection, and handle concurrent connections efficiently. Provide a high-level architecture and explain how you would implement the following:",
        "scores": {
          "relevance": 9,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core skills (live updates, room management, selective broadcasting, reconnection) and matches tech (Socket.io implied, Redis for scaling); minor ding as Redis not explicitly in role but fits perfectly. Clarity: Clear and well-scoped for high-level design, but lacks explicit constraints like scale metrics (-1 red flag avoided). Discriminative: Excellent at probing architecture, trade-offs, and implementation depth for real-time systems, separating strong candidates.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-477",
        "role_title": "Senior Backend Engineer - Data Catalog",
        "question": "\"Design a system to automatically extract metadata from a data pipeline using Apache Atlas, and ensure lineage is properly tracked through the pipeline. Describe how you would integrate Kafka for real-time event streaming, Airflow for orchestration, and PostgreSQL for storing metadata. Include specific examples of how you would define entities, relationships, and attributes in Apache Atlas, and how you would trigger metadata updates when data is transformed or moved through the pipeline.\"",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (metadata extraction, lineage tracking) with relevant tools (Atlas for catalog/lineage, Kafka/Airflow/PostgreSQL fit data pipelines/governance); minor ding as role doesn't explicitly list these integrations (-1). Clarity: Well-scoped design question with specific components, examples requested, and clear triggers; no bundling or vagueness. Discriminative: Requires applied knowledge of integrations, Atlas entity modeling, and real-time triggers, testing trade-offs and depth to separate strong candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-478",
        "role_title": "Backend Engineer - Cost Attribution",
        "question": "Explain how you would design a system to track and attribute cloud costs to individual teams or projects, using resource tagging and a database. Consider the following constraints:\n- Each cloud resource (e.g., EC2 instance, S3 bucket) has a set of tags (like `team`, `project`, `environment`)\n- Costs are billed at the monthly level, and you need to generate monthly chargeback reports\n- The system must support real-time cost tracking and monthly reporting\n- You must store cost data efficiently and avoid overwriting or losing data\n- You must handle edge cases like missing tags or inconsistent tagging\n- You should use Go, PostgreSQL, and Prometheus for monitoring",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core role skills (cost attribution, tagging, chargeback reports) with exact tech stack (Go, PostgreSQL, Prometheus); minor ding for assuming Prometheus is listed. Clarity: Well-scoped with specific constraints, unambiguous intent for screening. Discriminative: Probes deep design trade-offs (real-time vs monthly, edge cases, efficiency), separates strong architects from basic coders.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-479",
        "role_title": "Principal Engineer - Data Architecture",
        "question": "> \"You're designing a data architecture for a company that processes 10M+ daily transactions and runs complex analytical queries on aggregates. The transactional system is currently a PostgreSQL database with high write throughput. You need to build a scalable, cost-efficient data warehouse for analytical queries.  \n>  \n> Describe how you would design the architecture, including choices for data ingestion, storage, and query processing. Justify each decision with clear trade-offs. Consider performance, cost, scalability, and data consistency.\"",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core data architecture strategy, tech selection (builds on PostgreSQL), and analytics/transactional workloads from role; no red flags. Clarity: Crystal clear scenario with specifics (10M+ txns, PostgreSQL), well-scoped; minor deduction for no explicit time limit. Discriminative: Requires applied knowledge of trade-offs across ingestion/storage/query (e.g., CDC vs batch, Snowflake vs BigQuery, columnar stores), strongly separates experts from juniors; not too broad for screening.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-480",
        "role_title": "Junior Backend Engineer - Deployment Automation",
        "question": "\"Write a simple Ansible playbook that deploys a Python application using Docker to AWS EC2. The playbook should:\n- Launch an EC2 instance (using a specified AMI and instance type)\n- Install Docker on the instance\n- Build and run a Docker container from a local Dockerfile\n- Ensure the container starts automatically on boot\"",
        "scores": {
          "relevance": 4,
          "clarity": 8,
          "discriminative": 7,
          "reasoning": "Relevance: Low - Ansible is unmentioned in role (focus: deployment scripts, config mgmt, rollbacks, env setup); tests advanced Docker/AWS/EC2 not core to entry-level; generic devops (-3), misses role focus (-2), minor tech only (-2). Clarity: Clear intent and well-scoped, but lacks minor constraints like AMI ID (-1 red flag implicit, still high). Discriminative: Good applied knowledge of playbook writing with multi-step logic, separates those who can vs can't; not trivial recall.",
          "composite": 6.333333333333333
        }
      },
      {
        "role_id": "be-481",
        "role_title": "Backend Engineer - Canary Deployments",
        "question": "Design a canary deployment pipeline using Istio and ArgoCD that automatically rolls back a service if the canary traffic metrics exceed a predefined threshold (e.g., error rate > 1% or latency > 500ms) over a 5-minute window. Include details on how traffic shifting, metric monitoring, and rollback automation are implemented.",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core focus on canary deployments, traffic shifting, metrics, and rollback; directly tests Istio/ArgoCD implied by infrastructure needs. Clarity: Crystal clear with specific thresholds, window, and implementation details requested; minor deduction for no explicit time/format constraints. Discriminative: Tests deep applied knowledge of integration, trade-offs in monitoring/automation, distinguishes experts from juniors.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-482",
        "role_title": "Senior Backend Engineer - API Monetization",
        "question": "\"Design a system to track API usage and enforce quotas for different pricing tiers. Describe how you would structure the data model in PostgreSQL, implement quota enforcement in Go, and integrate with Stripe for billing. Include how you would handle edge cases like quota overages and billing disputes.\"",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Directly tests core role skills (API usage tracking, quotas, pricing tiers, billing) with exact tech stack (PostgreSQL, Go, Stripe). Crystal clear and well-scoped for screening, though minor deduction for lacking time/format constraints. Excellent trade-off probing via edge cases distinguishes strong candidates.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-483",
        "role_title": "Backend Engineer - WAF Integration",
        "question": "\"Design a Go service that integrates with ModSecurity to detect and log suspicious HTTP requests, including rate limiting and bot detection. The service should use Redis for rate limiting, PostgreSQL for persistent logging, and Nginx as the proxy. Explain how you would structure the service, handle the ModSecurity events, and ensure accurate bot detection using attack pattern analysis.\"",
        "scores": {
          "relevance": 8,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core WAF integration, rate limiting, bot detection, and attack patterns with Go/Redis/PostgreSQL; minor ding for ModSecurity/Nginx not in role desc (-2). Clarity: Well-scoped design question with specific tech, but lacks time/format constraints (-1, not fully 9-10). Discriminative: Strong on trade-offs in service structure, event handling, and bot analysis; distinguishes deep applied knowledge.",
          "composite": 8.333333333333334
        }
      },
      {
        "role_id": "be-484",
        "role_title": "Staff Engineer - Model Registry",
        "question": "\"Design a model registry system using MLflow and S3 that supports model versioning, automated deployment to Kubernetes, and governance through access control. Walk through your design decisions, including how you would structure the data model, how you'd manage model versions, how you'd automate deployment, and how you'd implement access control. Be specific about how these components integrate with MLflow, SML, and Kubernetes.\"",
        "scores": {
          "relevance": 8,
          "clarity": 7,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (model registry, versioning, deployment, governance) and key tech (MLflow, S3, Kubernetes), but penalize -2 for undefined 'SML' not in role desc. Clarity: Clear intent and specific components, but vague on scope/depth and 'SML' ambiguity (-1 jargon). Discriminative: Excellent for probing trade-offs, integration, and deep design thinking to separate strong architects.",
          "composite": 8.0
        }
      },
      {
        "role_id": "be-485",
        "role_title": "Junior Backend Engineer - Payment Webhooks",
        "question": "*You're processing a payment webhook from Stripe. The event contains the `id` and `data.object.payment_intent.id`. You need to ensure the webhook is processed only once even if it's delivered multiple times. How would you implement idempotency at the webhook receiver level using Python and FastAPI?*",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core idempotency skill for payment webhooks role using exact tech (Python/FastAPI); minor -1 for not mentioning signature verification. Clarity: Crystal clear scenario, specific fields, well-scoped for screening. Discriminative: Requires applied knowledge of idempotency (e.g., DB checks, caching), trade-offs like race conditions, distinguishes strong entry-level candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-486",
        "role_title": "Backend Engineer - Span Processing",
        "question": "Design a tail sampling strategy for OpenTelemetry spans in a high-throughput Kafka-based tracing pipeline. The system must handle 100K+ spans per second, with 10% of spans being high-value (e.g., user login, payment processing), and 90% being low-value (e.g., internal service calls). The pipeline processes spans via a Go service that consumes from Kafka, applies sampling logic, and writes to ClickHouse. Tail sampling must ensure that high-value spans are retained with 95% accuracy, while low-value spans are dropped with a configurable threshold. Implement a sampling decision logic that avoids data loss for critical operations and uses minimal CPU overhead. Describe your approach, including how you would validate its effectiveness.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core role skills (tail sampling, span processing, Kafka/Go/ClickHouse pipeline) with precise metrics; minor ding for not mentioning attribute enrichment/storage optimization (-1). Clarity: Crystal clear, well-scoped with throughput, ratios, accuracy targets, and tech stack; no ambiguity or missing constraints. Discriminative: Requires deep applied knowledge of tail sampling trade-offs, low-CPU implementation, validation in high-throughput context; distinguishes experts via design decisions and effectiveness metrics.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-487",
        "role_title": "Senior Backend Engineer - Search Ranking",
        "question": "Design a system to evaluate the performance of a learning-to-rank (LTR) model in a production search environment. The system should:",
        "scores": {
          "relevance": 7,
          "clarity": 4,
          "discriminative": 6,
          "reasoning": "Relevance: Tests core LTR evaluation skills relevant to role (pipelines, production), but generic/misses specific tech stack and role emphases like feature engineering or A/B testing (-3 generic). Clarity: Clear high-level intent but 'design a system' lacks scope/constraints like components, metrics, scale, or time (-2 red flag), multiple interpretations possible. Discriminative: Tests applied knowledge of eval systems (metrics, offline/online, A/B) with some trade-offs, but too open-ended (>10min, vague depth).",
          "composite": 5.666666666666667
        }
      },
      {
        "role_id": "be-488",
        "role_title": "Backend Engineer - Gaming Backend",
        "question": "Design a real-time state synchronization mechanism for a multiplayer game where clients connect via WebSocket and need to keep their game state in sync. The state includes player positions, health, and score. Describe how you would structure the state, handle network latency, and ensure consistency across clients. Specifically, consider the impact of network latency and how you would prevent desynchronization or \"stale\" states.",
        "scores": {
          "relevance": 9,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (real-time state sync, WebSocket implied for multiplayer backend) with exact state elements mentioned; minor deduct for not mentioning matchmaking/anti-cheat (-1). Clarity: Well-scoped and unambiguous, but lacks explicit constraints like scale or perf targets (-1 from red flag on design scope). Discriminative: Excellent at probing trade-offs (latency, consistency models), deep understanding of sync techniques (e.g., snapshots, interpolation, reconciliation); distinguishes strong candidates.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-489",
        "role_title": "Backend Engineer - Data Quality",
        "question": "\"Design a data quality monitoring system using dbt, Great Expectations, and Airflow to detect freshness, volume anomalies, and schema drift in a Snowflake warehouse. Walk through your approach, including how you\u2019d structure the data quality checks, define metrics, and schedule the checks in Airflow. Explain how you\u2019d detect schema drift and handle volume anomalies using statistical methods.\"",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core needs (freshness, volume anomalies, schema drift) and specifies exact tools (dbt, Great Expectations, Airflow, Snowflake). Clarity: Well-scoped with specific components, though minor deduction for lacking explicit time/format constraints. Discriminative: Tests applied design, trade-offs, and statistical methods, strongly separating candidates with deep understanding.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-490",
        "role_title": "Senior Backend Engineer - Environment Management",
        "question": "\"Design and implement a simple Go service that manages a preview environment for a microservice. The service should:",
        "scores": {
          "relevance": 4,
          "clarity": 3,
          "discriminative": 5,
          "reasoning": "Relevance: Superficially mentions Go (assuming in role) but tests generic service design, not core namespace isolation/quotas/TTL; generic (-3), misses specifics. Clarity: Incomplete/broken (cuts off), multiple bundled reqs (-2), no constraints (-2). Discriminative: Vague design task tests basics but poor separation due to ambiguity and breadth (>10min, -2).",
          "composite": 4.0
        }
      },
      {
        "role_id": "be-491",
        "role_title": "Junior Backend Engineer - Read Replicas",
        "question": "You're implementing a read replica routing system using Python and PostgreSQL with SQLAlchemy. Your system needs to route read queries to the appropriate replica based on a consistent, deterministic rule. The primary database is always at `primary.db.example.com:5432`. Read replicas are at `replica1.db.example.com:5432`, `replica2.db.example.com:5432`, and `replica3.db.example.com:5432`.",
        "scores": {
          "relevance": 9,
          "clarity": 6,
          "discriminative": 7,
          "reasoning": "Relevance: Directly tests core read replica routing skills with PostgreSQL/SQLAlchemy (9), minor ding for assuming exact tech stack not explicitly listed (-1). Clarity: Clear setup but incomplete-missing specific task, constraints, or expected output (6). Discriminative: Good setup for testing routing logic and replication considerations, but lacks prompt for trade-offs or lag handling to strongly differentiate (7).",
          "composite": 7.333333333333333
        }
      },
      {
        "role_id": "be-492",
        "role_title": "Backend Engineer - CD Pipelines",
        "question": "\"Design a GitOps-based continuous deployment pipeline using ArgoCD and GitHub Actions that supports multi-environment promotion with approval gates and rollback automation. Explain how you would structure the pipeline, the role of each tool, and how you ensure safety and reliability.\"",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core skills (GitOps, multi-env promotion, approvals, rollbacks) and specifies exact tools; no red flags. Clarity: Crystal clear and well-scoped for screening, minor deduction for lacking explicit time/format constraints. Discriminative: Tests deep applied knowledge, trade-offs, and architecture decisions to separate strong candidates.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-493",
        "role_title": "Staff Engineer - Reliability Standards",
        "question": "\"Design a production readiness review (PRR) checklist for a new Go service deployed to Kubernetes, with monitoring and alerting via Prometheus and PagerDuty. The checklist should include at least one reliability-focused item (e.g., circuit breakers, timeouts), one observability item (e.g., metrics, logs), and one incident management item (e.g., escalation path). Explain your rationale for including each category.\"",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role responsibilities (PRR design, reliability standards) with specific tech stack (Go, K8s, Prometheus, PagerDuty) matching role focus; minor ding for assuming those exact tools not explicitly listed. Clarity: Crystal clear with explicit requirements, categories, and rationale request; well-scoped for screening. Discriminative: Requires applied design thinking, trade-offs in reliability/observability/incident mgmt, and justification, strongly separating candidates with SRE depth.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-494",
        "role_title": "Backend Engineer - API Mocking",
        "question": "\"Design a WireMock stub configuration in TypeScript that handles a GET request to `/api/v1/users/{id}` with the following requirements:\n- Match the path and query parameters (e.g., `?active=true`)\n- Match the request headers, specifically `Authorization: Bearer <token>` and `X-Client-ID: 123`\n- Return a JSON response with user data for a specific user ID (e.g., `id: 42`)\n- Include a 200 status code and a `Content-Type: application/json` header\n- Handle a 404 if the user ID is not found (use a 404 stub with a custom error message)\n- Ensure the stub is reusable and can be easily configured with dynamic values (e.g., via environment variables or configuration files)\"",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests WireMock configuration in TypeScript for API mocking, request matching, and response templating\u2014core to role (-1 minor for assuming WireMock specifically). Clarity: Well-scoped with precise requirements, no ambiguity, single focused task. Discriminative: Requires applied knowledge of matchers, stubs, conditional logic (404), and reusability\u2014distinguishes deep understanding from superficial knowledge.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-495",
        "role_title": "Senior Backend Engineer - Audit Logging",
        "question": "\"Design a Go service that receives audit events from Kafka and writes them to ClickHouse with immutable storage and a time-based retention policy. The service should ensure data integrity and prevent tampering. Explain how you would implement tamper-proof storage and handle retention. Include considerations for high availability, scalability, and performance.\"",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core role skills (tamper-proof storage, retention) using relevant tech (Go, Kafka, ClickHouse); minor ding as role doesn't explicitly list Kafka/ClickHouse (-1). Clarity: Well-scoped design question with clear requirements and considerations, no bundling or vagueness. Discriminative: Probes deep trade-offs in integrity, HA, scalability, and performance, strongly separating strong candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-496",
        "role_title": "Junior Backend Engineer - Model Serving",
        "question": "\"Design and implement a FastAPI-based model serving endpoint that loads a pre-trained scikit-learn model (e.g., Random Forest Classifier) from disk and serves predictions via a REST API. The endpoint should support batch inference on a list of input data (a list of dictionaries), handle model loading and caching via Redis for performance, and include basic error handling and logging.\"",
        "scores": {
          "relevance": 4,
          "clarity": 6,
          "discriminative": 8,
          "reasoning": "Relevance: Low - generic FastAPI/ML serving but adds unmentioned Redis (red flag -2), ignores role's core entry-level focus on model loading/batching/monitoring; superficially mentions tech stack. Clarity: Understandable but vague on specifics like Redis usage details, input format, model path (missing constraints). Discriminative: Good - tests applied implementation with batching/caching/error handling, separates those who can code vs theory.",
          "composite": 6.0
        }
      },
      {
        "role_id": "be-497",
        "role_title": "Backend Engineer - Payout Systems",
        "question": "Design a system to process payout batches for a marketplace platform. Each batch contains multiple user payouts, and payouts must be validated against bank account details before being processed. The system must support batch processing, retry mechanisms, and reconciliation of processed payouts. Describe your architecture, including how you would handle bank account validation, batch processing, and reconciliation. Use Go, PostgreSQL, Kafka, Redis, and gRPC for your design.",
        "scores": {
          "relevance": 7,
          "clarity": 8,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core payout batch processing, validation, and reconciliation from role, but mandates specific tech stack (Go, Kafka, etc.) not listed in role description (-2). Clarity: Well-scoped system design with clear components and tech specified, though lacks time/format constraints (-1). Discriminative: Excellent at probing architecture trade-offs, retries, and integration of validation/reconciliation, strongly separates strong candidates.",
          "composite": 8.0
        }
      },
      {
        "role_id": "be-498",
        "role_title": "Senior Backend Engineer - Profiling Infrastructure",
        "question": "\"Explain how you would design a continuous profiling system for a Go application running in Kubernetes, using Pyroscope for data collection, ClickHouse for storage, and Grafana for visualization. Include how you would handle baseline comparison, detect anomalies, and optimize storage for long-term retention.\"",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core role skills (continuous profiling, flame graphs implied, baseline comparison, storage optimization) and specifies relevant tech (Go/K8s aligns with infra, tools match needs); minor ding for not explicitly mentioning flame graphs (-1). Clarity: Crystal clear, well-scoped design with specific components and features, no ambiguity or missing constraints. Discriminative: Tests deep applied knowledge, trade-offs in design/anomaly detection/storage, strong separation of strong vs weak candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-499",
        "role_title": "Principal Engineer - Search Platform",
        "question": "Design a resilient, scalable search platform for an enterprise with 10M+ daily active users and 50M+ daily search queries. The platform must support real-time indexing and querying of unstructured data (e.g., product catalogs, user-generated content). It must handle data consistency, availability, and fault tolerance across distributed systems. Your solution should include high-level architectural decisions, data flow, fault tolerance, and recovery strategies. Consider the trade-offs between performance, cost, and operational complexity. Explain how you would ensure data consistency and availability under network partitions and node failures. Describe how you would monitor and respond to failures.",
        "scores": {
          "relevance": 4,
          "clarity": 5,
          "discriminative": 7,
          "reasoning": "Relevance: Superficially matches search platform strategy but generic distributed systems design; no role-specific tech stack, enterprise adoption focus, or team/ops strategy (-3 generic, -2 no listed tech). Clarity: Understandable but classic 'design a system' without time/format constraints (-2); multiple aspects bundled (-2). Discriminative: Good on trade-offs and deep topics like CAP/consistency under partitions, separates strong architects, but too broad (>10min).",
          "composite": 5.333333333333333
        }
      },
      {
        "role_id": "be-500",
        "role_title": "Staff Engineer - Streaming Architecture",
        "question": "You're designing a real-time data pipeline for a financial trading platform that must deliver market data to thousands of connected clients with zero data loss and sub-100ms latency. The pipeline consumes market data from Kafka, processes it in Rust, and delivers it via WebSocket. Design the event schema, delivery guarantees, and fan-out pattern for this system. Explain how you would ensure zero data loss and sub-100ms latency.",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (event schemas, delivery guarantees, fan-out) using Kafka (streaming infra) and Rust (processing), perfectly aligned. Clarity: Crystal clear intent and scope for screening, though lacks explicit time/format constraints (-1). Discriminative: Requires deep applied knowledge of trade-offs in schemas, guarantees (e.g., at-least-once), fan-out (e.g., pub-sub), and perf/loss mitigations, strongly separates strong architects from weak ones.",
          "composite": 9.333333333333334
        }
      }
    ]
  },
  "gpt-5-mini": {
    "model": "gpt-5-mini",
    "num_samples": 100,
    "avg_relevance": 9.22,
    "avg_clarity": 9.06,
    "avg_discriminative": 9.18,
    "avg_composite": 9.153333333333334,
    "results": [
      {
        "role_id": "be-403",
        "role_title": "Backend Engineer - Inference Optimization",
        "question": "You have a GPU-backed Triton Inference Server serving a ResNet50 model exported as ONNX. Clients send many small requests (batch size 1) with tight p50 latency requirements. Describe a concrete set of changes you would make (at least 3) to reduce tail latency and improve throughput, including how you would implement them in Triton/TensorRT and any trade-offs (accuracy, memory, complexity).",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core role skills (quantization, batching, GPU memory opt) using exact tech stack (Triton, TensorRT, ONNX, ResNet50); minor -1 for not explicitly listing quantization though implied. Clarity: Crystal clear scenario, specific asks (3+ changes, implementation, trade-offs), well-scoped. Discriminative: Requires applied knowledge of optimizations, trade-offs, and Triton/TensorRT specifics, strongly separates experts from generalists.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-401",
        "role_title": "Backend Engineer - Training Infrastructure",
        "question": "Describe how you would design a Kubernetes-native job scheduler for distributed PyTorch training using Ray to ensure fair and efficient GPU allocation across multiple concurrent jobs. In your answer (2\u20133 short paragraphs), include:\n- how the scheduler would track and reserve GPUs per node,\n- how it would handle Ray cluster autoscaling and pod placement to avoid GPU contention,\n- and one concrete policy for prioritizing or preempting jobs when GPUs are scarce.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core role skills (distributed ML training, GPU allocation, scheduling) with exact tech stack (Kubernetes, PyTorch, Ray); minor ding for Ray not explicitly listed but fits infrastructure context (-1 red flag avoided). Clarity: Crystal clear with specific requirements, scoped length, and no ambiguity. Discriminative: Tests deep applied knowledge, trade-offs in autoscaling/pod placement, and policy design, strongly separating experts from superficial knowers.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-402",
        "role_title": "Backend Engineer - Model Deployment",
        "question": "Describe a production deployment plan to roll out a new ML model version using KServe with canary releases and automated rollback on increased error rate. Include the metrics to monitor, how you'd implement the traffic split and promotion/rollback logic (mention KServe features or Kubernetes primitives), and a brief outline of the Python automation components that would perform detection and trigger rollout/rollback.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Relevance: Directly tests core role skills (model deployment, strategies, rollback) with KServe (ML deployment tool); minor ding as KServe not explicitly in role desc but fits perfectly (-1). Clarity: Well-scoped, specific asks with no ambiguity, ideal for screening. Discriminative: Requires deep applied knowledge of KServe/K8s, trade-offs in metrics/traffic/automation, strongly separates experts from juniors.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-406",
        "role_title": "Senior Backend Engineer - LLM Serving",
        "question": "You\u2019re building a high-throughput vLLM-based serving layer on Kubernetes that must support many concurrent chat sessions with low latency. Describe a strategy to manage and optimize the KV cache (key-value attention cache) across requests to minimize memory use and maximize reuse, including:\n- what data you would evict or shard and why,\n- how you\u2019d handle cache consistency across model replicas/pods,\n- how you\u2019d integrate this with request batching and streaming responses,\n- and one metric you\u2019d track to validate the approach.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Relevance: Directly tests core role skills (KV cache management, batching, streaming) with vLLM/K8s from description; minor ding for not explicitly mentioning batching optimization (-1). Clarity: Crystal clear, well-scoped with 4 specific subpoints, no bundling issues. Discriminative: Excellent at probing deep trade-offs, applied reasoning, and metrics; strong separation of experts from juniors.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-405",
        "role_title": "Senior Backend Engineer - Model Monitoring",
        "question": "Describe a production design for monitoring real-time ML model drift and performance using Python, Prometheus, Grafana, and PostgreSQL. In your answer (2\u20133 minutes) include:\n- what metrics you would collect (at prediction and batch levels) and why,\n- how you\u2019d detect concept and data drift (algorithms/approaches) and where that logic runs,\n- how you\u2019d store and aggregate metrics in PostgreSQL for historical analysis and query examples,\n- how you\u2019d expose metrics to Prometheus and build Grafana alerts/dashboards,\n- one tradeoff and one failure mode you would mitigate and how.",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Relevance: Perfectly matches role's core focus on model monitoring, drift detection, performance tracking, and alerting; uses relevant tech stack (Python, Prometheus, Grafana, PostgreSQL) without red flags. Clarity: Crystal clear with specific subtopics and time constraint; minor deduction for no explicit format guidance. Discriminative: Tests deep applied knowledge, trade-offs, and failure modes, strongly separating strong ML engineers from weak ones.",
          "composite": 9.666666666666666
        }
      },
      {
        "role_id": "be-410",
        "role_title": "Backend Engineer - Bulk Operations",
        "question": "Describe how you would design a REST API endpoint in Go to upsert a batch of 10,000 records into PostgreSQL, ensuring:\n- reasonable memory use and throughput,\n- idempotency for retries,\n- partial success reporting to the client,\nand how you would handle coordinating with Kafka for downstream processing and Redis for deduplication. Outline the flow, key data structures, error-handling strategy, and any PostgreSQL features (e.g., transactions, COPY, ON CONFLICT) you would use. Keep the answer to ~6\u201310 bullet points.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core role needs (bulk ops, batch processing, partial success, idempotency) with exact tech stack (Go, PostgreSQL); minor deduction as Kafka/Redis not explicitly in role but support idempotency/deduplication (-1). Clarity: Crystal clear, well-scoped with bullet-point limit, specific constraints, no ambiguity. Discriminative: Tests deep applied knowledge, trade-offs (memory/throughput), integration, and PostgreSQL features, strongly separates strong candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-408",
        "role_title": "Junior Backend Developer - Health Endpoints",
        "question": "Describe how you would implement liveness and readiness HTTP endpoints in a Go web service running in Kubernetes. Include:\n- the minimal behaviors each endpoint should have (what they must check or not check),\n- example HTTP status codes to return for healthy vs unhealthy,\n- how you would expose a Prometheus metrics endpoint alongside them,\n- and how you would configure Kubernetes liveness and readiness probes (path, port, initial delay, period) for a service that depends on an external database connection.",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Directly tests core role skills (liveness/readiness/metrics in Go/K8s) with no red flags. Crystal clear with specific subpoints, only minor vagueness on exact config values. Strong separation via applied knowledge, trade-offs in probe configs, and integration\u2014not pure recall.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-407",
        "role_title": "Staff Backend Engineer - ML Infrastructure",
        "question": "Describe a design for a reproducible, cost-efficient distributed training platform for large PyTorch models on Kubernetes that uses Ray for job orchestration and supports multi-tenant isolation. In 2\u20133 short paragraphs, explain:\n1) how you\u2019d schedule and launch training jobs (Ray + Kubernetes interaction, resource types, autoscaling),\n2) how you\u2019d ensure reproducibility and model versioning (data, code, env, random seeds),\n3) two concrete cost-optimization strategies you would implement (with brief trade-offs).",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core focus on ML infrastructure, training platforms, cost optimization, and specifies exact tech (PyTorch, K8s, Ray) implied for such infra. No red flags.\nClarity: Crystal clear, well-scoped with specific points, paragraph limit, and structure; minor deduction as 'large models' lacks exact scale but still precise for screening.\nDiscriminative: Excellent at testing deep applied knowledge, trade-offs, and integration of multiple systems, strongly separating experienced ML infra leads from juniors.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-409",
        "role_title": "Junior Backend Engineer - File Upload",
        "question": "Design a small FastAPI endpoint (describe routes and request/response shapes\u2014no full code needed) to handle a client uploading a single file up to 50 MB that should be validated and stored to S3, and metadata recorded in PostgreSQL. Include:\n\n- the HTTP method and path,\n- how you'd accept the file in FastAPI (what parameter/type),\n- what validation you'd perform before upload (list checks),\n- how you'd stream or buffer the file to S3 to avoid high memory use,\n- what minimal metadata you'd write to PostgreSQL and when (what transaction strategy if any),\n- one brief error response you would return if validation fails.\n\nKeep the answer concise (2\u20135 short bullet points or lines).",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role (FastAPI file upload, multipart implied, validation, S3/PostgreSQL integration). Clarity: Crystal clear with specific subpoints and scoping (no full code, concise answer); minor -1 for bundled sub-asks. Discriminative: Tests applied knowledge of streaming, transactions, validation trade-offs for entry-level, strongly separates candidates.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-404",
        "role_title": "Senior Backend Engineer - Feature Platform",
        "question": "Describe how you would design and implement an end-to-end feature pipeline for a new real-time feature used in an online fraud detection model, using Feast (for feature store), Spark (for batch backfill/aggregation), Redis (for low-latency serving), and Python. In your answer, briefly outline:\n- the data sources and ingestion steps,\n- how you would compute and materialize the feature (batch and streaming) and handle backfills/consistency,\n- Feast entities/feature views and serving configuration (online vs. offline),\n- how you'd ensure low-latency reads in Redis and handle staleness/failure modes,\n- one observability or testing approach you would add to validate correctness in production.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core feature platform skills (computation, serving, discovery) with exact tech stack (Feast, Spark, Redis, Python) from role; minor generic penalty avoided (-0). Clarity: Crystal clear, well-scoped with 5 specific bullet points, no bundling issues. Discriminative: Probes deep applied knowledge, trade-offs (staleness, consistency, latency), and production concerns, strongly separating strong ML platform engineers.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-411",
        "role_title": "Backend Engineer - Admin APIs",
        "question": "Design a FastAPI endpoint for an admin-only route that deactivates a user account and records an audit log. In 6\u201310 lines of pseudocode or Python (no full app required), show:\n- the endpoint signature and method,\n- how you would enforce admin/privileged access (check a role in the request or token),\n- the transactional sequence to mark the user as deactivated in PostgreSQL and write an audit entry,\n- how you would handle and return errors if either the user does not exist or the audit write fails.\n\nAssume you have access to a DB session, a Redis cache client, and an Audit model; focus on control flow and error handling, not ORM specifics.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests FastAPI, PostgreSQL, admin auth/roles, audit logging, and transactional workflows\u2014core to role (-0 red flags). Clarity: Crystal clear with precise scope (6-10 lines pseudocode), explicit requirements, and assumptions provided. Discriminative: Requires applied knowledge of auth, transactions, error handling, and control flow trade-offs, strongly separating capable candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-416",
        "role_title": "Staff Backend Engineer - External APIs",
        "question": "Design a short API integration plan (4\u20136 bullet points) for exposing a new partner-facing endpoint that returns aggregated customer spending across multiple internal services. Include: the OpenAPI versioning approach, request/response shape considerations, authentication/authorization pattern (for partners), rate limiting/monetization model, and how you\u2019d deploy and enforce this with Kong on Kubernetes.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core API strategy, versioning, monetization, partner auth, and integration patterns with Kong/K8s (relevant for deployment/enforcement); minor ding as role emphasizes 'lead strategy' over specific tool like Kong (-1). Clarity: Crystal clear, well-scoped to 4-6 bullets with explicit topics, no bundling or ambiguity. Discriminative: Requires applied knowledge of trade-offs in API design, auth, rate limiting, monetization, and infra, strongly separating strategic thinkers from basics.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-415",
        "role_title": "Senior Backend Engineer - Multipart APIs",
        "question": "Design a resumable multipart upload API using Python + FastAPI for large files stored in S3. In 6\u20138 bullet points, describe the key endpoints you would expose, the request/response for each (including headers or parameters needed), how you would track upload state and chunks (data model or storage), how you'd ensure idempotency and handle duplicate/overlapping chunks, and how you\u2019d finalize and verify the assembled object in S3. Focus on concrete implementation details a senior engineer would use (e.g., checksums, chunk numbering, expiration, concurrency control), not high-level prose.",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Relevance: Perfectly matches role's core focus on resumable uploads, chunk management, S3, with Python/FastAPI directly testing required skills (no red flags). Clarity: Crystal clear, well-scoped with bullet point format, specific focus areas, and concrete details requested (minor deduction for no explicit time limit). Discriminative: Requires deep applied knowledge of trade-offs (idempotency, checksums, concurrency, state tracking), distinguishing seniors via implementation specifics vs superficial answers.",
          "composite": 9.666666666666666
        }
      },
      {
        "role_id": "be-412",
        "role_title": "Backend Engineer - Analytics APIs",
        "question": "Design an endpoint for a dashboard that returns per-minute counts of user events for a given 24-hour window, with optional filters by event type and user segment. Outline, in 6\u20138 bullet points, how you would implement it in Go using ClickHouse for storage and Redis for caching. Include (briefly) the ClickHouse query shape for efficient aggregation, cache key strategy and TTL, how to handle partial cache hits, and one safety/consistency consideration.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (aggregation queries, time-series, caching) with exact tech stack (Go, ClickHouse, Redis); minor deduction as role didn't specify Go (-1). Clarity: Crystal clear, well-scoped with bullet count, specific elements required, no ambiguity. Discriminative: Tests applied deep knowledge, trade-offs (partial cache, safety), query efficiency, and implementation specifics to separate strong candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-413",
        "role_title": "Senior Backend Engineer - API Gateway Logic",
        "question": "Describe how you would implement a per-tenant request transformation and routing feature in an Envoy-based API gateway using Go for control-plane logic and Lua for per-request processing. In your answer (2\u20133 short paragraphs), include:\n- where you would store tenant rules and how the proxy fetches them,\n- how you\u2019d apply transformations and routing decisions at request time (Envoy/Lua config and performance considerations),\n- how you\u2019d handle dynamic updates and safety (rollout, validation, failure modes).",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core role skills (API gateway extensions, request transformation, routing) with exact tech stack (Envoy, Go control-plane, Lua); minor ding for assuming Lua without explicit role mention (-1). Clarity: Crystal clear, well-scoped with specific points, paragraph limit, and no bundling/ambiguity. Discriminative: Probes deep applied knowledge, trade-offs (performance, safety, dynamic updates), distinguishing strong Envoy/Go/Lua experts from superficial ones.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-417",
        "role_title": "Junior Backend Developer - Event Sourcing",
        "question": "Given a Java service consuming user-account events from Kafka (events: AccountCreated, MoneyDeposited, MoneyWithdrawn) and storing a projection in PostgreSQL with columns (account_id, balance, version), briefly describe in 6\u20138 lines how you would implement the event handler for MoneyDeposited to ensure correct balances under concurrent delivery and out-of-order events. Mention how you would use the version column and any PostgreSQL feature or transaction pattern you\u2019d rely on.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core event sourcing skills (event handlers, projections, concurrency) for the role; uses exact events and tech stack (Java, Kafka, PostgreSQL); minor ding for not specifying all events but still spot-on (-1). Clarity: Crystal clear, well-scoped (6-8 lines, specific event), precise constraints. Discriminative: Excellent at separating candidates via trade-offs in versioning (optimistic locking), concurrency (idempotency, out-of-order handling), PostgreSQL features (UPSERT, transactions, SERIALIZABLE); requires applied deep understanding.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-414",
        "role_title": "Senior Backend Engineer - Long-Polling",
        "question": "Design a long-polling endpoint in Go for reporting progress of an async job (job IDs stored in PostgreSQL; recent progress updates pushed into Redis). In ~6\u20138 sentences describe:\n\n- the HTTP handler flow for a client that polls for job \"job123\" (how you check current state, decide to wait, and return),\n- how you\u2019d use Redis to avoid busy-waiting and wake waiting pollers when progress arrives,\n- how you\u2019d handle client timeouts and server-side cleanup to avoid leaked goroutines/connections,\n- one fallback strategy if Redis notifications are delayed or unavailable.\n\nBe concise and focus on concrete implementation choices (e.g., Redis pub/sub vs. streams, channels, context cancellation, use of setTimeout-like patterns).",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core role skills (long-polling APIs, connection mgmt, timeouts, fallbacks) with exact tech (Go, PostgreSQL, Redis); no red flags. Clarity: Crystal clear with precise scope (~6-8 sentences), specific subtopics, and concrete guidance; well-constrained for screening. Discriminative: Tests applied deep knowledge, trade-offs (pub/sub vs streams, context cancellation), and implementation choices to strongly separate strong candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-418",
        "role_title": "Junior Backend Engineer - Service Clients",
        "question": "Describe how you would implement a simple retry strategy for a Go gRPC client call that should:\n- retry only on transient errors (e.g., UNAVAILABLE, DEADLINE_EXCEEDED),\n- use exponential backoff with jitter,\n- limit to a maximum of 5 attempts and a total timeout of 10s,\n- avoid retrying non-idempotent RPCs.\n\nIn your answer, outline the key code-level steps or pseudocode (no full program required) and mention which gRPC or Go primitives/packages you would use.",
        "scores": {
          "relevance": 9,
          "clarity": 10,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core retry implementation for service clients in Go gRPC (-1 for not mentioning circuit breakers explicitly). Clarity: Crystal clear with precise constraints, scoped perfectly for screening, specifies pseudocode. Discriminative: Requires applied knowledge of gRPC errors, backoff/jitter, idempotency trade-offs, and Go primitives (e.g., google.golang.org/grpc/backoff), distinguishing strong entry-level candidates.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-420",
        "role_title": "Backend Engineer - Graceful Shutdown",
        "question": "Describe a concrete shutdown sequence for a Go gRPC server running in Kubernetes that ensures zero-downtime deployments. Include: the Kubernetes readiness/liveness signal changes, how you stop accepting new requests, how you handle in-flight RPCs and long-lived streams, how you coordinate with connection tracking (e.g., load balancer or proxy), and how you ensure the Pod terminates only after work is complete. Mention any relevant timeouts, gRPC APIs, and Kubernetes settings you would configure.",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Relevance: Perfectly matches role's core focus on graceful shutdown, zero-downtime deploys, signals, and connection draining in Go/gRPC/K8s context; no red flags. Clarity: Crystal clear, well-scoped with specific elements to cover, minor deduction only for lacking explicit time/format constraints. Discriminative: Requires deep applied knowledge of trade-offs, integrations (gRPC APIs, K8s probes, timeouts), and real-world coordination, strongly separating experts from others.",
          "composite": 9.666666666666666
        }
      },
      {
        "role_id": "be-419",
        "role_title": "Backend Engineer - Retry Logic",
        "question": "Describe, in Go-like pseudocode, a function that schedules retries for a distributed operation using exponential backoff with jitter and enforces a per-operation retry budget (max attempts and total elapsed time). Specify:\n- inputs (initialDelay, maxDelay, multiplier, jitterFraction, maxAttempts, maxElapsed)\n- how jitter is applied (explain exact formula)\n- how you decide to stop retrying (conditions)\n- one safety / distributed-systems concern (e.g., clock skew, duplicate side effects) and how you'd mitigate it.\n\nKeep the answer concise (pseudocode + 3\u20136 short bullets).",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Relevance: Perfectly matches role's core focus on retry infrastructure with exact parameters and concepts (10, no red flags). Clarity: Crystal clear with specific inputs, requirements, format, and scope; minor -1 for lacking strict word limit but well-constrained (9). Discriminative: Tests deep applied knowledge of backoff/jitter formulas, budget logic, pseudocode, and distributed systems trade-offs/safety\u2014strong separation of experts (10, no red flags).",
          "composite": 9.666666666666666
        }
      },
      {
        "role_id": "be-421",
        "role_title": "Senior Backend Engineer - Security Automation",
        "question": "Describe how you would design an automated pipeline that detects and remediates leaked static secrets in a Kubernetes cluster using Vault for secrets management. In your answer, include: the high-level components you would use (tools/services), how you would detect leaks (what signals and scanning approach), how you would validate and rotate affected secrets in Vault safely, how you would apply fixes to workloads in Kubernetes, and how you would ensure the pipeline is secure and minimizes disruption. Keep it high-level (no code), but include sequence/order and one potential failure mode and mitigation.",
        "scores": {
          "relevance": 9,
          "clarity": 10,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core security automation skills for vuln scanning/secrets rotation in cloud/K8s, matching role; minor ding for not explicitly mentioning compliance (-1). Clarity: Crystal clear, well-scoped with explicit requirements, sequence, high-level constraint, no ambiguity. Discriminative: Tests applied design knowledge, trade-offs (safety, disruption), failure modes; strong separation of deep vs shallow candidates.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-423",
        "role_title": "Staff Engineer - Incident Response Platform",
        "question": "Your incident platform receives alerts from Prometheus for a high-severity SLO breach. Describe, step-by-step, what automated actions your system should take within the first 5 minutes to (a) reduce customer impact and (b) ensure effective human response. For each step, name which component/service you'd implement it in (Go service, Kubernetes operator/job, PagerDuty, Slack integration, PostgreSQL, etc.) and one short reason why.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core incident response, SLO handling, alerting (Prometheus), and integration skills from role; minor ding for assuming PagerDuty/Slack not explicitly listed (-1). Clarity: Crystal clear, well-scoped to 5 minutes with parts (a/b), requires specific components/reasons. Discriminative: Forces applied reasoning on automation trade-offs, sequencing, and architecture choices, strongly separates candidates with SRE/incident expertise.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-422",
        "role_title": "Junior Backend Engineer - API Testing",
        "question": "You have a FastAPI endpoint POST /users that accepts JSON { \"email\": str, \"name\": str } and returns 201 with created user { \"id\": int, \"email\": str, \"name\": str }. Using pytest and TestClient, write a concise test that verifies:\n- posting valid data returns 201 and response body contains the same email and name and an integer id,\n- posting without \"email\" returns a 422 (validation error).\n\nYou don't need to implement the API\u2014just show the pytest test functions (including any minimal fixtures/imports) you would write.",
        "scores": {
          "relevance": 9,
          "clarity": 10,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests pytest + TestClient for API testing, core to role's API tests focus (-1 for not hitting contract testing/fixtures explicitly). Clarity: Crystal clear, well-scoped with exact requirements, no ambiguity. Discriminative: Requires writing functional test code covering happy path + validation error, tests applied pytest skills and FastAPI error handling knowledge, distinguishes capable entry-level from weak.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-425",
        "role_title": "Senior Backend Engineer - Graph Analytics",
        "question": "Describe a concrete approach to optimize a Neo4j query that computes personalized PageRank for a single user on a billion-edge social graph, to run within tight latency (e.g., <5s). Include: data/modeling changes, query or algorithm choice (built-in vs. custom), use of caching or precomputation (Redis/Postgres), and how you'd integrate Spark if needed. Be specific about trade-offs.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Relevance: Directly targets core role skills (graph analytics, Neo4j optimization, billion-scale traversal/PageRank) with minor ding for assuming Neo4j unlisted (-1). Clarity: Precise, well-scoped with specific components and example latency, no ambiguity. Discriminative: Forces deep trade-off analysis across modeling, algorithms, caching, and Spark integration, strongly separates experts from pretenders.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-424",
        "role_title": "Backend Engineer - Message Serialization",
        "question": "Describe how you would design an Avro-based message serialization strategy for Kafka that ensures backward and forward compatibility across services that evolve independently. In your answer, include:\n- the schema evolution rules you would enforce,\n- how you would use Schema Registry (including schema registration and compatibility settings),\n- how you would handle consumers that cannot be updated immediately (rolling upgrades),\n- one concrete example of a compatible schema change and one incompatible change and how your system would detect/prevent the incompatible change.",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core focus on schema evolution, backward/forward compatibility, and validation using Avro/Schema Registry (directly tests listed skills, no red flags). Clarity: Crystal clear, well-scoped with specific subpoints, minor deduction for no explicit time constraint. Discriminative: Tests deep applied knowledge, trade-offs in evolution rules, Schema Registry config, upgrade strategies, and examples, strongly separates experts from juniors.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-426",
        "role_title": "Junior Backend Engineer - DevOps Scripting",
        "question": "Write a short Bash script (6\u201312 lines) that will be used in a GitHub Actions job to: 1) check that Docker is installed and running, 2) build a Docker image from the repository\u2019s Dockerfile with tag myapp:ci, and 3) run the image\u2019s tests by executing pytest inside a container (fail the script if any step fails). Include comments explaining each step.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core DevOps automation skills (Bash scripting, Docker, CI pipelines) for entry-level role; minor ding as GitHub Actions is implied not explicit (-1). Clarity: Precise line limit, clear 3 steps, comments required, well-scoped. Discriminative: Requires applied knowledge of Docker commands, container exec, error handling, and Bash scripting; distinguishes those who can integrate tools vs rote knowledge.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-428",
        "role_title": "Senior Backend Engineer - Payment Orchestration",
        "question": "Design a Go function signature and describe the algorithm (in 6\u201310 short bullet points) for selecting a payment provider for a transaction given: real-time provider health metrics (success rate, latency), per-provider cost, per-provider daily volume limits, and retry/failover constraints. Include how you would incorporate recent success rate decay, avoid flapping between providers, and handle concurrent requests updating usage counts stored in Redis. Keep it focused\u2014no full code, just the signature and the algorithm steps.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (routing, failover, cost/success optimization) with Go and Redis implied in context; minor generic ding (-1). Clarity: Crystal clear scope (signature + 6-10 bullets, focused constraints), no ambiguity. Discriminative: Requires applied trade-offs (decay, flapping, concurrency, metrics weighting), distinguishes deep understanding from superficial knowledge.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-427",
        "role_title": "Backend Engineer - Feature Store",
        "question": "Describe how you would design and implement a Feast feature table for a real-time user churn prediction feature \"days_since_last_activity\" that is computed from an events stream (Kafka) and materialized for low-latency serving from Redis, while keeping historical versions in PostgreSQL for training. In your answer (2\u20133 short bullet points or steps), specify:\n- the Feast entity and feature table schema (fields and types),\n- how you would compute and materialize the feature in real time (mention components: streaming job, Feast ingestion, Redis), and\n- how you would ensure consistent versioning/backfills for training data in PostgreSQL.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests Feast (core to feature stores), real-time streaming (Kafka/Redis), versioning (PostgreSQL), and ML pipeline skills matching role (-0 red flags). Clarity: Crystal clear with explicit specs, scoped to 2-3 bullets, no ambiguity or bundling. Discriminative: Requires applied knowledge of Feast schema/entities, streaming pipelines, and backfill/versioning trade-offs, strongly separates experienced candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-429",
        "role_title": "Backend Engineer - Log Aggregation",
        "question": "You operate a log pipeline in Go that receives JSON log events from Kafka at millions/sec, enriches them (add geo IP, service metadata), and writes them to ClickHouse via batch inserts. After deploying an enrichment change, you observe a small percentage of logs now fail ClickHouse inserts due to type mismatches introduced by the enrichment (e.g., adding nulls where non-nullable columns exist). Describe, step-by-step, how you would detect, mitigate, and permanently fix this problem in production with minimal data loss and downtime. Mention concrete techniques/tools (within the stack: Go, Kafka, ClickHouse, Vector, Kubernetes) and specific short-term vs long-term actions.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Relevance: Directly tests core log pipeline skills (parsing/enrichment/retention at scale) with exact stack (Go, Kafka, ClickHouse); minor -1 for including Vector/K8s not in role desc but highly relevant. Clarity: Crystal clear scenario, well-scoped steps, explicit stack/tools/constraints. Discriminative: Forces trade-offs (minimal loss/downtime), production debugging, short/long-term strategies distinguishing deep ops experience from superficial knowledge.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-431",
        "role_title": "Staff Engineer - Real-time Collaboration",
        "question": "Design a high-level architecture (components and data flow) for a multi-user real-time text editor using CRDTs over WebSocket. Include how you would:\n- ensure causality and convergence between clients,\n- handle offline edits and reconnection,\n- persist document state in PostgreSQL and use Redis for presence/notification,\n- mitigate unbounded metadata growth in the CRDT.\n\nBe concise (3\u20136 bullet points or short paragraphs).",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's focus on real-time collaboration, CRDTs, operational transforms (implied), presence, conflict resolution; uses exact tech (PostgreSQL, Redis); no red flags. Clarity: Crystal clear, well-scoped with bullet points, constraints, and conciseness limit; minor jargon but contextual. Discriminative: Tests deep applied knowledge, trade-offs (causality, growth mitigation), architecture skills; distinguishes strong architects from basics; concise format prevents overlong answers.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-435",
        "role_title": "Junior Backend Engineer - Queue Consumers",
        "question": "You have a Python consumer that reads JSON messages from a RabbitMQ queue and inserts rows into PostgreSQL. Occasionally the same message is delivered twice. Describe a simple, practical design (including any DB schema changes and RabbitMQ acknowledgment strategy) to ensure each message is processed exactly once (idempotent behavior) without requiring distributed transactions. Explain briefly how your design handles duplicate deliveries and what the consumer should do on processing errors.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core idempotency skills for message queue consumers using exact tech stack (RabbitMQ, PostgreSQL, Python); minor deduction as it doesn't explicitly cover dead letter handling. Clarity: Crystal clear, well-scoped with specific constraints (no distributed tx, simple/practical), single focused question. Discriminative: Requires applied knowledge of acks, DB upserts, deduping strategies, error flows, and trade-offs, strongly separating candidates with real understanding.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-430",
        "role_title": "Junior Backend Engineer - Search Indexing",
        "question": "You need to build a periodic job (runs every few minutes) that updates an Elasticsearch index from a PostgreSQL table of products. Describe, in bullet points, a simple, reliable approach to:\n\n- detect which rows changed in PostgreSQL since the last run,\n- transform rows into documents suitable for Elasticsearch,\n- send updates efficiently to Elasticsearch (minimize load and ensure consistency),\n- handle transient failures so you don\u2019t lose updates or create duplicates.\n\nBe concise \u2014 6\u201310 short bullets covering the key steps/techniques and any small decisions (e.g., use of timestamps, bulk API, idempotency).",
        "scores": {
          "relevance": 9,
          "clarity": 10,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core skills (bulk indexing, search indexing jobs) for entry-level role; minor ding as doesn't explicitly cover mapping design or relevance tuning (-1). Clarity: Crystal clear, well-scoped with constraints (periodic, simple/reliable, 6-10 bullets, specific sub-asks). Discriminative: Requires applied knowledge of CDC patterns, ES bulk API, idempotency, failure handling; distinguishes those understanding trade-offs from rote memorizers.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-432",
        "role_title": "Backend Engineer - Data Validation",
        "question": "Describe how you would implement a Great Expectations checkpoint in an Airflow DAG to validate a daily batch of parquet files stored in S3 before loading into Postgres. Include:\n- the key expectations you would define (at least three) for schema, nulls, and distribution/uniqueness;\n- how you would trigger and fail the Airflow task on validation errors;\n- how you would surface validation results and lineage (where you would store reports and metadata).\n\nKeep the answer concise (2\u20135 minutes).",
        "scores": {
          "relevance": 9,
          "clarity": 10,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core data validation, schema checks, and lineage using Great Expectations (ideal for pipeline quality), Airflow, S3, Parquet, Postgres\u2014all highly relevant; minor ding as role doesn't explicitly list GE/Airflow (-1). Clarity: Crystal clear with specific requirements, scoped by time and components, no ambiguity. Discriminative: Requires applied knowledge of integration, trade-offs in expectations, failure handling, and reporting\u2014strong separation of experts from juniors.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-434",
        "role_title": "Principal Engineer - Database Strategy",
        "question": "You\u2019re designing a multi-region database strategy for a customer-facing SaaS with global users and strict RPO/RTO SLAs. Given PostgreSQL, CockroachDB, Redis, Kafka, and Kubernetes, describe (in 3\u20135 bullet points) your recommended architecture for OLTP (primary transactional) and caching/streaming components, explain one major trade-off for your choice, and list two operational controls you\u2019d enforce to meet RPO/RTO across regions.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core multi-region DB strategy, tech selection (PostgreSQL, CockroachDB match distributed needs; Redis/Kafka for caching/streaming; Kubernetes for ops), and migration/ops excellence (-1 for minor techs like Redis not core OLTP but role-appropriate). Clarity: Precise scope (3-5 bullets, components, trade-off, controls), unambiguous. Discriminative: Requires applied architecture knowledge, trade-offs, and RPO/RTO ops, distinguishing strong strategists.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-433",
        "role_title": "Senior Backend Engineer - Platform APIs",
        "question": "Design a small Go-based HTTP/gRPC service interface for a platform \"service catalog\" that lets developers (1) list available services, (2) request provisioning of a service instance with parameters, and (3) get the provisioning status. For each endpoint/method, specify:\n- the endpoint/method signature (HTTP path + verb or gRPC RPC name and request/response messages),\n- key request and response fields (include minimal validation rules),\n- one database table schema (Postgres) to store service instances and their status,\n- and one brief note on how you would ensure idempotent provisioning requests (concurrent-safe).\n\nKeep answers concise (bullet points OK).",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core role skills (platform APIs, service catalog, provisioning) with exact Go/HTTP/gRPC/Postgres match; minor ding for not explicitly mentioning self-service UI. Clarity: Crystal clear, well-scoped with exact requirements and constraints. Discriminative: Tests applied design (signatures, validation, schema, idempotency), trade-offs (concurrency), distinguishes strong API designers from basics.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-437",
        "role_title": "Senior Backend Engineer - Secrets Management",
        "question": "Design, at a high level, an automated secret rotation system for database credentials stored in HashiCorp Vault used by services running in Kubernetes. Explain the Vault components and policies you would use, how rotation would be triggered and performed (including how you would ensure zero-downtime credential swaps), how services authenticate to Vault, and how audit logging and rotation failures would be handled. Be specific about Vault roles/mounts/secret engines, Kubernetes auth, and any coordination with AWS KMS or PostgreSQL. (Answer in 2\u20135 minutes.)",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core secrets management, rotation, Vault, K8s, encryption (KMS), and DB (PostgreSQL) from role; minor ding for assuming AWS/PostgreSQL not explicitly listed (-1). Clarity: Crystal clear, well-scoped with time limit, specific components listed, no bundling or ambiguity. Discriminative: Requires deep applied knowledge of trade-offs (zero-downtime, failure handling), Vault internals, and integrations; strong separation of experts from juniors.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-436",
        "role_title": "Backend Engineer - API Documentation",
        "question": "You\u2019re building a TypeScript-based tool that scans a Node.js codebase to auto-generate OpenAPI specs for multiple API versions (v1, v2). Describe, at a high level and in 6\u20138 brief steps, how you would implement:\n- detection and grouping of endpoints by version,\n- generation of OpenAPI paths/components with example payloads,\n- and a strategy to handle incremental updates (only regenerate changed endpoints) so SDKs can be rebuilt efficiently.\n\nBe specific about file/metadata sources you\u2019d use (route annotations, code comments, separate manifest, tests), how you\u2019d generate examples (static examples, JSON Schema-based fuzzing, using TypeScript types), and how you\u2019d detect changes (hashing, timestamps, AST diffs).",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core role skills (auto-gen specs, versioning, SDK automation) with exact tech stack (TypeScript/Node.js/OpenAPI); minor deduction as role doesn't explicitly list all sub-details like AST diffs (-1). Clarity: Crystal clear, well-scoped (6-8 steps, high-level), specific constraints provided, no bundling. Discriminative: Requires applied knowledge of parsing, example gen strategies, change detection trade-offs; distinguishes deep thinkers via specifics like sources/hash methods.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-439",
        "role_title": "Backend Engineer - Billing Integration",
        "question": "Design a minimal approach (2\u20134 bullet steps or a short code sketch) to handle prorated subscription upgrades in a SaaS app using Stripe, PostgreSQL, and Celery. Include how you would:\n- detect and calculate a prorated charge when a customer upgrades mid-period,\n- create or adjust the Stripe invoice/payment so the customer is billed correctly,\n- update your local PostgreSQL subscription record atomically,\n- and ensure the background processing is idempotent (Celery task considerations).\n\nKeep the answer concise.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core billing/proration skills with exact tech stack (Stripe, PostgreSQL, Celery); no red flags. Clarity: Crystal clear, well-scoped with constraints (2-4 bullets/code sketch, concise), specific subpoints without bundling. Discriminative: Requires applied knowledge of integration, atomicity, idempotency, and trade-offs, strongly separates strong candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-440",
        "role_title": "Junior Backend Engineer - Metrics Collection",
        "question": "Describe how you would instrument a simple HTTP handler in Go to track:\n1) total requests received,\n2) current in-flight requests,\n3) request duration distribution (suitable for latency percentiles in Grafana).\n\nSpecify which Prometheus metric types you would use for each, show the minimal Go code (using the Prometheus Go client) to register and update these metrics inside the handler, and explain any important labeling choices.",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role (instrumenting with counter/gauge/histogram for metrics/dashboards in Go, Prometheus implied for Grafana). No red flags. Clarity: Crystal clear, well-scoped with specifics (types, minimal code, labels); minor deduction as no explicit time constraint. Discriminative: Requires applied knowledge of metric patterns, Go client usage, labeling trade-offs; distinguishes those who can implement from theorists.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-445",
        "role_title": "Junior Backend Engineer - Database Migrations",
        "question": "You need to add a non-nullable column \"status\" (text) to a large PostgreSQL table \"orders\" in production without causing downtime. Describe, step-by-step, how you would implement this change using Alembic migrations and SQL (including any data backfill and how to avoid long locks). Include the commands or SQL snippets you would run in each migration step and explain why each step is needed.",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Relevance: Perfectly targets core role skills (database migrations, zero-downtime, PostgreSQL) with Alembic; no red flags. Clarity: Crystal clear, step-by-step scope, specific constraints; minor deduction for lacking explicit time limit. Discriminative: Tests deep applied knowledge of migration patterns, backfill, locks, and trade-offs, strongly separates capable candidates.",
          "composite": 9.666666666666666
        }
      },
      {
        "role_id": "be-438",
        "role_title": "Staff Engineer - ML Serving Infrastructure",
        "question": "Describe a production design to serve a deep-learning model for real-time (\u226450ms p99) inference at 10,000 QPS using Triton on Kubernetes. In your answer (2\u20133 short paragraphs or bullet points) include: how you would configure model versioning and rollout, batching strategy (static vs dynamic, max batch size, latency tradeoffs), GPU utilization (TensorRT usage, instance sizing), and what Prometheus metrics/alerts you would instrument to detect SLA breaches and model regressions.",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Relevance: Perfectly matches role's core focus on ML serving infra, batching, versioning, and latency opt, directly testing Triton/K8s stack. Clarity: Crystal clear with precise scope (\u226450ms p99, 10k QPS), explicit requirements, and response format; minor jargon assumed for role. Discriminative: Forces deep trade-offs (batching types/sizes, GPU sizing, metrics), distinguishing architects from juniors via production reasoning.",
          "composite": 9.666666666666666
        }
      },
      {
        "role_id": "be-441",
        "role_title": "Senior Backend Engineer - Autocomplete Systems",
        "question": "Describe a complete backend design (components and data flow) for an autocomplete service that supports prefix matching and per-user personalization, must handle 100K QPS with p95 latency <50ms. Include:\n- how you\u2019d store/index suggestions (Elasticsearch/Redis/Postgres roles),\n- how you\u2019d incorporate personalization at query time (ranking, caching, feature sources),\n- how you\u2019d keep results fresh with updates (streaming from Kafka),\n- key scaling and latency optimizations (sharding, caching tiers, read path),\nand one concrete tradeoff you\u2019d make (and why).",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Relevance: Directly targets core role skills (autocomplete, prefix matching, personalization, 100K QPS, <50ms p95); mentions relevant tech (ES/Redis/Postgres/Kafka) without red flags (-0). Clarity: Well-scoped with explicit components, constraints, and structure; no bundling or ambiguity. Discriminative: Tests deep applied knowledge, trade-offs, and optimizations, strongly separating strong candidates who can reason about scale/personalization from weak ones.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-442",
        "role_title": "Backend Engineer - Event Streaming",
        "question": "Design a short architecture and describe the algorithm (no code) for a real-time event delivery component that meets these requirements:\n\n- Receives events published to NATS subjects.\n- Supports per-subscriber filtering by event type and a simple JSON field (e.g., user_id).\n- Delivers events to subscribers over WebSocket with low latency and at-least-once semantics.\n- Allows replay of missed events for up to 1 hour from Redis-backed storage.\n- Minimizes fan-out work when many subscribers share the same filters.\n\nIn your answer (2\u20135 minutes): outline the components, message flow, how you implement filtering and efficient fan-out, how replay is handled, and how at-least-once delivery is achieved. Mention any trade-offs.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core event streaming skills (fan-out, filtering, replay) and exact tech (NATS, WebSocket, Redis) from role; minor deduction as it doesn't specify scale. Clarity: Crystal clear with precise requirements, scoped to 2-5 min, no ambiguity. Discriminative: Requires applied architecture knowledge, trade-offs, and optimizations (efficient fan-out, at-least-once), strongly separating strong candidates who grasp real-time systems from weak ones.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-444",
        "role_title": "Backend Engineer - Service Templates",
        "question": "You\u2019re building a Cookiecutter-based service template for Go microservices that must enforce:\n- a standardized project layout,\n- automated GitHub Actions CI that builds, runs unit tests, and builds a Docker image,\n- Kubernetes manifests and an ArgoCD-friendly overlay,\n- default observability (Prometheus metrics + OpenTelemetry) and security best practices (RBAC least privilege, vulnerability scanning).\n\nDescribe, in 6\u20138 bullet points, the concrete files or template variables you would include in the Cookiecutter template and one CI job from GitHub Actions (name + key steps) to implement those requirements. Be specific about filenames/paths, tools, and any sensible defaults.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (Cookiecutter templates, CI/CD with GitHub Actions, observability, security, K8s/ArgoCD) with exact tech stack; minor deduction for not explicitly mentioning all role aspects like general scaffolding. Clarity: Crystal clear scope (6-8 bullets, specific files/vars/CI job), well-constrained for screening. Discriminative: Requires applied knowledge of tools/files/paths/practices with trade-offs (e.g., sensible defaults), strongly separates candidates familiar with Go microservices tooling from others.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-446",
        "role_title": "Staff Engineer - Distributed Tracing",
        "question": "Describe a complete sampling strategy you would implement for a high-traffic microservices platform (millions of traces/day) that balances cost, fidelity for debugging, and SLO-driven monitoring. Include:\n- how you would combine sampling types (e.g., head-based, tail-based, probabilistic, adaptive) and where they run (client/agent/collector),\n- the signals you would use to decide which traces to keep at higher fidelity,\n- how you would ensure rare but important traces (errors, latency outliers, transactional joins) are retained,\n- how OpenTelemetry/Jaeger components and Kafka/ClickHouse would fit into the pipeline,\n- and how you would measure and validate that the strategy meets both cost and observability goals.",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Relevance: Perfectly matches role's core focus on distributed tracing, sampling strategies, SLO correlation, and specified tech (OpenTelemetry/Jaeger, Kafka/ClickHouse); no red flags. Clarity: Crystal clear, well-scoped with explicit subpoints, minor deduction for lacking explicit time constraint. Discriminative: Tests deep applied knowledge, trade-offs (cost vs fidelity), system design integration, and validation\u2014excellent separation of strong architects from others.",
          "composite": 9.666666666666666
        }
      },
      {
        "role_id": "be-443",
        "role_title": "Senior Backend Engineer - Data Lake Architecture",
        "question": "Describe a concrete approach to implement partition pruning and efficient predicate pushdown for a large Iceberg table on S3 when queried via Spark and Trino. In your answer (briefly, 3\u20135 bullet points) include:\n- how you would design the table layout/partitioning and why,\n- what Iceberg features and metadata you would use to avoid full-file scans,\n- any Spark and Trino settings or query patterns you would apply to ensure pruning works end-to-end,\n- how you would validate performance and detect mis-pruning after deployment.",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Relevance: Perfectly matches role's data lake, Iceberg, schema evolution, partition pruning focus; no red flags. Clarity: Crystal clear with specific scope (3-5 bullets), well-formed components; minor deduction possible for bundled subpoints but highly scoped. Discriminative: Tests deep applied knowledge, trade-offs, end-to-end implementation, validation\u2014strong separation of experts from juniors.",
          "composite": 9.666666666666666
        }
      },
      {
        "role_id": "be-447",
        "role_title": "Backend Engineer - Load Shedding",
        "question": "You run a Go microservice behind Envoy that receives mixed-priority RPCs (high, medium, low). Under overload you must shed low-priority work first, then medium, while preserving high-priority requests. Describe, at a high level, a practical design you would implement to accomplish this using in-process and infrastructure signals (include how Envoy, Redis, and Prometheus metrics could be used). Specifically explain:\n- how requests are classified and queued/throttled in the Go service,\n- what backpressure or throttling signals you consume/emit (from/to Envoy, Redis, Prometheus),\n- how adaptive thresholds are computed/updated,\n- and how you ensure graceful degradation without starving medium/high priority work.\n\nKeep the answer to ~4\u20136 concise bullet points or short paragraphs.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core role skills (load shedding, priority queues, adaptive throttling, backpressure) with exact tech (Go, Envoy, Redis, Prometheus); minor ding for assuming Redis/Prometheus not explicitly listed (-1). Clarity: Crystal clear, well-scoped with specific subpoints and answer format; no bundling or ambiguity. Discriminative: Tests applied design with trade-offs (classification, signals, adaptive logic, non-starvation), strong separation of deep understanding vs superficial knowledge.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-448",
        "role_title": "Senior Backend Engineer - Webhook Infrastructure",
        "question": "Design at a high level the data model and delivery flow for a webhook delivery system that guarantees at-least-once delivery, supports per-recipient exponential backoff with jitter, and records delivery analytics (success/failure, latency, attempts). Use Go services, Kafka for work queuing, PostgreSQL for state, Redis for rate-limiting, and Kubernetes for running workers. In 6\u20138 concise bullet points show:\n- the main tables/keys you\u2019d store in Postgres/Redis (fields and purpose),\n- how Kafka topics and consumer groups are used,\n- how a worker processes a delivery (including signature verification, idempotency/deduplication, and retry scheduling),\n- how you implement exponential backoff with jitter and per-recipient rate limiting,\n- how you capture delivery metrics for analytics and avoid double-counting on retries.\n\n(Keep the answer short \u2014 enough detail to show you can implement this in the given stack.)",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's core needs (webhook delivery, retries, signatures, analytics) and exact tech stack. Clarity: Crystal clear, well-scoped with bullet points, stack, and length limit; minor deduction for no time constraint. Discriminative: Tests deep applied knowledge of distributed systems, trade-offs (backoff/jitter, idempotency), and stack integration, strongly separates strong candidates.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-449",
        "role_title": "Junior Backend Engineer - Auth Integration",
        "question": "You\u2019re implementing an OAuth2 social login flow in a Node.js + TypeScript backend. After exchanging an authorization code for an access token and refresh token from the provider, describe (in 6\u20138 short bullet points or ~4\u20136 sentences) the next backend steps you would take to:\n- securely store or persist tokens in PostgreSQL,\n- associate the external account with an internal user account (new or existing),\n- handle using and refreshing the access token for future API calls,\nand mention one security risk to mitigate and how you would mitigate it.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets OAuth2 flows, token handling, and social login patterns core to the role's Node.js/TS/PostgreSQL stack (-0 red flags). Clarity: Precise scope (6-8 bullets/~4-6 sentences), unambiguous steps requested, no bundling or vagueness. Discriminative: Requires applied knowledge of secure storage, account linking, refresh logic, and security trade-offs, distinguishing strong entry-level candidates without being trivial or overly broad.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-451",
        "role_title": "Backend Engineer - Subscription Billing",
        "question": "Describe how you would design and implement proration when a customer changes from Plan A ($10/month, billed monthly) to Plan B ($30/month, billed monthly) halfway through their billing period. In your answer, include:\n- the data you would store (relevant fields in billing/usage records),\n- how you would calculate the prorated charge/credit,\n- how you would record the invoice/ledger entries,\n- how you would handle edge cases such as negative balances, existing credits, and concurrent plan changes (assume Java + PostgreSQL + Kafka + Spring Boot).",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Directly tests core proration skill from role description with precise scenario and specified stack; clear sub-parts with good scope; probes deep applied knowledge, trade-offs (edges, concurrency), distinguishes strong candidates. Minor ding for assuming concurrent changes without full constraints.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-452",
        "role_title": "Senior Backend Engineer - Faceted Search",
        "question": "Describe a performant approach to compute accurate facet counts for multiple filters in Elasticsearch when users can apply arbitrary combinations of filters (e.g., category, brand, price ranges). Explain how you would:\n1) Structure the aggregations to avoid incorrect counts due to active filters,\n2) Optimize for low latency at scale (millions of docs, high QPS),\n3) Handle expensive facets like ranges or nested fields.\n\nBe concise\u2014outline the aggregation strategy (types and scope), caching or precomputation you\u2019d use (if any), and any trade-offs.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core role skills in faceted search, dynamic filtering, aggregation optimization, and facet accuracy at scale in Elasticsearch (-0 red flags). Clarity: Crystal clear, well-scoped with specific subpoints, constraints (concise, outline), no ambiguity. Discriminative: Tests deep applied knowledge of ES aggs (post_filter, filters agg), optimizations (composite aggs, caching), trade-offs\u2014strong separation of experts from juniors.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-450",
        "role_title": "Principal Engineer - ML Platform Strategy",
        "question": "You\u2019re advising a mid-size company that currently has scattered ML experiments (Jupyter notebooks, ad-hoc scripts), some models in production behind Flask APIs on VMs, and plans to scale from 5 to 50 ML engineers. They already use Kubernetes for services. In 3\u20134 bullet points, outline the top 4 strategic platform decisions you would make in the next 6 months (tooling choices + governance) to enable scalable, reproducible model development and safe productionization \u2014 include one concrete trade-off for each decision.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests ML platform strategy, tooling, governance, and scaling\u2014core to role; leverages Kubernetes from context, no red flags (-0). Clarity: Precise scope (3-4 bullets, top 4 decisions, 6 months, includes trade-offs), unambiguous for screening. Discriminative: Requires applied strategic thinking, trade-offs, and MLops depth to distinguish experts from generalists; structured format ensures depth without being too broad.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-453",
        "role_title": "Backend Engineer - Notification Delivery",
        "question": "Design a simple Go function (pseudocode or explanation + key code snippets) that takes a list of device registrations and a notification payload, then prepares and groups the deliveries for sending to FCM and APNS with batching and de-duplication. Include how you would:\n- determine platform (FCM vs APNS) and group devices into batches (e.g., max 500 for FCM),\n- de-duplicate multiple registrations for the same user/device token,\n- return a structure ready for concurrent workers to send (batch metadata: platform, tokens, user_ids, payload).\n\nKeep the answer short\u2014show the core algorithm and data structures, not full implementation.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests Go, device registration grouping, batching, de-duplication, and delivery prep for FCM/APNS\u2014core to push notification system (-1 minor for pseudocode allowance). Clarity: Well-scoped with constraints (max 500, short answer, core algorithm focus), unambiguous. Discriminative: Requires applied knowledge of grouping, dedup, platform logic, data structures, and concurrency prep\u2014tests trade-offs like batch sizing, strong separation of good candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-454",
        "role_title": "Junior Backend Engineer - ETL Scripts",
        "question": "You have a CSV file with columns: user_id (int), event_time (ISO timestamp), event_type (string), value (float). Write a short Python (pandas) code snippet that:\n- Loads the CSV,\n- Keeps only rows where event_type is \"purchase\" and value > 0,\n- Converts event_time to UTC datetime and sets it as the DataFrame index,\n- Resamples the data to hourly buckets computing the sum of value per hour,\n- Writes the result to a Postgres table named hourly_purchases (assume SQLAlchemy engine variable `engine` exists).\n\nYou can assume standard imports (pandas as pd) and that event_time has timezone info or is UTC.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core ETL skills (load, filter, transform, aggregate, write to DB) with pandas for entry-level data scripting; minor Postgres mention fits data patterns (-1 for not in desc). Clarity: Precise steps, assumptions clear, well-scoped snippet. Discriminative: Requires chaining pandas ops, resampling, datetime handling, DB integration; tests applied ETL knowledge with trade-offs in bucketing/indexing.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-457",
        "role_title": "Senior Backend Engineer - SLO Automation",
        "question": "Design a Prometheus + Go-based service that tracks SLOs and fires burn-rate alerts. Briefly (2\u20135 minutes) describe:\n1) the metrics you would ingest from services to compute error budget and burn rate (names, types, labels),\n2) the algorithm/Prometheus query you\u2019d use to compute burn rate over a sliding window and trigger an alert,\n3) one Go component you would implement for SLO automation (what it does, inputs/outputs), and one key reliability concern and how you\u2019d mitigate it.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets Prometheus, Go, SLO/burn rate/error budget core to role (-0 red flags). Clarity: Well-scoped (2-5 min, numbered parts, specific asks), unambiguous despite multi-part. Discriminative: Probes applied knowledge (metrics, queries, algo, component, trade-offs/concerns) to separate strong role-fit candidates from weak ones.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-455",
        "role_title": "Staff Engineer - Developer Portal",
        "question": "Design a Backstage plugin and data model to onboard a new microservice and expose its API docs and health status in the developer portal.\n\nDescribe, in 3\u20135 bullet points or a short paragraph:\n- the Backstage entities and annotations you would use or extend (what fields/relations matter),\n- the plugin responsibilities (what backend and frontend parts do),\n- how you\u2019d implement an automated onboarding flow (what components run, validations, and how to persist metadata to the catalog),\n- one approach to surface runtime health and API docs (e.g., OpenAPI) in the UI and how you'd keep them up to date.\n\nFocus on concrete technologies/techniques (TypeScript/Node backend, PostgreSQL storage if used, how to validate/ingest metadata, polling vs event-driven updates), not high-level fluff.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests Backstage plugin design, entities, and onboarding flows\u2014core to the role; uses exact tech (TypeScript/Node) with no red flags (-0). Clarity: Precise scope (3-5 bullets/paragraph, specific subtopics), concrete tech requirements, well-constrained (-0). Discriminative: Probes deep applied knowledge of Backstage specifics, trade-offs (polling vs events), automation\u2014strong separation of experts from superficial knowledge.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-458",
        "role_title": "Backend Engineer - API Gateway Plugins",
        "question": "Describe how you would implement a Kong plugin (in Lua) that enforces per-client rate limits stored in Redis and adds an X-RateLimit-Remaining header to responses. In 6\u20138 bullet points, outline the plugin phases you\u2019d use, where you\u2019d interact with Redis, how you\u2019d handle race conditions and Redis failures, and what response/status you\u2019d return when a client is over the limit. Include any Kong or Lua patterns you would use for performance (e.g., timers, shared dictionaries, or nginx lua phases).",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Relevance: Directly tests Kong/Lua plugin development, Redis integration, and API gateway features (rate limiting, headers) core to the role; minor ding as role doesn't explicitly list Redis/Kong but matches plugin/auth/transformation focus (-1 red flag avoided). Clarity: Crystal clear with scoped bullet-point format, specific phases/handling asked, no ambiguity or bundling. Discriminative: Excellent at separating strong candidates via trade-offs (race conditions, failures), applied Kong/Lua patterns, and performance optimizations; not recall, deeply role-specific.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-456",
        "role_title": "Backend Engineer - Query Optimization",
        "question": "You have a PostgreSQL table orders(order_id PK, user_id, product_id, status, created_at TIMESTAMP, total NUMERIC) with millions of rows. Users query recent completed orders for a user like:\n\nSELECT * FROM orders WHERE user_id = $1 AND status = 'completed' AND created_at >= now() - interval '30 days' ORDER BY created_at DESC LIMIT 50;\n\nThe current execution is a sequential scan. Describe (briefly) what index(s) you would create to optimize this query, why, and any trade-offs or additional changes (e.g., vacuuming, stats, partial or covering index) you would consider.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (query optimization, index recommendations, slow query detection) using PostgreSQL and exact query pattern from role (-1 minor for not explicitly mentioning plan analysis). Clarity: Crystal clear, well-scoped with specific table/query/EXPLAIN result, asks for brief response with explicit considerations. Discriminative: Requires applied knowledge of multicolumn/partial/covering indexes, trade-offs (write overhead, storage), and PG-specific maintenance, strongly separates candidates able to reason through real optimization.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-460",
        "role_title": "Backend Engineer - Experiment Tracking",
        "question": "You have an ML experiment pipeline that logs metrics and models to MLflow (tracking server backed by PostgreSQL, artifacts to S3). Developers report that when training many short-lived experiments in parallel (10s\u2013100s concurrently), MLflow\u2019s tracking server slows and some runs fail to log artifacts. Describe the most likely causes at the MLflow / infra layer and list three concrete, prioritized changes you would make to reliably support high-concurrency experiment logging. For each change, include the reason, a brief implementation approach, and one way to validate it worked.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests MLflow, PostgreSQL, S3, and high-concurrency logging core to the experiment tracking role; minor -1 for not explicitly mentioning model comparison. Clarity: Crystal clear, well-scoped with specific scenario, asks for prioritized changes with exact structure. Discriminative: Requires deep applied knowledge of MLflow bottlenecks, trade-offs in infra scaling, and validation\u2014strong separation of experts from superficial knowers.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-459",
        "role_title": "Junior Backend Engineer - Container Security",
        "question": "You need to add automated container image scanning to a CI pipeline using Trivy. Describe, step-by-step, the minimal set of commands or CI steps to (1) scan a built Docker image for CVEs, (2) fail the pipeline if any HIGH or CRITICAL vulnerabilities are found, and (3) output a human-readable report. Include the Trivy command(s) and any Docker commands or flags you would use.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests container scanning with Trivy, core to role's vulnerability detection focus (-1 for assuming Trivy/CI knowledge not explicitly listed). Clarity: Precise, step-by-step with clear tasks 1-2-3, well-scoped for screening. Discriminative: Requires practical applied knowledge of Trivy flags, thresholding, and pipeline integration, distinguishing those who can execute from theorists.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-463",
        "role_title": "Junior Backend Engineer - Full-text Search",
        "question": "You have a PostgreSQL table \"articles\" with columns id (int), title (text), body (text), and a tsvector column search_vector that combines title and body. Write a single SQL query that:\n- searches for the phrase \"open source\" (should match as a phrase),\n- ranks results giving twice the weight to matches in title vs body,\n- returns id, title, snippet (a short excerpt highlighting matches), and a relevance score,\n- orders by score descending and limits to top 10.\n\nYou may assume the search_vector was created using setweight(to_tsvector(title), 'A') || setweight(to_tsvector(body), 'B'). Use plainto_tsquery or phraseto_tsquery as appropriate.",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's PostgreSQL full-text search focus (tsvector, ranking, optimization), uses exact tech from role; no red flags. Clarity: Crystal clear, well-scoped with assumptions and hints provided; minor deduction for not specifying exact ts_rank function. Discriminative: Requires applied knowledge of phrase search, weighting, ranking, snippets, distinguishing strong PostgreSQL candidates; not trivial recall.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-464",
        "role_title": "Staff Engineer - Presence Systems",
        "question": "Design a scalable heartbeat and connection-management scheme for a WebSocket-based presence system used by 10 million concurrent users across multiple datacenters. In 3\u20135 bullet points, describe:\n1) the heartbeat protocol (intervals, payload, client/server behavior) and how it minimizes false disconnects while limiting bandwidth;\n2) how you detect and reconcile stale connections when clients or servers crash (including cross-datacenter cases);\n3) how you store and propagate presence state (online/away/typing) with Redis and Kafka to ensure eventual consistency and low read latency.",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Relevance: Perfectly matches role's core focus on heartbeat protocols, connection management, state synchronization at scale, using specified tech (Redis/Kafka); no red flags. Clarity: Crystal clear with precise scoping (3-5 bullets, numbered parts), minor deduction for lacking exact time constraint. Discriminative: Requires deep applied knowledge of trade-offs (bandwidth vs reliability, crash detection, multi-DC consistency), strongly separates architects from juniors.",
          "composite": 9.666666666666666
        }
      },
      {
        "role_id": "be-461",
        "role_title": "Senior Backend Engineer - Fraud Detection",
        "question": "Design a real-time request flow (no diagrams required, 2\u20133 paragraphs) for scoring an incoming payment for fraud with these constraints: average end-to-end latency \u226450ms, allow synchronous rule engine checks, asynchronous ML model scoring if needed, support fast lookups (user/device history) and rate limits per user, and ensure failed ML service calls do not block approvals. Specify:\n- the components you would use (Kafka, Redis, PostgreSQL, Python services, etc.) and their roles,\n- which checks run synchronously vs asynchronously and how you propagate/merge their results,\n- how you handle state (history, rate limits), retries/fallbacks, and ensuring consistency for manual review queues.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core fraud detection pipeline skills (real-time scoring, rule engines, ML integration, history lookups, manual queues) with role-relevant tech like Kafka/Redis/PostgreSQL; minor deduction as exact stack not predefined in role (-1). Clarity: Crystal clear with precise constraints, scoped to 2-3 paragraphs, specific asks without ambiguity. Discriminative: Excellent at probing trade-offs (sync/async, latency, failure handling, consistency), deep system design understanding separates strong candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-468",
        "role_title": "Junior Backend Engineer - CI Pipelines",
        "question": "You have a Python project that uses pytest and a Docker-based test environment. Tests run slowly because each job builds the Docker image from scratch. Show a short GitHub Actions job snippet (YAML) that caches Docker build layers to speed up subsequent runs, and explain in one sentence how the cache works.\n\nExpect: a valid job using actions/cache or docker/build-push-action with cache-from/cache-to or buildx bake, and a one-sentence explanation.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Directly tests core CI pipeline skill (caching Docker layers) for entry-level role; clear YAML snippet request with precise expectations; requires applied knowledge of GitHub Actions caching options and concise explanation, distinguishing candidates who understand layer caching from those who don't. Minor ding on relevance for assuming pytest/Docker specifics not explicit in role, but highly aligned.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-466",
        "role_title": "Senior Backend Engineer - GitOps Tooling",
        "question": "Describe how you'd design a reconciliation loop for a custom GitOps controller (written in Go) that ensures Helm-released Kubernetes resources declared in Git stay in sync with the cluster, including how you would:\n1) detect and categorize drift (intent drift vs. configuration drift),\n2) avoid flapping and conflicting concurrent reconciliations (e.g., manual edits vs. ongoing syncs),\n3) implement safe automated rollback when a Git update causes failures.\n\nBe concise but include the key components, algorithms, and Kubernetes/ArgoCD/Helm primitives you'd use.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core GitOps skills (reconciliation loops, drift detection, rollback) with exact tech stack (Go, Helm, K8s); minor ding for not mentioning ArgoCD explicitly in role but it's GitOps standard (-1). Clarity: Well-scoped with specific subpoints, constraints (concise, key components/primitives), no bundling issues. Discriminative: Probes deep applied knowledge of trade-offs (flapping, concurrency, drift types, safe rollback), requiring synthesis of K8s/Helm primitives\u2014not rote recall.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-469",
        "role_title": "Senior Backend Engineer - Chaos Engineering",
        "question": "Describe a production-like scenario where a critical microservice in Kubernetes intermittently fails CPU spikes and packet loss. Outline a Chaos Mesh experiment (types of chaos, scope/blast radius, schedule) and a short Go-based health-check approach that detects failures and validates recovery using Prometheus metrics. Be concise: list the Chaos Mesh CRs/actions, blast-radius rules, Prometheus metrics/alerts used, and the Go health-check logic (key steps and thresholds).",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core chaos engineering (Chaos Mesh in K8s), failure injection, blast radius, recovery validation, and Go/Prometheus from role; minor ding for not explicitly naming 'infrastructure' but spot-on (-1 red flag avoidance). Clarity: Crystal clear, concise scoping, specific deliverables (CRs, rules, metrics, logic steps), no ambiguity or bundling. Discriminative: Requires applied knowledge of Chaos Mesh config, blast control, Prometheus integration, and Go health logic with trade-offs/thresholds; strong separation beyond recall.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-465",
        "role_title": "Backend Engineer - CDC Pipelines",
        "question": "Describe how you would design a Debezium-to-Kafka-to-Snowflake CDC pipeline to ensure exactly-once delivery and safe handling of schema changes (e.g., added/renamed columns). Include: (a) how Debezium configuration and Kafka topic/partitioning choices support ordering and deduplication, (b) how you would perform initial snapshots without duplicating data, and (c) how the Snowflake ingestion (or connector) should be configured to apply schema evolution and guarantee idempotent writes. Be concise and list the key settings/components you would use.",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core CDC pipeline skills (Debezium, Kafka, Snowflake) for exactly-once delivery, schema changes, and snapshots from role description; no red flags. Clarity: Well-scoped with clear parts (a-c), concise directive, and specific tech; minor deduction for lacking time constraint. Discriminative: Requires applied knowledge of configurations, trade-offs in ordering/deduplication/schema evolution, and integration specifics to separate strong candidates; not mere recall.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-467",
        "role_title": "Backend Engineer - Connection Pooling",
        "question": "You have a PostgreSQL-backed service running on Kubernetes using PgBouncer in transaction pooling mode. The service uses short-lived transactions but occasionally performs multi-statement transactions (explicit BEGIN/COMMIT). Recently you see intermittent \"server closed the connection unexpectedly\" errors and long tail latency during bursts of requests. Describe the likely cause and outline the minimal configuration/architecture changes (PgBouncer + Kubernetes + app) you would make to fix it while preserving pooling benefits. Be specific about PgBouncer pool_mode, max_client_conn, default_pool_size, and any app-side changes.",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Relevance: Perfectly matches role's core focus on connection pooling, pool sizing, transaction modes, and failover/scaling in PostgreSQL/K8s context\u2014no red flags. Clarity: Crystal clear scenario with specific expected details; minor vagueness on 'minimal' but well-scoped. Discriminative: Forces deep trade-off analysis (transaction vs session mode, config tuning) distinguishing experts from juniors.",
          "composite": 9.666666666666666
        }
      },
      {
        "role_id": "be-470",
        "role_title": "Backend Engineer - GraphQL Federation",
        "question": "You have two Apollo subgraphs: Users (exposes type User @key(fields: \"id\") with id, name, email) and Orders (exposes type Order with id, total, and a field userId). You must enable the gateway to resolve User fields when a client queries orders { id total user { id name email } }.\n\nIn 2\u20133 sentences, describe what key directive and resolver(s) you would add in each subgraph (including resolver signature and what it should return) and any schema changes needed to properly federate the relation.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core GraphQL Federation skills (schema stitching, @key, entity resolution) matching role exactly; minor deduction as role mentions query planning too (-1). Clarity: Crystal clear with specific schemas, query example, scoped to 2-3 sentences and exact artifacts needed. Discriminative: Requires applied knowledge of federation directives, resolver implementation, and trade-offs in entity resolution, strongly separating experts from juniors.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-471",
        "role_title": "Staff Engineer - Zero Trust Architecture",
        "question": "Design a production-ready approach to automate workload identities and mTLS for a Kubernetes-based microservices platform using SPIFFE and Envoy. In your answer (2\u20133 short paragraphs or bullet points), include:\n- how identities are issued and rotated (components and flow),\n- how Envoy sidecars obtain and present identity for mTLS,\n- how you enforce and evolve authorization policies across clusters (briefly mention tooling/integration points and outage/rollout considerations).",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (mTLS automation, workload identities in K8s microservices) with exact tech (SPIFFE/Envoy); minor ding as role mentions identity federation/policy enforcement more broadly (-1). Clarity: Crystal clear, well-scoped with format/depth guidance, no ambiguity. Discriminative: Probes deep applied knowledge, trade-offs (rotation/outages/rollouts), strong separation of experts from superficial knowers.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-475",
        "role_title": "Junior Backend Engineer - Search Analytics",
        "question": "You have logs of search requests stored in PostgreSQL with columns: user_id, query_text, timestamp, results_returned (integer), and click_positions (an integer array of 0-based positions in the results that the user clicked, or empty if no clicks). Write a single SQL query (PostgreSQL) that returns, for each query_text in the last 7 days, the total number of searches, the click-through rate (CTR = searches with \u22651 click / total searches), and the average position of first click (NULL if no clicks). Order results by number of searches descending and limit to top 50 queries.",
        "scores": {
          "relevance": 9,
          "clarity": 10,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests PostgreSQL SQL skills on query logging data central to search analytics role (-1 for entry-level but still core). Clarity: Crystal clear with exact schema, requirements, constraints, and output specs. Discriminative: Requires array handling, conditional aggregation, window functions, and edge cases (NULLs, empty arrays), distinguishing strong SQL thinkers from basic ones.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-462",
        "role_title": "Backend Engineer - Alert Routing",
        "question": "You\u2019re designing an Alertmanager-based routing service in Go that must: (a) group related alerts into a single notification when they share the same service and cluster labels, (b) deduplicate repeated alerts from the same instance within a 5-minute window, and (c) escalate to a PagerDuty team if an alert remains firing for >15 minutes. Briefly outline (in bullet points) the key components and algorithms you would implement (including data stores, key data structures, and how you\u2019d use Alertmanager/webhooks, Redis/Postgres, and PagerDuty APIs) to satisfy these requirements, and note one concurrency or consistency pitfall to watch for.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (grouping, deduplication, escalation) with exact tech (Alertmanager, Go implied, Redis/Postgres, PagerDuty); no red flags. Clarity: Crystal clear, well-scoped with specific requirements, explicit format (bullets), and tech; minor jargon but contextualized. Discriminative: Requires applied architecture knowledge, trade-offs in components/algorithms, and deep insight via pitfall; strong separation without being too broad.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-473",
        "role_title": "Senior Backend Engineer - Refund Processing",
        "question": "Design, at a high level (4\u20136 brief bullet points or a short diagram), how you would implement a partial refund flow in a payments service using Java + Spring Boot, PostgreSQL, Kafka and Redis. Your design should show:\n- how refund requests are recorded and validated,\n- how ledger entries are created/updated to maintain correct balances for partial refunds,\n- how you ensure idempotency and handle provider callback reconciliation (including out-of-order or duplicate callbacks),\n- how you provide an audit trail and reconcile eventual state with the ledger (describe any background job or Kafka usage).\n\nFocus on the key components, data you would store, and the failure/retry behaviors; keep it concise.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core refund processing, ledger reconciliation, partial refunds, callbacks, and audit trails using exact role tech stack (Java/Spring Boot, PostgreSQL, Kafka, Redis); minor deduction as role didn't explicitly list all techs but they align perfectly (-1 avoided). Clarity: Crystal clear with explicit scope (high-level, 4-6 bullets, specific aspects), constraints, and focus; no bundling or ambiguity. Discriminative: Requires applied knowledge of distributed systems, idempotency, trade-offs in failure/retry, reconciliation\u2014strong separation of strong vs weak candidates on real role challenges.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-472",
        "role_title": "Backend Engineer - Vector Search",
        "question": "Design a concise API endpoint (describe inputs, outputs, and key backend steps) for adding a batch of documents to a semantic search system that uses Pinecone for vector indexing, a Redis cache for recent query results, PostgreSQL for document metadata, and a Python-based embedding service. Include how you would:\n1) validate and deduplicate incoming documents,\n2) generate and store embeddings,\n3) upsert vectors into Pinecone with metadata linking to PostgreSQL rows,\n4) invalidate or update Redis cache entries affected by the new documents,\nand\n5) handle partial failures so the system remains consistent.\n\nKeep your answer high-level but specific about the order of operations, error handling strategy, and any needed idempotency mechanisms.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Relevance: Directly tests core vector search skills (embeddings, indexing, hybrid components) with exact tech stack (Pinecone, Redis, PostgreSQL); minor deduction for not explicitly mentioning hybrid search. Clarity: Crystal clear, well-scoped with numbered requirements, constraints on high-level/specific, order, and error handling; no bundling or ambiguity. Discriminative: Excellent at separating candidates via trade-offs in order of ops, partial failures, idempotency, and multi-system consistency\u2014requires deep applied understanding, not recall.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-474",
        "role_title": "Backend Engineer - Dashboards API",
        "question": "Describe how you would design a Go HTTP API endpoint to render a Grafana dashboard template with variable substitution and caching. Include:\n\n- the request/response shape you\u2019d accept (JSON fields for template id, variables, user/context for sharing),\n- how you\u2019d perform variable substitution safely (preventing injection),\n- where/what you would cache (Redis? cache keys/ttl) and invalidation strategy when templates or variables change,\n- how you\u2019d expose errors (template missing, variable mismatch, permission denied).\n\nKeep the answer high-level but specific enough to show practical decisions.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets Go APIs, Grafana templates, variable substitution, and sharing from role desc (-1 for assuming Redis/caching not explicitly listed). Clarity: Well-scoped, specific subtopics listed, high-level directive prevents over-engineering. Discriminative: Probes practical trade-offs (safety, caching strategy, invalidation, errors), separates strong Go/API engineers from juniors.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-477",
        "role_title": "Senior Backend Engineer - Data Catalog",
        "question": "Describe how you would design an automated pipeline that extracts metadata and lineage from running ETL jobs (Airflow tasks producing/consuming Kafka topics and writing to PostgreSQL) and ingests that metadata into Apache Atlas. In your answer, include:\n- what metadata and lineage you would capture (minimum set),\n- how you would capture it from Airflow, Kafka, and PostgreSQL (specific mechanisms/instrumentation),\n- how you would model and represent lineage in Atlas (entities/relationships),\n- how you would ensure correctness and handle schema drift and job failures.\n\nKeep the answer high-level but concrete; 4\u20136 bullet points or ~150\u2013250 words.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (automated metadata extraction, lineage tracking) using exact tech stack (Airflow, Kafka, PostgreSQL, implied Atlas for governance); minor ding as Atlas not explicitly listed (-1). Clarity: Crystal clear, well-scoped with specifics, word limit, and exact subtopics; no bundling or ambiguity. Discriminative: Requires applied knowledge of multiple systems, trade-offs in instrumentation/error handling, and Atlas modeling; distinguishes strong candidates via depth over recall.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-480",
        "role_title": "Junior Backend Engineer - Deployment Automation",
        "question": "You have an Ansible playbook that deploys a Python web app in a Docker container on an EC2 instance. During deployment you must:\n- ensure the required Python dependencies (from requirements.txt) are installed inside the container,\n- guarantee the container is restarted only if the image changed,\n- and provide a rollback task that runs if the deployment fails to restore the previous container image.\n\nWrite the minimal Ansible tasks (YAML snippets) that accomplish these three requirements. Assume variables: image_name, new_image_tag, container_name, requirements_path, and previous_image_tag.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests Ansible deployment scripting core to role (9; minor -1 for Docker/EC2 not explicitly listed but configuration management aligned). Clarity: Precise scope, clear requirements, well-defined variables and constraints (9). Discriminative: Requires applied Ansible knowledge (docker modules, conditions, handlers, error handling), tests trade-offs like idempotency/rollback, strong separation (9).",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-479",
        "role_title": "Principal Engineer - Data Architecture",
        "question": "Design a high-level data architecture for a product that currently uses PostgreSQL for transactional data but needs sub-second analytics queries on event streams (high ingest via Kafka) and occasional large-scale time-series queries. Using PostgreSQL, ClickHouse, Kafka, Redis, and Kubernetes, outline the components you would introduce or change, how data would flow between them, where OLTP vs OLAP responsibilities would live, and how you would ensure low-latency reads, data consistency, and operational simplicity. Call out one key trade-off and one important operational risk and how you'd mitigate it.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Relevance: Directly tests core data architecture strategy, tech selection (matches exact stack: PostgreSQL, Kafka + implied others), OLTP/OLAP split for role (-0 red flags). Clarity: Crystal clear, well-scoped with specific tech, flows, responsibilities, and explicit asks for trade-off/risk; no ambiguity or bundling. Discriminative: Requires deep applied knowledge of systems integration, trade-offs (e.g., consistency vs latency), and mitigation strategies, strongly separating architects from juniors.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-476",
        "role_title": "Backend Engineer - Live Updates",
        "question": "Design, in TypeScript, the Socket.io event handlers and Redis usage (high-level, pseudocode is fine) for a dashboard live-updates service that must:\n- let clients subscribe to a dashboard \"room\" to receive only that dashboard's updates,\n- publish updates from backend workers to all active room members across a multi-node Socket.io cluster,\n- ensure a client that disconnects and reconnects within 30 seconds receives missed updates (only for that dashboard),\n- avoid sending updates to clients viewing other dashboards.\n\nInclude:\n- the Socket.io events and room join/leave logic,\n- how you'd use Redis (or Redis Pub/Sub) to broadcast updates across nodes,\n- how you'd implement the 30-second missed-update reconnection behavior (briefly explain state storage and reconnection flow).",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (Socket.io rooms, Redis broadcasting, reconnection logic) in TypeScript; minor ding as role doesn't explicitly list Socket.io/Redis but matches 'live update system' and multi-node needs (-1). Clarity: Well-scoped with specific requirements, allows pseudocode, no bundling or ambiguity. Discriminative: Requires applied knowledge of Socket.io clustering, Redis Pub/Sub, state management, and trade-offs (e.g., TTL storage), strongly separates candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-478",
        "role_title": "Backend Engineer - Cost Attribution",
        "question": "You need to design a simple approach to attribute cloud VM costs to teams for chargeback reporting.\n\nGiven:\n- Each VM has a set of tags: {\"team\": string, \"env\": string, ...} but some VMs are missing the \"team\" tag.\n- You have Prometheus metrics of resource usage (CPU seconds, memory bytes) per VM and cloud billing line items with cost per VM per hour.\n- Requirement: allocate total cost for each VM-hour to teams when \"team\" tag is present; when missing, allocate that VM-hour proportionally across teams based on that hour\u2019s observed CPU-seconds by team (using only VMs that have a \"team\" tag). Describe, at a high level, the algorithm and data pipeline you would implement to compute hourly chargeback amounts per team. Include:\n  - the data sources and key joins/aggregations,\n  - how you handle missing/late data and small/noisy usage,\n  - complexity or performance considerations for running this nightly for thousands of VMs,\n  - one concrete Go data structure or function signature you would use to represent and compute the per-hour allocation.\n\n(Answer in 2\u20135 minutes; be concise.)",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (cost attribution, tagging, chargeback) with exact tech (Prometheus, cloud billing, VMs); minor ding for not specifying Go in role but it's fitting (-1). Clarity: Crystal clear with precise scope, constraints, and explicit subpoints; time-bound perfectly. Discriminative: Requires applied knowledge, trade-offs (missing data, noise, perf), and concrete implementation; strong separation of strong vs weak candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-481",
        "role_title": "Backend Engineer - Canary Deployments",
        "question": "You\u2019re implementing an automated canary pipeline for a Go microservice running on Kubernetes with Istio, Prometheus metrics, and ArgoCD for deployments. Describe, at a high level, the sequence of steps (including the Istio and Prometheus actions) your controller should perform to safely promote a canary from 0% to 100% traffic and automatically rollback on degradation. For each step, name the key checks or metrics you would evaluate and the thresholds or comparison logic you would use to decide continue/promote/rollback. (Answer in 2\u20135 minutes.)",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core canary deployment skills (traffic shifting, metrics, rollback) with exact tech stack (Go, K8s, Istio, Prometheus, ArgoCD) from role; no red flags (-0). Clarity: Unambiguous, well-scoped with time limit, clear structure requested, no bundling or vagueness (-0). Discriminative: Requires applied knowledge of integration, trade-offs in metrics/thresholds/steps, distinguishes deep understanding from superficial recall; time-bound for screening.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-483",
        "role_title": "Backend Engineer - WAF Integration",
        "question": "Describe how you would design and implement, in Go, a ModSecurity-based request flow that (1) receives an incoming HTTP request in Nginx, (2) consults a Redis-backed custom rule store to decide whether to block, challenge, or allow the request, and (3) logs attack patterns to PostgreSQL for later analysis. In your answer include:\n\n- the integration points between Nginx, ModSecurity, and a Go service (brief sequence of calls/events),\n- how the Go service would query/update Redis atomically for rate-limiting and bot-detection decisions,\n- how you would send decisions back to ModSecurity/Nginx (block/challenge/allow) with minimal latency,\n- one schema idea for PostgreSQL to store attack patterns and one example query to retrieve top attack sources in the last 24 hours.\n\nKeep the answer concise (2\u20135 minutes).",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core WAF integration, rate limiting, bot detection, attack analysis with Go/Redis/PostgreSQL; minor ding for Nginx/ModSecurity not explicitly listed (-1). Clarity: Crystal clear, well-scoped with numbered points, constraints, and time limit. Discriminative: Tests deep applied knowledge of integrations, atomic ops, latency, schemas/queries, forcing trade-offs and specifics to separate strong candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-484",
        "role_title": "Staff Engineer - Model Registry",
        "question": "Describe a production-ready design for a model registry using MLflow that supports versioning, staged promotion (e.g., staging \u2192 production), deployment automation to Kubernetes (pulling artifacts from S3), and audit/governance (who approved what and why). Include:\n- how you would structure MLflow model/version metadata and tags,\n- how promotion is implemented (automation and safeguards),\n- how deployments to Kubernetes are triggered and how you ensure deployed code + model are reproducible,\n- what audit trails and access controls you would enforce (and where you store them),\n- one failure scenario (e.g., bad model promoted) and how your design detects and remediates it.",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 10,
          "reasoning": "Relevance: Perfectly matches role's core needs (MLflow registry, versioning, promotion, K8s deployment, governance); no red flags. Clarity: Crystal clear with specific subpoints and scope; minor deduction as 'production-ready' could imply varying depths but well-constrained overall. Discriminative: Tests deep applied knowledge, trade-offs, failure handling; excellent separation of strong MLOps architects from weaker candidates.",
          "composite": 9.666666666666666
        }
      },
      {
        "role_id": "be-485",
        "role_title": "Junior Backend Engineer - Payment Webhooks",
        "question": "You receive POST webhook requests from a payment provider to your FastAPI endpoint /webhook. Each request includes a unique event ID in the JSON body and must be processed exactly once (idempotent). Describe, step-by-step and at a high level, how you would implement the webhook handler to:\n- verify and reject duplicate events,\n- ensure processing is safe under concurrent delivery (multiple identical requests arriving at the same time),\n- and persist the event outcome in PostgreSQL.\n\nMention any Redis or DB primitives you would use and where you would check or write the event record. (Keep the answer concise \u2014 2\u20135 short steps.)",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (idempotency, event processing, PostgreSQL) with relevant tech (FastAPI); minor ding for assuming Redis (-1). Clarity: Crystal clear, well-scoped with step-by-step, primitives specified, and length constraint. Discriminative: Requires applied knowledge of concurrency, DB primitives (e.g., UPSERT, locks), trade-offs in webhook handling, distinguishing strong entry-level candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-486",
        "role_title": "Backend Engineer - Span Processing",
        "question": "Design a tail-sampling component (in Go) for an OpenTelemetry-based span pipeline that reads completed spans from Kafka, performs policy-based tail sampling, enriches selected spans with service metadata, and writes them to ClickHouse. In 5\u20137 bullets, outline the processing flow and specify:\n- how you would ensure sampling decisions are consistent for traces spanning multiple partitions/hosts,\n- what data you would store in Kafka messages to enable sampling and enrichment,\n- one efficient approach to deduplicate or compress span data before writing to ClickHouse,\n- two failure/retry considerations for correctness.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (tail sampling, attribute enrichment, span pipeline) with exact tech stack (Go, OpenTelemetry, Kafka, ClickHouse); no red flags (-0). Clarity: Crystal clear, well-scoped with bullet format, specific sub-questions, and constraints; minor jargon is role-appropriate (-0). Discriminative: Probes deep applied knowledge, trade-offs (consistency, efficiency, failures), distinguishes strong distributed systems thinkers; not recall or trivial (-0).",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-489",
        "role_title": "Backend Engineer - Data Quality",
        "question": "You have a daily Airflow DAG that runs dbt models to populate tables in Snowflake. Recent incidents show (a) occasional late upstream feeds that make tables missing for a day, (b) sporadic schema drift where a column type changes from VARCHAR to NUMBER, and (c) sudden drops in row counts. Describe a concise, practical monitoring design (checks, thresholds, tooling, and actions) you would implement to detect and respond to these three issues. Be specific about where you would implement checks (Airflow, dbt, Great Expectations, or Snowflake), what logic/metrics you\u2019d use (e.g., freshness window, percentage change, schema diff), and the automated actions or alerts you would trigger for each failure type.",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Directly targets core role skills (freshness, volume anomalies, schema drift) using exact tech stack (Airflow, dbt, Snowflake). Crystal clear with specific issues, tools, metrics, and actions requested; minor vagueness on exact thresholds but well-scoped. Tests applied design with trade-offs (tool choice, logic, automation), strongly separates candidates with warehouse monitoring experience.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-487",
        "role_title": "Senior Backend Engineer - Search Ranking",
        "question": "You have a production search that returns results ranked by a BM25 score from Elasticsearch. Users often prefer newer items even when BM25 relevance is slightly lower. You need to implement a simple learning-to-rank adjustment in Python that boosts newer items while preserving relevance for clearly better matches.\n\nDescribe, in pseudocode or short Python, how you would:\n1) compute a final score combining BM25 (bm25_score) and item age in days (age_days) using a smooth, monotonic function that gives diminishing boost for very new items and negligible boost for old items; \n2) ensure that a difference of 0.3 BM25 points still outweighs maximal age boost (i.e., rank stability); and \n3) how you would validate this change via a small offline test using click/impression logs.\n\nKeep the answer concise and show the formula/logic and the offline validation steps.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core LTR pipeline skills (feature engineering, scoring, validation) matching role's BM25/Elasticsearch focus; minor ding for not mentioning A/B but covers offline eval (-1). Clarity: Precise scope, explicit constraints, clear numbering; well-formed for screening. Discriminative: Requires applied math (smooth monotonic combo with stability), formula design, and validation process; tests trade-offs deeply without being too broad.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-488",
        "role_title": "Backend Engineer - Gaming Backend",
        "question": "Design a WebSocket-based message flow (sequence of events and data structures) for a 4-player real-time game room using Go where:\n- players can join/leave, send input events (e.g., move/ability), and receive authoritative state updates;\n- the server must minimize bandwidth and handle transient packet loss;\n- matchmaking must ensure players are placed into rooms of similar latency.\n\nDescribe (in 4\u20138 short bullet points or a small diagram):\n1. the client\u2192server and server\u2192client messages (names and key fields, e.g., seq, timestamp, proto buffer types);\n2. how you handle ordering, deduplication, and lost messages with minimal bandwidth (no full snapshot every tick);\n3. how the server produces authoritative state updates (tick approach, interpolation/extrapolation hints to clients);\n4. one simple matchmaking rule that uses latency to form rooms.\n\nKeep the answer concise \u2014 2\u20135 minutes to explain.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core role skills (real-time sync, matchmaking, anti-cheat via authoritative state) and WebSocket/Go; minor ding for assuming Go/proto without explicit role mention (-1). Clarity: Crystal clear with precise scope (4-8 bullets, 2-5 min), numbered parts, constraints; no bundling issues. Discriminative: Tests applied trade-offs (bandwidth, loss, ordering, latency matchmaking, auth state), deep understanding over recall; strong separation of strong/weak candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-493",
        "role_title": "Staff Engineer - Reliability Standards",
        "question": "You're designing a production readiness checklist for a new Go service running on Kubernetes that will be critical to customer transactions. List the top 8 checklist items you would require before marking the service \"production-ready.\" For each item, give one concrete, measurable acceptance criterion (e.g., specific metric threshold, test result, or configuration) and briefly (1 sentence) explain why it matters for reliability.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Highly relevant to core SRE duties like production readiness reviews, directly tests Go/K8s knowledge in context; crystal clear with precise scoping (top 8 items, measurable criteria, 1-sentence explanation); strong differentiation via applied SRE trade-offs and concrete metrics, not mere recall.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-491",
        "role_title": "Junior Backend Engineer - Read Replicas",
        "question": "You have a web application using PostgreSQL with one primary (write) and two read replicas. In Python (SQLAlchemy), describe in 3\u20135 bullet points how you would route queries so that:\n- all INSERT/UPDATE/DELETE statements always go to the primary, and\n- SELECTs go to replicas, but if the most recent write might not yet be visible on replicas (e.g., immediately after creating a record), the app reads from the primary.\n\nInclude:\n- how you'd detect query type,\n- how you'd decide when to force a primary read after a recent write,\n- one simple implementation approach or pattern (no full code required).",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (read replica routing, replication lag handling) with exact tech stack (PostgreSQL, Python/SQLAlchemy); minor deduct for entry-level but no red flags. Clarity: Crystal clear, well-scoped with bullet constraints, specific requirements, no ambiguity. Discriminative: Requires applied knowledge of query routing, lag detection (e.g., timestamps/sticky sessions), and patterns (e.g., write-follows-reads), distinguishing those understanding trade-offs from rote memorizers.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-492",
        "role_title": "Backend Engineer - CD Pipelines",
        "question": "Describe a GitOps-based CD pipeline for deploying a Go microservice to Kubernetes across three environments (dev \u2192 staging \u2192 prod) using ArgoCD and GitHub Actions. Include:\n- the repo layout (what goes in app config vs. environment overlays),\n- how promotion between environments is implemented (automation vs. manual approval),\n- how you would implement a rollback triggered by a failed health check in prod,\n- and one concrete ArgoCD/GitHub Actions setting or pattern you\u2019d use to prevent accidental prod deployments.\n\nKeep the answer brief (2\u20135 minutes).",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core GitOps, ArgoCD, multi-env promotion, approvals, and rollback\u2014exact role focus (-1 for minor GitHub Actions inclusion, not core but relevant). Clarity: Crystal clear with specific components, scoped by time limit. Discriminative: Requires applied knowledge of repo patterns, automation trade-offs, failure handling, and safety measures\u2014strong separation of experts from juniors.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-494",
        "role_title": "Backend Engineer - API Mocking",
        "question": "You have an existing Node.js/TypeScript test suite that spins up a WireMock server in Docker to mock downstream HTTP services. Sometimes tests fail because WireMock returns a default 404 instead of the expected stubbed response. Describe three concrete, actionable reasons this can happen (related to request matching, templating, lifecycle/timing, or Docker/networking) and for each give one specific fix you would implement.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core API mocking skills (request matching, templating) in Node.js/Docker context matching role; minor deduction as WireMock is specific tool not explicitly listed (-1). Clarity: Crystal clear, well-scoped to 3 reasons/fixes with explicit categories, no ambiguity. Discriminative: Requires applied debugging knowledge of real-world failure modes and trade-offs in fixes, strongly separates experienced candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-482",
        "role_title": "Senior Backend Engineer - API Monetization",
        "question": "Design a high-level approach (3\u20136 concise bullet points) for implementing per-API-key usage tracking and quota enforcement in a Go-based API gateway using Kong, PostgreSQL, and Redis. Your answer should cover:\n- where and how usage is recorded in the request path (low-latency path),\n- how Redis and PostgreSQL are used (what data each stores and why),\n- an approach to enforce quotas atomically under high concurrency,\n- how to handle eventual billing integration with Stripe for overages or tier changes.\n\nFocus on concrete mechanisms (e.g., Redis ops, Kong plugin placement, schema ideas, consistency trade-offs), not on boilerplate code.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (usage tracking, quotas) with exact tech stack (Go, Kong, PostgreSQL, Redis); minor deduction as Stripe is specified but not in role desc (-1). Clarity: Crystal clear with precise scope (3-6 bullets, specific topics), well-constrained for screening. Discriminative: Requires applied knowledge of distributed systems, trade-offs (latency vs consistency), and integrations, strongly separates strong candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-495",
        "role_title": "Senior Backend Engineer - Audit Logging",
        "question": "Design a tamper-evident, queryable audit logging pipeline using Go, Kafka, ClickHouse (for analytics) and PostgreSQL (for metadata) running on Kubernetes. In 6\u20138 bullet points, describe:\n- How you would ensure logs are tamper-evident from ingestion through storage (include any cryptographic techniques, where hashes/signatures are stored, and who has the keys).\n- How you would route/partition events between ClickHouse and PostgreSQL and what each stores.\n- How you would implement retention and legal-hold policies without breaking tamper-evidence.\n- One Kubernetes operational concern (e.g., scaling, backups, rolling upgrades) and how you would address it.\n\nKeep answers practical and specific to the tech stack.",
        "scores": {
          "relevance": 10,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Perfectly matches role's audit logging focus with tamper-proofing, querying, retention, using exact tech stack (Go, Kafka, ClickHouse, PostgreSQL, K8s). No red flags. Clarity: Crystal clear, well-scoped with bullet points, specific tech, minor deduction for lacking exact time constraint. Discriminative: Tests deep applied knowledge, trade-offs (crypto, partitioning, retention with tamper-evidence, ops), separates strong candidates effectively. Minor deduction as very strong candidates might finish under 10min.",
          "composite": 9.333333333333334
        }
      },
      {
        "role_id": "be-496",
        "role_title": "Junior Backend Engineer - Model Serving",
        "question": "You need to serve a pretrained scikit-learn classification model with FastAPI. Describe (in 6\u20138 short bullet points) how you would implement an endpoint that:\n- loads the model on startup,\n- accepts JSON with a list of feature vectors,\n- performs batched prediction,\n- returns predictions as JSON,\nand mention one simple way to handle concurrent requests safely when using the model object.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 8,
          "reasoning": "Relevance: Directly tests core skills (model serving as API with scikit-learn/FastAPI, batching, loading) matching entry-level role; minor ding for not mentioning monitoring (-1). Clarity: Crystal clear, well-scoped with bullet point limit, specific requirements, no ambiguity. Discriminative: Tests applied knowledge of FastAPI startup, batch prediction, concurrency (e.g., locks), distinguishes practical candidates from theorists; not perfect depth but strong for screening.",
          "composite": 8.666666666666666
        }
      },
      {
        "role_id": "be-499",
        "role_title": "Principal Engineer - Search Platform",
        "question": "Design, at a high level, the architecture for an enterprise search platform that must serve multi-tenant web and API clients with low latency and near-real-time indexing. Your design should name and justify choices for data ingestion (including Kafka), indexing (Elasticsearch), caching (Redis), deployment/operations (Kubernetes), and how to handle schema evolution, tenant isolation, and scaling (both read and write). Also state one key operational metric to track and one runbook action for when query latency SLOs are breached. (You have 2\u20135 minutes to answer.)",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets enterprise search platform architecture/strategy, explicitly names key components (Kafka/ES/Redis/K8s) central to role's tech leadership focus (-0 red flags). Clarity: Crystal clear, well-scoped with time limit, specific components/justifications/areas required, no ambiguity. Discriminative: Tests applied system design, trade-offs (isolation/scaling/schema), and ops excellence (metrics/runbooks), strongly separates strategic thinkers from basic engineers.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-498",
        "role_title": "Senior Backend Engineer - Profiling Infrastructure",
        "question": "Describe a design for a continuous CPU profiling pipeline for a Kubernetes microservices environment (Go services). Include:\n- where and how profiles are collected (tools/agents, sampling frequency, security/overhead tradeoffs),\n- how profiles are stored and indexed for efficient baseline comparisons (consider ClickHouse or object storage),\n- how you would generate and serve flame graphs and diff comparisons (Pyroscope or other),\n- one strategy to reduce storage cost while preserving useful historical comparison capability.\n\nKeep the answer to a brief bullet-list or short paragraphs (2\u20135 minutes).",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core role skills (continuous profiling, flame graphs, baseline comparison, storage optimization) in exact context (Kubernetes/Go); minor ding for not explicitly naming all tools but aligns perfectly (-0). Clarity: Crystal clear with explicit subtopics, scoped to 2-5 min bullet/short paras; no bundling issues. Discriminative: Requires applied design knowledge with tradeoffs (security/overhead, storage cost), distinguishes deep expertise from superficial recall; excellent separation for role.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-497",
        "role_title": "Backend Engineer - Payout Systems",
        "question": "Design a short-outline answer (2\u20133 bullet points each) for how you would implement a daily batch payout job in Go that reads settlement records from PostgreSQL, groups payments by bank account to minimize transfers, validates bank details via a (rate-limited) external bank-validation service, and publishes finalized payout instructions to Kafka for execution. Include:\n- key data model fields you\u2019d query from Postgres,\n- how you\u2019d implement grouping and deduplication to minimize transfers,\n- how you\u2019d handle bank-validation with rate limits and transient failures,\n- what reconciliation events you\u2019d publish to Kafka and what minimal schema/metadata each event should include.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly targets core role skills (batch processing, bank validation, reconciliation) in Go/PostgreSQL/Kafka; minor deduction as exact tech stack not in description but perfectly matches domain (-1). Clarity: Precise scope (2-3 bullets per section), unambiguous requirements, well-constrained for screening. Discriminative: Tests applied architecture, trade-offs (grouping/rate-limits/failures), data modeling, and schemas\u2014strong separation of strong vs weak candidates.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-490",
        "role_title": "Senior Backend Engineer - Environment Management",
        "question": "Design at a high level (2\u20133 short bullet points or a small pseudo-code snippet) a Go-based controller that ensures each preview deployment gets:\n- its own Kubernetes namespace with a unique name,\n- a ResourceQuota limiting CPU/memory and pod count,\n- an owner label and a TTL annotation so a cleanup job can remove namespaces after expiry.\n\nDescribe how your controller would:\n1) create/ensure the namespace and ResourceQuota when a new preview request arrives,\n2) ensure idempotency and handle reconciliation if kube-api objects are modified outside the controller,\n3) surface or persist the TTL so a separate cleanup process (or the controller) can reliably find and delete expired namespaces.\n\nKeep the answer concise and focus on practical implementation hooks (client-go usage, reconcile loop behavior, labels/annotations, retries/optimistic concurrency), not full code.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core role skills (namespace isolation, quotas, TTL cleanup) in Go/Kubernetes context (-0 red flags). Clarity: Well-scoped with explicit format (2-3 bullets/pseudo-code), numbered sub-questions, practical focus, and constraints. Discriminative: Probes high-level design, idempotency, reconciliation, trade-offs in client-go/reconcile loops\u2014separates strong K8s controller devs from basics.",
          "composite": 9.0
        }
      },
      {
        "role_id": "be-500",
        "role_title": "Staff Engineer - Streaming Architecture",
        "question": "Design a Kafka-based streaming pipeline to deliver real-time user presence updates (connect/disconnect/status) to 10k concurrent WebSocket-connected clients per user device, with these constraints:\n- End-to-end latency target <200ms\n- At-least-once delivery but avoid duplicate state flashes to clients\n- Clients should only receive updates for users they subscribe to\n- Must scale horizontally on Kubernetes; use Redis as needed\n\nIn 2\u20135 minutes, describe:\n1) the topic/schema design and partitioning strategy,\n2) how you'd implement fan-out to WebSocket servers (including Redis usage) to meet latency and delivery guarantees while minimizing duplicate messages,\n3) one failure mode and how your design handles it.",
        "scores": {
          "relevance": 9,
          "clarity": 9,
          "discriminative": 9,
          "reasoning": "Relevance: Directly tests core streaming architecture, Kafka (implied in role's event delivery), schemas, fan-out, guarantees (-1 for not explicitly listing Kafka/Redis/K8s). Clarity: Crystal clear with precise constraints, scoped to 2-5 min, structured subparts. Discriminative: Probes deep trade-offs in partitioning, fan-out, dedup, scaling, failure handling; distinguishes architects from basic Kafka users.",
          "composite": 9.0
        }
      }
    ]
  }
}